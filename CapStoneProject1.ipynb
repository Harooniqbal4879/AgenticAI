{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwxPqMT0omBEQHM/6lGK38",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harooniqbal4879/AgenticAI/blob/main/CapStoneProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capstone Project : AI Content Marketing Assistant"
      ],
      "metadata": {
        "id": "PjY2d8zxakQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Business Use Case\n",
        "Content marketing has become the backbone of modern digital strategy, yet creating high-quality, engaging content at scale remains one of the biggest challenges facing businesses today. Content creators, marketers, and entrepreneurs struggle with research fatigue, inconsistent quality, and the time-intensive nature of producing diverse content formats that resonate with their audiences."
      ],
      "metadata": {
        "id": "mNaycFd3avyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Business Opportunity:\n",
        "â— Democratized Content Creation: By providing intelligent content generation across multiple formats (research, blogs, LinkedIn posts, visuals), it enables small businesses and solo creators to compete with larger organizations that have dedicated content teams.\n",
        "\n",
        "â— Intelligent Content Strategy: The multi-agent system understands context, maintains brand voice consistency, and creates content that aligns with specifi c marketing objectives, eliminating the guesswork in content planning.\n",
        "\n",
        "â— Time-to-Market Acceleration: What traditionally takes hours or days of research, writing, and design can be accomplished in minutes, allowing businesses to respond rapidly to market trends and opportunities.\n",
        "\n",
        "â— Cross-Platform Content Optimization: Each agent specializes in platform-specifi c requirements (SEO for blogs, engagement for LinkedIn, visual appeal for images), ensuring content performs optimally across different channels."
      ],
      "metadata": {
        "id": "BgTYbAU5bZ7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following architeectural Components:\n",
        "\n",
        "LangGraph\n",
        "\n",
        "OPenAI GPT -4\n",
        "SERP API + GPT\n",
        "DALL-E 3\n",
        "Custom LLM Prompts\n",
        "Streamlit\n",
        "LangGraph Memory\n",
        "\n"
      ],
      "metadata": {
        "id": "-bK8QUXMbq6-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d217503"
      },
      "source": [
        "# LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# OpenAI GPT-4\n",
        "# You will typically use the openai library\n",
        "import openai\n",
        "\n",
        "# SERP API + GPT\n",
        "# You will need a library to interact with the SERP API, e.g., serpapi-python\n",
        "# You will also use the openai library\n",
        "# from serpapi import GoogleSearch\n",
        "\n",
        "# DALL-E 3\n",
        "# DALL-E 3 is accessed via the openai library\n",
        "# import openai\n",
        "\n",
        "# Custom LLM Prompts\n",
        "# This doesn't require a specific import statement, it's about how you structure your prompts\n",
        "# You might use f-strings or other string formatting techniques\n",
        "\n",
        "# Streamlit\n",
        "# You will need to install streamlit\n",
        "# import streamlit as st\n",
        "\n",
        "# LangGraph Memory\n",
        "# Memory in LangGraph is typically handled within the graph definition and state.\n",
        "# Specific memory implementations might require imports depending on what you use (e.g., for persistent storage)\n",
        "# from langgraph.prebuilt import create_agent_executor\n",
        "\n",
        "# Placeholder imports for potential use\n",
        "# import os\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "# load_dotenv()\n",
        "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "# os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8099de83"
      },
      "source": [
        "%pip install langgraph openai serpapi streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Query Handler Agent: Routes requests to appropriate specialized agents\n",
        "2. Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "3. SEO Blog Writer Agent: Creates search-optimized long-form content\n",
        "4. LinkedIn Post Writer Agent: Generates engaging professional social content\n",
        "5. Image Generation Agent: Produces custom visuals with prompt optimization\n",
        "6. Content Strategist Agent: Formats and organizes research into readable content"
      ],
      "metadata": {
        "id": "u7wd3pipd9jf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cec5408"
      },
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define the state for the graph\n",
        "class GraphState:\n",
        "    def __init__(self, query=None, research_data=None, blog_content=None, linkedin_content=None, image_prompt=None, generated_image=None):\n",
        "        self.query = query\n",
        "        self.research_data = research_data\n",
        "        self.blog_content = blog_content\n",
        "        self.linkedin_content = linkedin_content\n",
        "        self.image_prompt = image_prompt\n",
        "        self.generated_image = generated_image\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"GraphState(query='{self.query}', research_data='{self.research_data}', \"\n",
        "                f\"blog_content='{self.blog_content}', linkedin_content='{self.linkedin_content}', \"\n",
        "                f\"image_prompt='{self.image_prompt}', generated_image='{self.generated_image}')\")\n",
        "\n",
        "\n",
        "# Define the Query Handler Agent\n",
        "def query_handler_agent(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to.\n",
        "    \"\"\"\n",
        "    print(\"---QUERY HANDLER AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    # Basic routing logic (can be expanded)\n",
        "    if \"research\" in query.lower():\n",
        "        print(\"Routing to Deep Research Agent\")\n",
        "        return \"deep_research\"\n",
        "    elif \"blog\" in query.lower():\n",
        "        print(\"Routing to SEO Blog Writer Agent\")\n",
        "        return \"seo_blog_writer\"\n",
        "    elif \"linkedin\" in query.lower():\n",
        "        print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "        return \"linkedin_post_writer\"\n",
        "    elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "        print(\"Routing to Image Generation Agent\")\n",
        "        return \"image_generation\"\n",
        "    else:\n",
        "        print(\"Routing to Content Strategist Agent (default)\")\n",
        "        return \"content_strategist\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-search-results"
      ],
      "metadata": {
        "id": "5_SrJkKE6z98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141bd7e1"
      },
      "source": [
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "@dataclass\n",
        "class ResearchResult:\n",
        "    \"\"\"Represents a single research result from a source.\"\"\"\n",
        "    title: str\n",
        "    url: str\n",
        "    snippet: str\n",
        "    source: str\n",
        "    relevance_score: float\n",
        "    timestamp: datetime\n",
        "\n",
        "class DeepResearchAgent:\n",
        "    \"\"\"\n",
        "    Comprehensive research agent using multiple search strategies and synthesis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, serpapi_key: Optional[str] = None, tavily_api_key: Optional[str] = None):\n",
        "        self.serpapi_key = serpapi_key or os.environ.get(\"SERPAPI_API_KEY\")\n",
        "        self.tavily_api_key = tavily_api_key or os.environ.get(\"TAVILY_API_KEY\")\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def run(self, state: Any) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Main research agent routine.\n",
        "        \"\"\"\n",
        "        print(\"---DEEP RESEARCH AGENT---\")\n",
        "        query = getattr(state, \"query\", \"\")\n",
        "        try:\n",
        "            search_results = self._multi_source_search(query)\n",
        "            detailed_content = self._extract_detailed_content(search_results[:5])\n",
        "            research_summary = self._synthesize_research(query, detailed_content)\n",
        "            insights = self._generate_insights(query, research_summary)\n",
        "\n",
        "            research_data = {\n",
        "                \"query\": query,\n",
        "                \"search_results\": [r.__dict__ for r in search_results],\n",
        "                \"detailed_content\": detailed_content,\n",
        "                \"summary\": research_summary,\n",
        "                \"insights\": insights,\n",
        "                \"sources_count\": len(search_results),\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            print(f\"Research complete for: {query}\")\n",
        "            print(f\"Found {len(search_results)} sources\")\n",
        "            print(f\"Extracted content from {len(detailed_content)} pages\")\n",
        "\n",
        "            return {\"research_data\": research_data}\n",
        "        except Exception as e:\n",
        "            print(f\"Research error: {str(e)}\")\n",
        "            return {\n",
        "                \"research_data\": {\n",
        "                    \"query\": query,\n",
        "                    \"error\": str(e),\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def _multi_source_search(self, query: str, num_results: int = 10) -> List[ResearchResult]:\n",
        "        \"\"\"Searches using multiple APIs and aggregates the results.\"\"\"\n",
        "        all_results: List[ResearchResult] = []\n",
        "\n",
        "        # SERPAPI\n",
        "        if self.serpapi_key:\n",
        "            try:\n",
        "                results = self._search_with_serpapi(query, num_results)\n",
        "                all_results.extend(results)\n",
        "                print(f\"SERPAPI: Found {len(results)} results\")\n",
        "            except Exception as e:\n",
        "                print(f\"SERPAPI error: {e}\")\n",
        "\n",
        "        # TAVILY\n",
        "        if self.tavily_api_key and len(all_results) < num_results:\n",
        "            try:\n",
        "                remaining = num_results - len(all_results)\n",
        "                results = self._search_with_tavily(query, remaining)\n",
        "                all_results.extend(results)\n",
        "                print(f\"Tavily: Found {len(results)} results\")\n",
        "            except Exception as e:\n",
        "                print(f\"Tavily error: {e}\")\n",
        "\n",
        "        # DuckDuckGo Fallback\n",
        "        if len(all_results) < 3:\n",
        "            try:\n",
        "                results = self._fallback_web_search(query, num_results - len(all_results))\n",
        "                all_results.extend(results)\n",
        "                print(f\"Fallback search: Found {len(results)} results\")\n",
        "            except Exception as e:\n",
        "                print(f\"Fallback error: {e}\")\n",
        "\n",
        "        # Deduplicate and rank\n",
        "        unique_results = self._deduplicate_results(all_results)\n",
        "        ranked_results = self._rank_results(query, unique_results)\n",
        "\n",
        "        return ranked_results[:num_results]\n",
        "\n",
        "    def _search_with_serpapi(self, query: str, num_results: int) -> List[ResearchResult]:\n",
        "        \"\"\"Search using SERPAPI (Google Search).\"\"\"\n",
        "        try:\n",
        "            from serpapi import GoogleSearch\n",
        "            params = {\n",
        "                \"engine\": \"google\",\n",
        "                \"q\": query,\n",
        "                \"num\": num_results,\n",
        "                \"api_key\": self.serpapi_key\n",
        "            }\n",
        "            search = GoogleSearch(params)\n",
        "            results = search.get_dict()\n",
        "\n",
        "            research_results = []\n",
        "            for result in results.get(\"organic_results\", []):\n",
        "                research_results.append(ResearchResult(\n",
        "                    title=result.get(\"title\", \"\"),\n",
        "                    url=result.get(\"link\", \"\"),\n",
        "                    snippet=result.get(\"snippet\", \"\"),\n",
        "                    source=\"SERPAPI\",\n",
        "                    relevance_score=self._calculate_relevance(query, result.get(\"snippet\", \"\")),\n",
        "                    timestamp=datetime.now()\n",
        "                ))\n",
        "\n",
        "            for result in results.get(\"news_results\", [])[:3]:\n",
        "                research_results.append(ResearchResult(\n",
        "                    title=result.get(\"title\", \"\"),\n",
        "                    url=result.get(\"link\", \"\"),\n",
        "                    snippet=result.get(\"snippet\", \"\"),\n",
        "                    source=\"SERPAPI_NEWS\",\n",
        "                    relevance_score=self._calculate_relevance(query, result.get(\"snippet\", \"\")),\n",
        "                    timestamp=datetime.now()\n",
        "                ))\n",
        "            return research_results\n",
        "        except ImportError:\n",
        "            print(\"SERPAPI library not installed. Install with: pip install google-search-results\")\n",
        "            return []\n",
        "\n",
        "    def _search_with_tavily(self, query: str, num_results: int) -> List[ResearchResult]:\n",
        "        \"\"\"Search using Tavily API.\"\"\"\n",
        "        try:\n",
        "            url = \"https://api.tavily.com/search\"\n",
        "            payload = {\n",
        "                \"api_key\": self.tavily_api_key,\n",
        "                \"query\": query,\n",
        "                \"max_results\": num_results,\n",
        "                \"search_depth\": \"advanced\",\n",
        "                \"include_answer\": True,\n",
        "                \"include_raw_content\": True\n",
        "            }\n",
        "            response = requests.post(url, json=payload, timeout=15)\n",
        "            results = response.json()\n",
        "\n",
        "            return [\n",
        "                ResearchResult(\n",
        "                    title=result.get(\"title\", \"\"),\n",
        "                    url=result.get(\"url\", \"\"),\n",
        "                    snippet=result.get(\"content\", \"\")[:500],\n",
        "                    source=\"TAVILY\",\n",
        "                    relevance_score=result.get(\"score\", 0.5),\n",
        "                    timestamp=datetime.now()\n",
        "                )\n",
        "                for result in results.get(\"results\", [])\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            print(f\"Tavily error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _fallback_web_search(self, query: str, num_results: int) -> List[ResearchResult]:\n",
        "        \"\"\"Fallback search using DuckDuckGo Instant Answer API.\"\"\"\n",
        "        results = []\n",
        "        try:\n",
        "            ddg_url = f\"https://api.duckduckgo.com/?q={query}&format=json&no_html=1&skip_disambig=1\"\n",
        "            response = requests.get(ddg_url, timeout=10)\n",
        "            data = response.json()\n",
        "            if data.get(\"AbstractText\"):\n",
        "                results.append(ResearchResult(\n",
        "                    title=data.get(\"Heading\", query),\n",
        "                    url=data.get(\"AbstractURL\", \"\"),\n",
        "                    snippet=data.get(\"AbstractText\", \"\"),\n",
        "                    source=\"DUCKDUCKGO\",\n",
        "                    relevance_score=0.8,\n",
        "                    timestamp=datetime.now()\n",
        "                ))\n",
        "            for topic in data.get(\"RelatedTopics\", [])[:3]:\n",
        "                if isinstance(topic, dict) and topic.get(\"Text\"):\n",
        "                    results.append(ResearchResult(\n",
        "                        title=f\"Related: {topic.get('Text', '')[:100]}\",\n",
        "                        url=topic.get(\"FirstURL\", \"\"),\n",
        "                        snippet=topic.get(\"Text\", \"\"),\n",
        "                        source=\"DUCKDUCKGO_RELATED\",\n",
        "                        relevance_score=0.6,\n",
        "                        timestamp=datetime.now()\n",
        "                    ))\n",
        "        except Exception as e:\n",
        "            print(f\"DuckDuckGo error: {e}\")\n",
        "        return results[:num_results]\n",
        "\n",
        "    def _extract_detailed_content(self, results: List[ResearchResult]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extracts and processes main content from result URLs.\"\"\"\n",
        "        detailed_content = []\n",
        "        for result in results:\n",
        "            try:\n",
        "                response = self.session.get(result.url, timeout=10)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                content = self._extract_main_content(soup)\n",
        "\n",
        "                detailed_content.append({\n",
        "                    \"title\": result.title,\n",
        "                    \"url\": result.url,\n",
        "                    \"content\": content,\n",
        "                    \"word_count\": len(content.split()),\n",
        "                    \"source\": result.source,\n",
        "                    \"extracted_at\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                time.sleep(1)  # Rate limiting\n",
        "            except Exception as e:\n",
        "                print(f\"Content extraction error for {result.url}: {e}\")\n",
        "                continue\n",
        "        return detailed_content\n",
        "\n",
        "    def _extract_main_content(self, soup: BeautifulSoup) -> str:\n",
        "        \"\"\"Extracts main textual content of a web page.\"\"\"\n",
        "        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
        "            element.decompose()\n",
        "\n",
        "        main_content = \"\"\n",
        "        selectors = [\n",
        "            'article', 'main', '[role=\"main\"]', '.content', '#content',\n",
        "            '.post-content', '.entry-content', '.article-content'\n",
        "        ]\n",
        "        for selector in selectors:\n",
        "            content_elem = soup.select_one(selector)\n",
        "            if content_elem:\n",
        "                main_content = content_elem.get_text(separator=' ', strip=True)\n",
        "                break\n",
        "\n",
        "        if not main_content:\n",
        "            main_content = soup.body.get_text(separator=' ', strip=True) if soup.body else soup.get_text(separator=' ', strip=True)\n",
        "        main_content = re.sub(r'\\s+', ' ', main_content)\n",
        "        return main_content[:5000]\n",
        "\n",
        "    def _synthesize_research(self, query: str, detailed_content: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Synthesizes research findings into a summary.\"\"\"\n",
        "        if not detailed_content:\n",
        "            return \"No detailed content available for synthesis.\"\n",
        "        all_text = \" \".join([item[\"content\"] for item in detailed_content])\n",
        "        words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            if len(word) > 4:\n",
        "                word_freq[word] = word_freq.get(word, 0) + 1\n",
        "        top_themes = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        summary = [\n",
        "            f\"Research Summary for: {query}\",\n",
        "            f\"Key Sources: {len(detailed_content)} websites analyzed\",\n",
        "            f\"Total Content: {len(all_text)} characters\",\n",
        "            \"Top Themes Identified: \" + ', '.join([theme[0] for theme in top_themes[:5]]),\n",
        "            \"Research Findings:\"\n",
        "        ]\n",
        "        for i, item in enumerate(detailed_content[:3], 1):\n",
        "            excerpt = item[\"content\"][:300] + \"...\" if len(item[\"content\"]) > 300 else item[\"content\"]\n",
        "            summary.append(f\"\\n{i}. From {item['title']}:\\n{excerpt}\")\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def _generate_insights(self, query: str, research_summary: str) -> List[str]:\n",
        "        \"\"\"Creates action-oriented insights based on the research summary.\"\"\"\n",
        "        insights = [\n",
        "            f\"Research on '{query}' reveals multiple perspectives.\",\n",
        "            \"Key findings are synthesized from credible sources.\",\n",
        "            \"Consider consulting additional specialized sources.\",\n",
        "            \"Monitor for updates; the field evolves rapidly.\"\n",
        "        ]\n",
        "        if len(research_summary) > 1000:\n",
        "            insights.append(\"Comprehensive data foundâ€”consider deeper analysis.\")\n",
        "        if any(time_kw in query.lower() for time_kw in [\"recent\", \"2024\", \"2025\"]):\n",
        "            insights.append(\"Query is time-sensitiveâ€”results reflect current info.\")\n",
        "        return insights\n",
        "\n",
        "    def _calculate_relevance(self, query: str, content: str) -> float:\n",
        "        \"\"\"Calculates a relevance score for query/content overlap.\"\"\"\n",
        "        if not content:\n",
        "            return 0.0\n",
        "        query_words = set(query.lower().split())\n",
        "        content_words = set(content.lower().split())\n",
        "        intersection = query_words & content_words\n",
        "        union = query_words | content_words\n",
        "        return len(intersection) / len(union) if union else 0.0\n",
        "\n",
        "    def _deduplicate_results(self, results: List[ResearchResult]) -> List[ResearchResult]:\n",
        "        \"\"\"Eliminates duplicate results by URL.\"\"\"\n",
        "        unique, seen_urls = [], set()\n",
        "        for result in results:\n",
        "            if result.url not in seen_urls:\n",
        "                unique.append(result)\n",
        "                seen_urls.add(result.url)\n",
        "        return unique\n",
        "\n",
        "    def _rank_results(self, query: str, results: List[ResearchResult]) -> List[ResearchResult]:\n",
        "        \"\"\"Ranks results by relevance score.\"\"\"\n",
        "        return sorted(results, key=lambda r: r.relevance_score, reverse=True)\n",
        "\n",
        "# Agent factory\n",
        "def create_research_agent():\n",
        "    \"\"\"Creates and configures the DeepResearchAgent.\"\"\"\n",
        "    agent = DeepResearchAgent(\n",
        "        serpapi_key=os.environ.get(\"SERPAPI_API_KEY\"),\n",
        "        tavily_api_key=os.environ.get(\"TAVILY_API_KEY\")\n",
        "    )\n",
        "    return agent.run\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    class MockState:\n",
        "        def __init__(self, query):\n",
        "            self.query = query\n",
        "    research_agent_func = create_research_agent()\n",
        "    test_state = MockState(\"artificial intelligence trends 2025\")\n",
        "    result = research_agent_func(test_state)\n",
        "    print(json.dumps(result, indent=2, default=str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aeb9e0b"
      },
      "source": [
        "# SEO Blog Writer Agent - Optimized for Google Colab\n",
        "# Install required packages in Colab:\n",
        "# !pip install openai google-generativeai transformers torch accelerate langchain beautifulsoup4 requests nltk\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "# Suppress warnings for cleaner Colab output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data (run once in Colab)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "@dataclass\n",
        "class SEOMetrics:\n",
        "    \"\"\"Structure for SEO optimization metrics\"\"\"\n",
        "    keyword_density: float\n",
        "    readability_score: float\n",
        "    word_count: int\n",
        "    heading_structure: Dict[str, int]\n",
        "    meta_description: str\n",
        "    title_tag: str\n",
        "\n",
        "class SEOBlogWriterAgent:\n",
        "    \"\"\"\n",
        "    Advanced SEO Blog Writer Agent with multiple LLM support and optimization features\n",
        "    Optimized for Google Colab environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 llm_provider: str = \"openai\",\n",
        "                 model_name: str = \"gpt-3.5-turbo\",\n",
        "                 api_key: str = None,\n",
        "                 max_length: int = 3000,\n",
        "                 temperature: float = 0.7):\n",
        "        \"\"\"\n",
        "        Initialize the SEO Blog Writer Agent\n",
        "\n",
        "        Args:\n",
        "            llm_provider: \"openai\", \"google\", or \"huggingface\"\n",
        "            model_name: Specific model to use\n",
        "            api_key: API key for the chosen provider\n",
        "            max_length: Maximum blog post length\n",
        "            temperature: Creativity level (0.0-1.0)\n",
        "        \"\"\"\n",
        "        self.llm_provider = llm_provider.lower()\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key or self._get_api_key()\n",
        "        self.max_length = max_length\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Initialize the chosen LLM\n",
        "        self._initialize_llm()\n",
        "\n",
        "        # SEO optimization settings\n",
        "        self.target_keyword_density = 0.02  # 2%\n",
        "        self.min_word_count = 800\n",
        "        self.max_word_count = 3000\n",
        "\n",
        "        print(f\"âœ… SEO Blog Writer initialized with {llm_provider} ({model_name})\")\n",
        "\n",
        "    def _get_api_key(self) -> str:\n",
        "        \"\"\"Get API key from environment variables\"\"\"\n",
        "        if self.llm_provider == \"openai\":\n",
        "            return os.getenv(\"OPENAI_API_KEY\")\n",
        "        elif self.llm_provider == \"google\":\n",
        "            return os.getenv(\"GOOGLE_API_KEY\")\n",
        "        return None\n",
        "\n",
        "    def _initialize_llm(self):\n",
        "        \"\"\"Initialize the selected LLM\"\"\"\n",
        "        try:\n",
        "            if self.llm_provider == \"openai\":\n",
        "                import openai\n",
        "                self.client = openai.OpenAI(api_key=self.api_key)\n",
        "\n",
        "            elif self.llm_provider == \"google\":\n",
        "                import google.generativeai as genai\n",
        "                genai.configure(api_key=self.api_key)\n",
        "                self.client = genai.GenerativeModel(self.model_name or 'gemini-pro')\n",
        "\n",
        "            elif self.llm_provider == \"huggingface\":\n",
        "                from transformers import pipeline\n",
        "                self.client = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=self.model_name or \"microsoft/DialoGPT-medium\",\n",
        "                    device=-1  # Use CPU for Colab compatibility\n",
        "                )\n",
        "\n",
        "            print(f\"âœ… {self.llm_provider} LLM initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error initializing {self.llm_provider}: {e}\")\n",
        "            print(\"ðŸ’¡ Falling back to template-based generation\")\n",
        "            self.client = None\n",
        "\n",
        "    def seo_blog_writer_agent(self, state) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Main SEO blog writer function - creates optimized long-form content\n",
        "        \"\"\"\n",
        "        print(\"---SEO BLOG WRITER AGENT---\")\n",
        "\n",
        "        query = getattr(state, 'query', '')\n",
        "        research_data = getattr(state, 'research_data', {})\n",
        "\n",
        "        if not research_data:\n",
        "            print(\"âŒ No research data available. Cannot generate blog content.\")\n",
        "            return {\"blog_content\": \"Error: No research data available for blog generation.\"}\n",
        "\n",
        "        print(f\"ðŸš€ Generating SEO-optimized blog for: '{query}'\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Extract key information from research\n",
        "            extracted_info = self._extract_research_insights(research_data, query)\n",
        "\n",
        "            # Step 2: Generate SEO-optimized content\n",
        "            blog_content = self._generate_blog_content(query, extracted_info)\n",
        "\n",
        "            # Step 3: Optimize for SEO\n",
        "            optimized_content = self._optimize_for_seo(blog_content, query)\n",
        "\n",
        "            # Step 4: Calculate SEO metrics\n",
        "            seo_metrics = self._calculate_seo_metrics(optimized_content, query)\n",
        "\n",
        "            # Step 5: Generate meta data\n",
        "            meta_data = self._generate_meta_data(optimized_content, query)\n",
        "\n",
        "            result = {\n",
        "                \"blog_content\": optimized_content,\n",
        "                \"seo_metrics\": seo_metrics.__dict__,\n",
        "                \"meta_data\": meta_data,\n",
        "                \"word_count\": len(optimized_content.split()),\n",
        "                \"generated_at\": datetime.now().isoformat(),\n",
        "                \"query\": query\n",
        "            }\n",
        "\n",
        "            print(f\"âœ… Blog generation complete!\")\n",
        "            print(f\"ðŸ“Š Word count: {result['word_count']}\")\n",
        "            print(f\"ðŸŽ¯ SEO Score: {seo_metrics.readability_score:.1f}/100\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating blog content: {str(e)}\"\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return {\"blog_content\": error_msg, \"error\": str(e)}\n",
        "\n",
        "    def _extract_research_insights(self, research_data: Dict, query: str) -> Dict:\n",
        "        \"\"\"Extract key insights from research data\"\"\"\n",
        "        insights = {\n",
        "            \"key_points\": [],\n",
        "            \"sources\": [],\n",
        "            \"statistics\": [],\n",
        "            \"quotes\": [],\n",
        "            \"related_topics\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Extract from search results\n",
        "            if isinstance(research_data, dict):\n",
        "                search_results = research_data.get('search_results', [])\n",
        "                detailed_content = research_data.get('detailed_content', [])\n",
        "                summary = research_data.get('summary', '')\n",
        "\n",
        "                # Process search results\n",
        "                for result in search_results[:5]:\n",
        "                    if hasattr(result, 'snippet'):\n",
        "                        insights[\"key_points\"].append(result.snippet)\n",
        "                        insights[\"sources\"].append(result.url)\n",
        "                    elif isinstance(result, dict):\n",
        "                        insights[\"key_points\"].append(result.get('snippet', ''))\n",
        "                        insights[\"sources\"].append(result.get('url', ''))\n",
        "\n",
        "                # Process detailed content\n",
        "                for content in detailed_content[:3]:\n",
        "                    if isinstance(content, dict):\n",
        "                        text = content.get('content', '')\n",
        "                        insights[\"key_points\"].append(text[:500])  # First 500 chars\n",
        "\n",
        "                # Extract statistics (numbers and percentages)\n",
        "                if summary:\n",
        "                    stats = re.findall(r'\\d+(?:\\.\\d+)?%?', summary)\n",
        "                    insights[\"statistics\"] = stats[:5]\n",
        "\n",
        "                # Extract potential quotes\n",
        "                quotes = re.findall(r'\"([^\"]*)\"', str(research_data))\n",
        "                insights[\"quotes\"] = [q for q in quotes if len(q.split()) > 5][:3]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Warning: Could not extract insights: {e}\")\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def _generate_blog_content(self, query: str, insights: Dict) -> str:\n",
        "        \"\"\"Generate blog content using the selected LLM\"\"\"\n",
        "\n",
        "        # Create comprehensive prompt\n",
        "        prompt = self._create_blog_prompt(query, insights)\n",
        "\n",
        "        try:\n",
        "            if self.client is None:\n",
        "                return self._generate_template_content(query, insights)\n",
        "\n",
        "            if self.llm_provider == \"openai\":\n",
        "                return self._generate_with_openai(prompt)\n",
        "            elif self.llm_provider == \"google\":\n",
        "                return self._generate_with_google(prompt)\n",
        "            elif self.llm_provider == \"huggingface\":\n",
        "                return self._generate_with_huggingface(prompt)\n",
        "            else:\n",
        "                return self._generate_template_content(query, insights)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ LLM generation failed: {e}\")\n",
        "            return self._generate_template_content(query, insights)\n",
        "\n",
        "    def _create_blog_prompt(self, query: str, insights: Dict) -> str:\n",
        "        \"\"\"Create optimized prompt for blog generation\"\"\"\n",
        "\n",
        "        key_points = '\\n'.join(insights.get('key_points', [])[:5])\n",
        "        statistics = ', '.join(insights.get('statistics', [])[:3])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Write a comprehensive, SEO-optimized blog post about \"{query}\".\n",
        "\n",
        "REQUIREMENTS:\n",
        "- 1500-2500 words\n",
        "- Professional, engaging tone\n",
        "- Include H1, H2, H3 headings\n",
        "- Use bullet points and numbered lists\n",
        "- Include a compelling introduction and conclusion\n",
        "- Integrate relevant statistics and data points\n",
        "- Write for general audience but include expert insights\n",
        "\n",
        "RESEARCH DATA TO INCORPORATE:\n",
        "{key_points}\n",
        "\n",
        "STATISTICS TO INCLUDE:\n",
        "{statistics}\n",
        "\n",
        "STRUCTURE:\n",
        "1. Compelling H1 title with main keyword\n",
        "2. Engaging introduction (hook + preview)\n",
        "3. 4-6 main sections with H2 headings\n",
        "4. Subsections with H3 headings where relevant\n",
        "5. Actionable insights and takeaways\n",
        "6. Strong conclusion with call-to-action\n",
        "\n",
        "SEO GUIDELINES:\n",
        "- Use \"{query}\" naturally throughout (2% keyword density)\n",
        "- Include related keywords and synonyms\n",
        "- Write meta description under 160 characters\n",
        "- Use semantic HTML structure\n",
        "- Include internal linking opportunities\n",
        "\n",
        "Write the complete blog post now:\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _generate_with_openai(self, prompt: str) -> str:\n",
        "        \"\"\"Generate content using OpenAI\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert SEO content writer specializing in creating engaging, search-optimized blog posts.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=self.max_length,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"OpenAI generation error: {e}\")\n",
        "\n",
        "    def _generate_with_google(self, prompt: str) -> str:\n",
        "        \"\"\"Generate content using Google Gemini\"\"\"\n",
        "        try:\n",
        "            response = self.client.generate_content(\n",
        "                prompt,\n",
        "                generation_config={\n",
        "                    'temperature': self.temperature,\n",
        "                    'max_output_tokens': self.max_length,\n",
        "                }\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Google AI generation error: {e}\")\n",
        "\n",
        "    def _generate_with_huggingface(self, prompt: str) -> str:\n",
        "        \"\"\"Generate content using HuggingFace\"\"\"\n",
        "        try:\n",
        "            # Truncate prompt for local models\n",
        "            truncated_prompt = prompt[:1000]\n",
        "            response = self.client(\n",
        "                truncated_prompt,\n",
        "                max_length=min(self.max_length, 2048),  # Local model limitation\n",
        "                temperature=self.temperature,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "            return response[0]['generated_text']\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"HuggingFace generation error: {e}\")\n",
        "\n",
        "    def _generate_template_content(self, query: str, insights: Dict) -> str:\n",
        "        \"\"\"Generate template-based content when LLM is unavailable\"\"\"\n",
        "\n",
        "        key_points = insights.get('key_points', ['Key insights about the topic'])[:3]\n",
        "        statistics = insights.get('statistics', ['Recent data shows growth'])[:2]\n",
        "\n",
        "        content = f\"\"\"# The Complete Guide to {query.title()}\n",
        "\n",
        "## Introduction\n",
        "\n",
        "{query.title()} has become increasingly important in today's digital landscape. This comprehensive guide explores the key aspects, latest trends, and practical insights you need to understand this topic thoroughly.\n",
        "\n",
        "## What is {query.title()}?\n",
        "\n",
        "{key_points[0] if key_points else f\"{query.title()} represents a significant area of interest with multiple applications and implications.\"}\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "- **Innovation**: Continuous evolution and improvement\n",
        "- **Impact**: Wide-reaching effects across industries\n",
        "- **Opportunities**: New possibilities for growth and development\n",
        "\n",
        "## Current Trends and Statistics\n",
        "\n",
        "Recent research reveals important insights about {query}:\n",
        "\n",
        "- {statistics[0] if statistics else \"Growth has been steady over recent years\"}\n",
        "- {statistics[1] if len(statistics) > 1 else \"Market adoption continues to expand\"}\n",
        "\n",
        "### Market Analysis\n",
        "\n",
        "{key_points[1] if len(key_points) > 1 else f\"The {query} market shows promising indicators for continued expansion and development.\"}\n",
        "\n",
        "## Best Practices and Implementation\n",
        "\n",
        "### Getting Started\n",
        "\n",
        "1. **Research and Planning**: Understand your specific needs\n",
        "2. **Strategy Development**: Create a clear roadmap\n",
        "3. **Implementation**: Execute with careful monitoring\n",
        "4. **Optimization**: Continuously improve based on results\n",
        "\n",
        "### Common Challenges\n",
        "\n",
        "{key_points[2] if len(key_points) > 2 else \"While opportunities abound, careful consideration of challenges ensures successful outcomes.\"}\n",
        "\n",
        "## Future Outlook\n",
        "\n",
        "The future of {query} looks promising with continued innovation and adoption across various sectors. Organizations that adapt early will likely see the greatest benefits.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "{query.title()} represents a significant opportunity for those who understand its implications and applications. By staying informed about trends, best practices, and emerging developments, you can position yourself for success in this evolving landscape.\n",
        "\n",
        "**Ready to get started?** Begin by assessing your current situation and identifying areas where {query} can make the biggest impact.\n",
        "\n",
        "---\n",
        "\n",
        "*This comprehensive guide provides essential insights into {query}. For more detailed information and personalized advice, consider consulting with industry experts.*\n",
        "\"\"\"\n",
        "\n",
        "        return content\n",
        "\n",
        "    def _optimize_for_seo(self, content: str, query: str) -> str:\n",
        "        \"\"\"Apply SEO optimizations to the content\"\"\"\n",
        "\n",
        "        optimized = content\n",
        "\n",
        "        # Ensure proper heading structure\n",
        "        if not re.search(r'^#\\s', optimized, re.MULTILINE):\n",
        "            optimized = f\"# {query.title()}: Complete Guide\\n\\n\" + optimized\n",
        "\n",
        "        # Add keyword variations naturally\n",
        "        keyword_variations = self._generate_keyword_variations(query)\n",
        "\n",
        "        # Insert keywords naturally (avoid keyword stuffing)\n",
        "        current_density = self._calculate_keyword_density(optimized, query)\n",
        "        target_insertions = max(0, int(len(optimized.split()) * self.target_keyword_density) - current_density)\n",
        "\n",
        "        # Add FAQ section for long-tail keywords\n",
        "        if \"how\" in query.lower() or \"what\" in query.lower():\n",
        "            faq_section = self._generate_faq_section(query, keyword_variations)\n",
        "            optimized += f\"\\n\\n## Frequently Asked Questions\\n\\n{faq_section}\"\n",
        "\n",
        "        # Ensure proper content structure\n",
        "        optimized = self._ensure_content_structure(optimized)\n",
        "\n",
        "        return optimized\n",
        "\n",
        "    def _generate_keyword_variations(self, query: str) -> List[str]:\n",
        "        \"\"\"Generate keyword variations and related terms\"\"\"\n",
        "        variations = []\n",
        "        words = query.lower().split()\n",
        "\n",
        "        # Add plurals\n",
        "        if not query.endswith('s'):\n",
        "            variations.append(query + 's')\n",
        "\n",
        "        # Add question formats\n",
        "        variations.extend([\n",
        "            f\"what is {query}\",\n",
        "            f\"how to {query}\",\n",
        "            f\"{query} guide\",\n",
        "            f\"{query} tips\",\n",
        "            f\"best {query}\"\n",
        "        ])\n",
        "\n",
        "        # Add context variations\n",
        "        if len(words) > 1:\n",
        "            variations.extend([\n",
        "                f\"{query} strategy\",\n",
        "                f\"{query} implementation\",\n",
        "                f\"{query} benefits\"\n",
        "            ])\n",
        "\n",
        "        return variations[:5]  # Limit to top 5\n",
        "\n",
        "    def _generate_faq_section(self, query: str, variations: List[str]) -> str:\n",
        "        \"\"\"Generate FAQ section for additional keyword coverage\"\"\"\n",
        "\n",
        "        faqs = [\n",
        "            f\"**What is {query}?**\\n{query.title()} refers to a comprehensive approach that encompasses multiple aspects and considerations.\",\n",
        "            f\"**How does {query} work?**\\nThe process involves several key steps and considerations that work together to achieve optimal results.\",\n",
        "            f\"**What are the benefits of {query}?**\\nKey benefits include improved efficiency, better outcomes, and enhanced understanding of the topic.\"\n",
        "        ]\n",
        "\n",
        "        return '\\n\\n'.join(faqs)\n",
        "\n",
        "    def _ensure_content_structure(self, content: str) -> str:\n",
        "        \"\"\"Ensure proper content structure for SEO\"\"\"\n",
        "\n",
        "        lines = content.split('\\n')\n",
        "        structured_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Convert markdown headers to proper format\n",
        "            if line.startswith('# '):\n",
        "                structured_lines.append(line)\n",
        "            elif line.startswith('## '):\n",
        "                structured_lines.append(line)\n",
        "            elif line.startswith('### '):\n",
        "                structured_lines.append(line)\n",
        "            else:\n",
        "                structured_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(structured_lines)\n",
        "\n",
        "    def _calculate_seo_metrics(self, content: str, query: str) -> SEOMetrics:\n",
        "        \"\"\"Calculate comprehensive SEO metrics\"\"\"\n",
        "\n",
        "        words = content.split()\n",
        "        word_count = len(words)\n",
        "\n",
        "        # Keyword density\n",
        "        keyword_count = content.lower().count(query.lower())\n",
        "        keyword_density = keyword_count / word_count if word_count > 0 else 0\n",
        "\n",
        "        # Readability score (simplified Flesch formula)\n",
        "        sentences = content.count('.') + content.count('!') + content.count('?')\n",
        "        avg_sentence_length = word_count / max(sentences, 1)\n",
        "        readability = max(0, min(100, 206.835 - (1.015 * avg_sentence_length)))\n",
        "\n",
        "        # Heading structure\n",
        "        heading_structure = {\n",
        "            'h1': len(re.findall(r'^#\\s', content, re.MULTILINE)),\n",
        "            'h2': len(re.findall(r'^##\\s', content, re.MULTILINE)),\n",
        "            'h3': len(re.findall(r'^###\\s', content, re.MULTILINE))\n",
        "        }\n",
        "\n",
        "        # Generate meta description\n",
        "        first_paragraph = content.split('\\n\\n')[1] if '\\n\\n' in content else content[:160]\n",
        "        meta_description = re.sub(r'[#*]', '', first_paragraph)[:155] + \"...\"\n",
        "\n",
        "        # Generate title tag\n",
        "        title_match = re.search(r'^#\\s(.+)', content, re.MULTILINE)\n",
        "        title_tag = title_match.group(1) if title_match else f\"{query.title()} - Complete Guide\"\n",
        "\n",
        "        return SEOMetrics(\n",
        "            keyword_density=keyword_density,\n",
        "            readability_score=readability,\n",
        "            word_count=word_count,\n",
        "            heading_structure=heading_structure,\n",
        "            meta_description=meta_description,\n",
        "            title_tag=title_tag\n",
        "        )\n",
        "\n",
        "    def _calculate_keyword_density(self, content: str, keyword: str) -> int:\n",
        "        \"\"\"Calculate current keyword density\"\"\"\n",
        "        return content.lower().count(keyword.lower())\n",
        "\n",
        "    def _generate_meta_data(self, content: str, query: str) -> Dict[str, str]:\n",
        "        \"\"\"Generate SEO meta data\"\"\"\n",
        "\n",
        "        # Extract title\n",
        "        title_match = re.search(r'^#\\s(.+)', content, re.MULTILINE)\n",
        "        title = title_match.group(1) if title_match else f\"{query.title()} - Complete Guide\"\n",
        "\n",
        "        # Generate description\n",
        "        first_paragraph = content.split('\\n\\n')[1] if '\\n\\n' in content else content[:160]\n",
        "        description = re.sub(r'[#*]', '', first_paragraph)[:155] + \"...\"\n",
        "\n",
        "        # Generate keywords\n",
        "        words = query.split()\n",
        "        keywords = [query] + [word for word in words if len(word) > 3]\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"keywords\": \", \".join(keywords[:10]),\n",
        "            \"og_title\": title,\n",
        "            \"og_description\": description,\n",
        "            \"canonical_url\": f\"/blog/{query.replace(' ', '-').lower()}\"\n",
        "        }\n",
        "\n",
        "# Google Colab Optimization Functions\n",
        "def setup_colab_environment():\n",
        "    \"\"\"Set up the environment for Google Colab\"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Setting up Google Colab environment...\")\n",
        "\n",
        "    # Install required packages\n",
        "    packages = [\n",
        "        \"openai\",\n",
        "        \"google-generativeai\",\n",
        "        \"transformers\",\n",
        "        \"torch\",\n",
        "        \"nltk\",\n",
        "        \"beautifulsoup4\",\n",
        "        \"requests\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package.replace('-', '_'))\n",
        "            print(f\"âœ… {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"ðŸ“¦ Installing {package}...\")\n",
        "            os.system(f\"pip install {package}\")\n",
        "\n",
        "    # Download NLTK data\n",
        "    import nltk\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "    print(\"âœ… Environment setup complete!\")\n",
        "\n",
        "def create_seo_blog_writer(\n",
        "    llm_provider: str = \"openai\",\n",
        "    model_name: str = \"gpt-3.5-turbo\",\n",
        "    api_key: str = None\n",
        ") -> callable:\n",
        "    \"\"\"\n",
        "    Factory function to create SEO blog writer for different environments\n",
        "\n",
        "    Usage in Colab:\n",
        "    ```python\n",
        "    # Setup environment\n",
        "    setup_colab_environment()\n",
        "\n",
        "    # Set API key\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
        "\n",
        "    # Create writer\n",
        "    blog_writer = create_seo_blog_writer(\n",
        "        llm_provider=\"openai\",\n",
        "        model_name=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "    # Use in your workflow\n",
        "    result = blog_writer(state)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    agent = SEOBlogWriterAgent(\n",
        "        llm_provider=llm_provider,\n",
        "        model_name=model_name,\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    return agent.seo_blog_writer_agent\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Mock state for testing\n",
        "    class MockState:\n",
        "        def __init__(self, query, research_data=None):\n",
        "            self.query = query\n",
        "            self.research_data = research_data or {\n",
        "                \"search_results\": [\n",
        "                    {\"snippet\": \"AI is transforming industries\", \"url\": \"example.com\"},\n",
        "                    {\"snippet\": \"Machine learning advances rapidly\", \"url\": \"example2.com\"}\n",
        "                ],\n",
        "                \"summary\": \"AI technology shows 40% growth in adoption\"\n",
        "            }\n",
        "\n",
        "    # Test the agent\n",
        "    print(\"ðŸ§ª Testing SEO Blog Writer Agent...\")\n",
        "\n",
        "    # Create agent (will use template mode if no API key)\n",
        "    writer = create_seo_blog_writer(llm_provider=\"template\")  # Fallback mode\n",
        "\n",
        "    # Test with sample data\n",
        "    test_state = MockState(\n",
        "        query=\"artificial intelligence in healthcare\",\n",
        "        research_data={\n",
        "            \"search_results\": [\n",
        "                {\"snippet\": \"AI improves diagnostic accuracy by 25%\", \"url\": \"medical-ai.com\"},\n",
        "                {\"snippet\": \"Healthcare AI market grows 40% annually\", \"url\": \"health-tech.com\"}\n",
        "            ],\n",
        "            \"detailed_content\": [\n",
        "                {\"content\": \"Artificial intelligence in healthcare represents a paradigm shift...\"}\n",
        "            ],\n",
        "            \"summary\": \"AI healthcare applications show 25% improvement in accuracy\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Generate blog\n",
        "    result = writer(test_state)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š GENERATION RESULTS:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Word Count: {result.get('word_count', 0)}\")\n",
        "    if 'seo_metrics' in result:\n",
        "        metrics = result['seo_metrics']\n",
        "        print(f\"Keyword Density: {metrics.get('keyword_density', 0):.2%}\")\n",
        "        print(f\"Readability Score: {metrics.get('readability_score', 0):.1f}/100\")\n",
        "\n",
        "    print(f\"\\nðŸ“ First 500 characters of content:\")\n",
        "    content = result.get('blog_content', '')\n",
        "    print(content[:500] + \"...\" if len(content) > 500 else content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LinkedIn Post Writer Agent - Optimized for Google Colab\n",
        "# Install required packages in Colab:\n",
        "# !pip install openai google-generativeai transformers emoji textstat nltk requests beautifulsoup4\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "# Suppress warnings for cleaner Colab output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class LinkedInMetrics:\n",
        "    \"\"\"Structure for LinkedIn post engagement metrics\"\"\"\n",
        "    character_count: int\n",
        "    word_count: int\n",
        "    hashtag_count: int\n",
        "    emoji_count: int\n",
        "    engagement_score: float\n",
        "    readability_level: str\n",
        "    call_to_action: bool\n",
        "\n",
        "class LinkedInPostWriterAgent:\n",
        "    \"\"\"\n",
        "    Advanced LinkedIn Post Writer Agent with engagement optimization\n",
        "    Optimized for Google Colab environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 llm_provider: str = \"openai\",\n",
        "                 model_name: str = \"gpt-3.5-turbo\",\n",
        "                 api_key: str = None,\n",
        "                 post_style: str = \"professional\",\n",
        "                 target_length: str = \"medium\"):\n",
        "        \"\"\"\n",
        "        Initialize the LinkedIn Post Writer Agent\n",
        "\n",
        "        Args:\n",
        "            llm_provider: \"openai\", \"google\", \"huggingface\", or \"template\"\n",
        "            model_name: Specific model to use\n",
        "            api_key: API key for the chosen provider\n",
        "            post_style: \"professional\", \"casual\", \"thought_leadership\", \"personal\"\n",
        "            target_length: \"short\" (130 chars), \"medium\" (600 chars), \"long\" (1300 chars)\n",
        "        \"\"\"\n",
        "        self.llm_provider = llm_provider.lower()\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key or self._get_api_key()\n",
        "        self.post_style = post_style\n",
        "        self.target_length = target_length\n",
        "\n",
        "        # LinkedIn optimization settings\n",
        "        self.length_limits = {\n",
        "            \"short\": 130,\n",
        "            \"medium\": 600,\n",
        "            \"long\": 1300\n",
        "        }\n",
        "        self.optimal_hashtag_count = 5\n",
        "        self.engagement_triggers = [\n",
        "            \"What do you think?\",\n",
        "            \"Share your experience\",\n",
        "            \"Agree or disagree?\",\n",
        "            \"What's your take?\",\n",
        "            \"Drop your thoughts below\",\n",
        "            \"What would you add?\",\n",
        "            \"Have you experienced this?\",\n",
        "            \"What's been your experience?\"\n",
        "        ]\n",
        "\n",
        "        # Initialize LLM\n",
        "        self._initialize_llm()\n",
        "\n",
        "        print(f\"âœ… LinkedIn Post Writer initialized with {llm_provider} ({post_style} style)\")\n",
        "\n",
        "    def _get_api_key(self) -> str:\n",
        "        \"\"\"Get API key from environment variables\"\"\"\n",
        "        if self.llm_provider == \"openai\":\n",
        "            return os.getenv(\"OPENAI_API_KEY\")\n",
        "        elif self.llm_provider == \"google\":\n",
        "            return os.getenv(\"GOOGLE_API_KEY\")\n",
        "        return None\n",
        "\n",
        "    def _initialize_llm(self):\n",
        "        \"\"\"Initialize the selected LLM\"\"\"\n",
        "        try:\n",
        "            if self.llm_provider == \"openai\":\n",
        "                import openai\n",
        "                self.client = openai.OpenAI(api_key=self.api_key)\n",
        "\n",
        "            elif self.llm_provider == \"google\":\n",
        "                import google.generativeai as genai\n",
        "                genai.configure(api_key=self.api_key)\n",
        "                self.client = genai.GenerativeModel(self.model_name or 'gemini-pro')\n",
        "\n",
        "            elif self.llm_provider == \"huggingface\":\n",
        "                from transformers import pipeline\n",
        "                self.client = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=self.model_name or \"microsoft/DialoGPT-medium\",\n",
        "                    device=-1  # Use CPU for Colab\n",
        "                )\n",
        "\n",
        "            print(f\"âœ… {self.llm_provider} LLM initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error initializing {self.llm_provider}: {e}\")\n",
        "            print(\"ðŸ’¡ Falling back to template-based generation\")\n",
        "            self.client = None\n",
        "\n",
        "    def linkedin_post_writer_agent(self, state) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Main LinkedIn post writer function - generates engaging professional content\n",
        "        \"\"\"\n",
        "        print(\"---LINKEDIN POST WRITER AGENT---\")\n",
        "\n",
        "        query = getattr(state, 'query', '')\n",
        "        research_data = getattr(state, 'research_data', {})\n",
        "        blog_content = getattr(state, 'blog_content', '')\n",
        "\n",
        "        # Determine content source priority\n",
        "        content_to_summarize = self._select_content_source(blog_content, research_data)\n",
        "\n",
        "        if not content_to_summarize:\n",
        "            print(\"âŒ No content available to generate LinkedIn post.\")\n",
        "            return {\"linkedin_content\": \"Error: No content available to generate LinkedIn post.\"}\n",
        "\n",
        "        print(f\"ðŸš€ Generating LinkedIn post for: '{query}'\")\n",
        "        print(f\"ðŸ“Š Using {self._get_content_type(blog_content, research_data)} as source\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Extract key insights from content\n",
        "            key_insights = self._extract_key_insights(content_to_summarize, query)\n",
        "\n",
        "            # Step 2: Generate multiple post variations\n",
        "            post_variations = self._generate_post_variations(query, key_insights)\n",
        "\n",
        "            # Step 3: Select best performing variation\n",
        "            best_post = self._select_best_post(post_variations, query)\n",
        "\n",
        "            # Step 4: Optimize for engagement\n",
        "            optimized_post = self._optimize_for_engagement(best_post, query)\n",
        "\n",
        "            # Step 5: Add professional formatting\n",
        "            formatted_post = self._format_for_linkedin(optimized_post, query)\n",
        "\n",
        "            # Step 6: Calculate engagement metrics\n",
        "            metrics = self._calculate_engagement_metrics(formatted_post)\n",
        "\n",
        "            # Step 7: Generate post variations and hashtags\n",
        "            hashtags = self._generate_hashtags(query, key_insights)\n",
        "            variations = self._create_post_variations(formatted_post, hashtags)\n",
        "\n",
        "            result = {\n",
        "                \"linkedin_content\": formatted_post,\n",
        "                \"hashtags\": hashtags,\n",
        "                \"variations\": variations,\n",
        "                \"metrics\": metrics.__dict__,\n",
        "                \"post_style\": self.post_style,\n",
        "                \"target_length\": self.target_length,\n",
        "                \"engagement_tips\": self._generate_engagement_tips(),\n",
        "                \"generated_at\": datetime.now().isoformat(),\n",
        "                \"query\": query\n",
        "            }\n",
        "\n",
        "            print(f\"âœ… LinkedIn post generation complete!\")\n",
        "            print(f\"ðŸ“ Character count: {metrics.character_count}\")\n",
        "            print(f\"ðŸŽ¯ Engagement score: {metrics.engagement_score:.1f}/100\")\n",
        "            print(f\"ðŸ“± Style: {self.post_style} | Length: {self.target_length}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating LinkedIn post: {str(e)}\"\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return {\"linkedin_content\": error_msg, \"error\": str(e)}\n",
        "\n",
        "    def _select_content_source(self, blog_content: str, research_data: Dict) -> str:\n",
        "        \"\"\"Select the best content source for post generation\"\"\"\n",
        "\n",
        "        # Priority: blog_content > detailed research > research summary > search results\n",
        "        if blog_content and len(blog_content.strip()) > 100:\n",
        "            return blog_content[:2000]  # Use first 2000 chars for efficiency\n",
        "\n",
        "        if isinstance(research_data, dict):\n",
        "            # Try detailed content first\n",
        "            detailed_content = research_data.get('detailed_content', [])\n",
        "            if detailed_content:\n",
        "                combined_content = \"\"\n",
        "                for item in detailed_content[:2]:  # Use top 2 sources\n",
        "                    if isinstance(item, dict):\n",
        "                        combined_content += item.get('content', '')[:500] + \" \"\n",
        "                if combined_content.strip():\n",
        "                    return combined_content\n",
        "\n",
        "            # Fall back to summary\n",
        "            summary = research_data.get('summary', '')\n",
        "            if summary and len(summary.strip()) > 50:\n",
        "                return summary\n",
        "\n",
        "            # Last resort: search results\n",
        "            search_results = research_data.get('search_results', [])\n",
        "            if search_results:\n",
        "                combined_snippets = \"\"\n",
        "                for result in search_results[:3]:\n",
        "                    if hasattr(result, 'snippet'):\n",
        "                        combined_snippets += result.snippet + \" \"\n",
        "                    elif isinstance(result, dict):\n",
        "                        combined_snippets += result.get('snippet', '') + \" \"\n",
        "                return combined_snippets\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _get_content_type(self, blog_content: str, research_data: Dict) -> str:\n",
        "        \"\"\"Identify the type of content being used\"\"\"\n",
        "        if blog_content and len(blog_content.strip()) > 100:\n",
        "            return \"blog content\"\n",
        "        elif isinstance(research_data, dict) and research_data.get('detailed_content'):\n",
        "            return \"detailed research\"\n",
        "        elif isinstance(research_data, dict) and research_data.get('summary'):\n",
        "            return \"research summary\"\n",
        "        else:\n",
        "            return \"search results\"\n",
        "\n",
        "    def _extract_key_insights(self, content: str, query: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract key insights from content for LinkedIn post\"\"\"\n",
        "\n",
        "        insights = {\n",
        "            \"main_points\": [],\n",
        "            \"statistics\": [],\n",
        "            \"quotes\": [],\n",
        "            \"actionable_tips\": [],\n",
        "            \"trends\": [],\n",
        "            \"benefits\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Extract main points (first sentences of paragraphs)\n",
        "            paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
        "            for para in paragraphs[:5]:\n",
        "                sentences = para.split('.')\n",
        "                if sentences and len(sentences[0].strip()) > 20:\n",
        "                    insights[\"main_points\"].append(sentences[0].strip()[:200])\n",
        "\n",
        "            # Extract statistics and numbers\n",
        "            stats = re.findall(r'\\d+(?:\\.\\d+)?%|\\d+(?:,\\d{3})*(?:\\.\\d+)?(?:\\s*(?:billion|million|thousand|percent|%))', content, re.IGNORECASE)\n",
        "            insights[\"statistics\"] = stats[:3]\n",
        "\n",
        "            # Extract quotes\n",
        "            quotes = re.findall(r'\"([^\"]*)\"', content)\n",
        "            insights[\"quotes\"] = [q for q in quotes if 20 < len(q) < 150][:2]\n",
        "\n",
        "            # Find actionable content\n",
        "            actionable_patterns = [\n",
        "                r'how to\\s+([^.!?]*)',\n",
        "                r'tips?\\s+(?:for|to)\\s+([^.!?]*)',\n",
        "                r'steps?\\s+to\\s+([^.!?]*)',\n",
        "                r'ways?\\s+to\\s+([^.!?]*)'\n",
        "            ]\n",
        "\n",
        "            for pattern in actionable_patterns:\n",
        "                matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "                insights[\"actionable_tips\"].extend(matches[:2])\n",
        "\n",
        "            # Identify trends\n",
        "            trend_keywords = ['trend', 'growing', 'increasing', 'emerging', 'rising', 'future', 'innovation']\n",
        "            for keyword in trend_keywords:\n",
        "                pattern = rf'{keyword}[^.!?]*'\n",
        "                matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "                insights[\"trends\"].extend([m[:100] for m in matches[:1]])\n",
        "\n",
        "            # Extract benefits\n",
        "            benefit_patterns = [\n",
        "                r'benefit[s]?\\s+(?:of|include)\\s+([^.!?]*)',\n",
        "                r'advantage[s]?\\s+(?:of|include)\\s+([^.!?]*)',\n",
        "                r'helps?\\s+(?:to\\s+)?([^.!?]*)'\n",
        "            ]\n",
        "\n",
        "            for pattern in benefit_patterns:\n",
        "                matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "                insights[\"benefits\"].extend(matches[:2])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Warning: Could not extract insights: {e}\")\n",
        "\n",
        "        # Clean up insights\n",
        "        for key in insights:\n",
        "            insights[key] = [item.strip() for item in insights[key] if item.strip()]\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def _generate_post_variations(self, query: str, insights: Dict) -> List[str]:\n",
        "        \"\"\"Generate multiple post variations using different approaches\"\"\"\n",
        "\n",
        "        variations = []\n",
        "\n",
        "        # Generate using LLM if available\n",
        "        if self.client:\n",
        "            try:\n",
        "                llm_variations = self._generate_with_llm(query, insights)\n",
        "                variations.extend(llm_variations)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ LLM generation failed: {e}\")\n",
        "\n",
        "        # Always generate template variations as backup\n",
        "        template_variations = self._generate_template_variations(query, insights)\n",
        "        variations.extend(template_variations)\n",
        "\n",
        "        return variations[:5]  # Return top 5 variations\n",
        "\n",
        "    def _generate_with_llm(self, query: str, insights: Dict) -> List[str]:\n",
        "        \"\"\"Generate variations using the selected LLM\"\"\"\n",
        "\n",
        "        prompt = self._create_linkedin_prompt(query, insights)\n",
        "        variations = []\n",
        "\n",
        "        try:\n",
        "            if self.llm_provider == \"openai\":\n",
        "                variations = self._generate_openai_variations(prompt)\n",
        "            elif self.llm_provider == \"google\":\n",
        "                variations = self._generate_google_variations(prompt)\n",
        "            elif self.llm_provider == \"huggingface\":\n",
        "                variations = self._generate_huggingface_variations(prompt)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ LLM variation generation failed: {e}\")\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def _create_linkedin_prompt(self, query: str, insights: Dict) -> str:\n",
        "        \"\"\"Create optimized prompt for LinkedIn post generation\"\"\"\n",
        "\n",
        "        main_points = '. '.join(insights.get('main_points', [])[:3])\n",
        "        statistics = ', '.join(insights.get('statistics', [])[:2])\n",
        "        tips = '. '.join(insights.get('actionable_tips', [])[:2])\n",
        "\n",
        "        style_instructions = {\n",
        "            \"professional\": \"Write in a professional, authoritative tone with industry expertise.\",\n",
        "            \"casual\": \"Write in a conversational, approachable tone with personal touches.\",\n",
        "            \"thought_leadership\": \"Write as an industry thought leader sharing insights and predictions.\",\n",
        "            \"personal\": \"Write from personal experience with authentic storytelling.\"\n",
        "        }\n",
        "\n",
        "        length_instructions = {\n",
        "            \"short\": \"Keep it under 130 characters for maximum engagement.\",\n",
        "            \"medium\": \"Aim for 400-600 characters with good detail.\",\n",
        "            \"long\": \"Write 800-1300 characters with comprehensive insights.\"\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Create 3 engaging LinkedIn posts about \"{query}\".\n",
        "\n",
        "STYLE: {style_instructions.get(self.post_style, 'Professional and engaging')}\n",
        "LENGTH: {length_instructions.get(self.target_length, 'Medium length')}\n",
        "\n",
        "KEY INFORMATION TO INCLUDE:\n",
        "- Main insights: {main_points}\n",
        "- Statistics: {statistics}\n",
        "- Actionable tips: {tips}\n",
        "\n",
        "REQUIREMENTS:\n",
        "1. Start with a hook that grabs attention\n",
        "2. Include relevant emojis (2-4 maximum)\n",
        "3. End with an engaging question or call-to-action\n",
        "4. Make it scannable with line breaks\n",
        "5. Include industry-relevant insights\n",
        "6. Sound authentic and conversational\n",
        "\n",
        "LINKEDIN BEST PRACTICES:\n",
        "- Use \"you\" to directly address the audience\n",
        "- Include social proof or credibility indicators\n",
        "- Make it actionable and valuable\n",
        "- Create curiosity or urgency\n",
        "- Use power words that drive engagement\n",
        "\n",
        "Generate 3 different variations with different angles and approaches:\n",
        "\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _generate_openai_variations(self, prompt: str) -> List[str]:\n",
        "        \"\"\"Generate variations using OpenAI\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a LinkedIn content expert who creates viral, engaging professional posts that drive high engagement.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=800,\n",
        "                temperature=0.8,\n",
        "                n=3  # Generate 3 variations\n",
        "            )\n",
        "\n",
        "            return [choice.message.content.strip() for choice in response.choices]\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"OpenAI generation error: {e}\")\n",
        "\n",
        "    def _generate_google_variations(self, prompt: str) -> List[str]:\n",
        "        \"\"\"Generate variations using Google Gemini\"\"\"\n",
        "        try:\n",
        "            variations = []\n",
        "            for i in range(3):\n",
        "                response = self.client.generate_content(\n",
        "                    prompt + f\" (Variation {i+1})\",\n",
        "                    generation_config={\n",
        "                        'temperature': 0.7 + (i * 0.1),  # Vary creativity\n",
        "                        'max_output_tokens': 500,\n",
        "                    }\n",
        "                )\n",
        "                variations.append(response.text.strip())\n",
        "\n",
        "            return variations\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Google AI generation error: {e}\")\n",
        "\n",
        "    def _generate_huggingface_variations(self, prompt: str) -> List[str]:\n",
        "        \"\"\"Generate variations using HuggingFace\"\"\"\n",
        "        try:\n",
        "            variations = []\n",
        "            for i in range(2):  # Limit for local model\n",
        "                response = self.client(\n",
        "                    prompt[:500],  # Truncate for local model\n",
        "                    max_length=400,\n",
        "                    temperature=0.8,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=50256\n",
        "                )\n",
        "                variations.append(response[0]['generated_text'][len(prompt[:500]):].strip())\n",
        "\n",
        "            return variations\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"HuggingFace generation error: {e}\")\n",
        "\n",
        "    def _generate_template_variations(self, query: str, insights: Dict) -> List[str]:\n",
        "        \"\"\"Generate high-quality template variations\"\"\"\n",
        "\n",
        "        main_points = insights.get('main_points', [])\n",
        "        statistics = insights.get('statistics', [])\n",
        "        actionable_tips = insights.get('actionable_tips', [])\n",
        "\n",
        "        # Template 1: Question Hook + Insight\n",
        "        template1 = self._create_question_hook_post(query, main_points, statistics)\n",
        "\n",
        "        # Template 2: Personal Story + Lesson\n",
        "        template2 = self._create_story_lesson_post(query, main_points, actionable_tips)\n",
        "\n",
        "        # Template 3: List + CTA\n",
        "        template3 = self._create_list_cta_post(query, actionable_tips, statistics)\n",
        "\n",
        "        # Template 4: Contrarian Take\n",
        "        template4 = self._create_contrarian_post(query, main_points)\n",
        "\n",
        "        # Template 5: Prediction/Trend Post\n",
        "        template5 = self._create_trend_post(query, insights.get('trends', []), statistics)\n",
        "\n",
        "        return [template1, template2, template3, template4, template5]\n",
        "\n",
        "    def _create_question_hook_post(self, query: str, main_points: List[str], statistics: List[str]) -> str:\n",
        "        \"\"\"Create post with question hook format\"\"\"\n",
        "\n",
        "        hooks = [\n",
        "            f\"What if I told you that {query} could transform your business? ðŸ¤”\",\n",
        "            f\"Here's something interesting about {query} that most people miss...\",\n",
        "            f\"Why is everyone talking about {query}? Here's what I discovered:\",\n",
        "            f\"Quick question: How well do you really understand {query}?\"\n",
        "        ]\n",
        "\n",
        "        hook = random.choice(hooks)\n",
        "        main_insight = main_points[0] if main_points else f\"The key to {query} success lies in understanding the fundamentals.\"\n",
        "        stat = statistics[0] if statistics else \"Growth is accelerating faster than expected.\"\n",
        "\n",
        "        cta = random.choice(self.engagement_triggers)\n",
        "\n",
        "        post = f\"\"\"{hook}\n",
        "\n",
        "{main_insight[:150]}{'...' if len(main_insight) > 150 else ''}\n",
        "\n",
        "ðŸ“Š Key insight: {stat}\n",
        "\n",
        "{cta}\n",
        "\n",
        "#{query.replace(' ', '').title()} #Professional #Growth\"\"\"\n",
        "\n",
        "        return post\n",
        "\n",
        "    def _create_story_lesson_post(self, query: str, main_points: List[str], tips: List[str]) -> str:\n",
        "        \"\"\"Create post with story + lesson format\"\"\"\n",
        "\n",
        "        story_starters = [\n",
        "            \"Last week, I had a conversation that changed my perspective on\",\n",
        "            \"Three months ago, I made a mistake with\",\n",
        "            \"Here's what I learned from working on\",\n",
        "            \"A client recently asked me about\"\n",
        "        ]\n",
        "\n",
        "        starter = random.choice(story_starters)\n",
        "        lesson = main_points[0] if main_points else f\"Understanding {query} requires a different approach than most people think.\"\n",
        "        tip = tips[0] if tips else f\"Focus on the fundamentals of {query}.\"\n",
        "\n",
        "        post = f\"\"\"{starter} {query}...\n",
        "\n",
        "The lesson? {lesson[:120]}{'...' if len(lesson) > 120 else ''}\n",
        "\n",
        "ðŸ’¡ Key takeaway: {tip[:100]}{'...' if len(tip) > 100 else ''}\n",
        "\n",
        "What's been your experience with this?\n",
        "\n",
        "#{query.replace(' ', '').title()} #LessonsLearned #Professional\"\"\"\n",
        "\n",
        "        return post\n",
        "\n",
        "    def _create_list_cta_post(self, query: str, tips: List[str], statistics: List[str]) -> str:\n",
        "        \"\"\"Create post with list format + CTA\"\"\"\n",
        "\n",
        "        list_tips = tips[:3] if len(tips) >= 3 else [\n",
        "            f\"Understand the basics of {query}\",\n",
        "            f\"Apply {query} strategically\",\n",
        "            f\"Monitor {query} results carefully\"\n",
        "        ]\n",
        "\n",
        "        stat = statistics[0] if statistics else \"Recent data shows significant growth\"\n",
        "\n",
        "        post = f\"\"\"3 essential things about {query} that everyone should know:\n",
        "\n",
        "1ï¸âƒ£ {list_tips[0][:80]}{'...' if len(list_tips[0]) > 80 else ''}\n",
        "\n",
        "2ï¸âƒ£ {list_tips[1][:80]}{'...' if len(list_tips[1]) > 80 else ''}\n",
        "\n",
        "3ï¸âƒ£ {list_tips[2][:80]}{'...' if len(list_tips[2]) > 80 else ''}\n",
        "\n",
        "ðŸ“ˆ {stat}\n",
        "\n",
        "Which one resonates most with you?\n",
        "\n",
        "#{query.replace(' ', '').title()} #Tips #Professional\"\"\"\n",
        "\n",
        "        return post\n",
        "\n",
        "    def _create_contrarian_post(self, query: str, main_points: List[str]) -> str:\n",
        "        \"\"\"Create contrarian/controversial take post\"\"\"\n",
        "\n",
        "        contrarian_starters = [\n",
        "            \"Unpopular opinion:\",\n",
        "            \"Here's what most people get wrong about\",\n",
        "            \"Controversial take:\",\n",
        "            \"Let me challenge the common thinking on\"\n",
        "        ]\n",
        "\n",
        "        starter = random.choice(contrarian_starters)\n",
        "        point = main_points[0] if main_points else f\"The conventional wisdom about {query} might be holding you back.\"\n",
        "\n",
        "        post = f\"\"\"{starter} {query} isn't what most people think it is.\n",
        "\n",
        "{point[:180]}{'...' if len(point) > 180 else ''}\n",
        "\n",
        "The reality is more nuanced than the headlines suggest.\n",
        "\n",
        "Agree or disagree? ðŸ‘‡\n",
        "\n",
        "#{query.replace(' ', '').title()} #Debate #ThoughtLeadership\"\"\"\n",
        "\n",
        "        return post\n",
        "\n",
        "    def _create_trend_post(self, query: str, trends: List[str], statistics: List[str]) -> str:\n",
        "        \"\"\"Create trend/prediction post\"\"\"\n",
        "\n",
        "        trend = trends[0] if trends else f\"{query} is evolving rapidly\"\n",
        "        stat = statistics[0] if statistics else \"adoption rates are increasing\"\n",
        "\n",
        "        post = f\"\"\"The future of {query} is here, and it's happening faster than expected.\n",
        "\n",
        "ðŸš€ {trend[:120]}{'...' if len(trend) > 120 else ''}\n",
        "\n",
        "ðŸ“Š The numbers don't lie: {stat}\n",
        "\n",
        "Are you ready for what's coming next?\n",
        "\n",
        "#{query.replace(' ', '').title()} #Future #Innovation #Trends\"\"\"\n",
        "\n",
        "        return post\n",
        "\n",
        "    def _select_best_post(self, variations: List[str], query: str) -> str:\n",
        "        \"\"\"Select the best performing variation based on engagement factors\"\"\"\n",
        "\n",
        "        if not variations:\n",
        "            return f\"Exciting insights about {query}! More details coming soon.\"\n",
        "\n",
        "        # Score each variation\n",
        "        scored_variations = []\n",
        "        for variation in variations:\n",
        "            score = self._calculate_post_score(variation)\n",
        "            scored_variations.append((variation, score))\n",
        "\n",
        "        # Return highest scoring variation\n",
        "        best_post = max(scored_variations, key=lambda x: x[1])[0]\n",
        "        return best_post\n",
        "\n",
        "    def _calculate_post_score(self, post: str) -> float:\n",
        "        \"\"\"Calculate engagement score for a post\"\"\"\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        # Length optimization\n",
        "        char_count = len(post)\n",
        "        target_chars = self.length_limits.get(self.target_length, 600)\n",
        "\n",
        "        if abs(char_count - target_chars) / target_chars < 0.2:  # Within 20% of target\n",
        "            score += 20\n",
        "\n",
        "        # Engagement elements\n",
        "        if any(trigger.lower() in post.lower() for trigger in self.engagement_triggers):\n",
        "            score += 15\n",
        "\n",
        "        # Emoji usage (optimal 2-4)\n",
        "        emoji_count = len(re.findall(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', post))\n",
        "        if 2 <= emoji_count <= 4:\n",
        "            score += 10\n",
        "        elif emoji_count > 0:\n",
        "            score += 5\n",
        "\n",
        "        # Question marks (engagement)\n",
        "        question_count = post.count('?')\n",
        "        if question_count >= 1:\n",
        "            score += 10\n",
        "\n",
        "        # Numbers and statistics\n",
        "        if re.search(r'\\d+(?:\\.\\d+)?%|\\d+', post):\n",
        "            score += 10\n",
        "\n",
        "        # Line breaks for readability\n",
        "        if post.count('\\n') >= 2:\n",
        "            score += 10\n",
        "\n",
        "        # Power words\n",
        "        power_words = ['amazing', 'incredible', 'essential', 'critical', 'breakthrough', 'revolutionary', 'secret', 'proven']\n",
        "        if any(word in post.lower() for word in power_words):\n",
        "            score += 5\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _optimize_for_engagement(self, post: str, query: str) -> str:\n",
        "        \"\"\"Apply engagement optimizations to the post\"\"\"\n",
        "\n",
        "        optimized = post\n",
        "\n",
        "        # Ensure it starts strong\n",
        "        if not re.match(r'^[ðŸŽ¯ðŸ’¡ðŸš€ðŸ“Šâ­ï¸ðŸ”¥ðŸ’ªðŸŽ‰]', optimized):\n",
        "            attention_grabbers = ['ðŸŽ¯', 'ðŸ’¡', 'ðŸš€', 'ðŸ“Š', 'â­ï¸']\n",
        "            optimized = random.choice(attention_grabbers) + ' ' + optimized\n",
        "\n",
        "        # Ensure it ends with engagement\n",
        "        if not any(trigger.lower() in optimized.lower() for trigger in self.engagement_triggers):\n",
        "            cta = random.choice(self.engagement_triggers)\n",
        "            optimized += f\"\\n\\n{cta}\"\n",
        "\n",
        "        # Add line breaks for readability if missing\n",
        "        if optimized.count('\\n') < 2:\n",
        "            sentences = optimized.split('. ')\n",
        "            if len(sentences) > 2:\n",
        "                optimized = sentences[0] + '.\\n\\n' + '. '.join(sentences[1:])\n",
        "\n",
        "        return optimized\n",
        "\n",
        "    def _format_for_linkedin(self, post: str, query: str) -> str:\n",
        "        \"\"\"Apply LinkedIn-specific formatting\"\"\"\n",
        "\n",
        "        formatted = post\n",
        "\n",
        "        # Ensure proper spacing around emojis\n",
        "        formatted = re.sub(r'([a-zA-Z])([ðŸŽ¯ðŸ’¡ðŸš€ðŸ“Šâ­ï¸ðŸ”¥ðŸ’ªðŸŽ‰])', r'\\1 \\2', formatted)\n",
        "        formatted = re.sub(r'([ðŸŽ¯ðŸ’¡ðŸš€ðŸ“Šâ­ï¸ðŸ”¥ðŸ’ªðŸŽ‰])([a-zA-Z])', r'\\1 \\2', formatted)\n",
        "\n",
        "        # Clean up extra whitespace\n",
        "        formatted = re.sub(r'\\n{3,}', '\\n\\n', formatted)\n",
        "        formatted = re.sub(r' {2,}', ' ', formatted)\n",
        "\n",
        "        # Ensure it fits length requirements\n",
        "        target_length = self.length_limits.get(self.target_length, 600)\n",
        "        if len(formatted) > target_length:\n",
        "            # Truncate intelligently\n",
        "            sentences = formatted.split('.')\n",
        "            truncated = \"\"\n",
        "            for sentence in sentences:\n",
        "                if len(truncated + sentence + '.') <= target_length - 50:  # Leave room for hashtags\n",
        "                    truncated += sentence + '.'\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            if truncated:\n",
        "                formatted = truncated.rstrip('.') + '...'\n",
        "\n",
        "        return formatted.strip()\n",
        "\n",
        "    def _calculate_engagement_metrics(self, post: str) -> LinkedInMetrics:\n",
        "        \"\"\"Calculate comprehensive engagement metrics\"\"\"\n",
        "\n",
        "        char_count = len(post)\n",
        "        word_count = len(post.split())\n",
        "        hashtag_count = len(re.findall(r'#\\w+', post))\n",
        "        emoji_count = len(re.findall(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', post))\n",
        "\n",
        "        # Engagement score calculation\n",
        "        engagement_score = 0\n",
        "\n",
        "        # Character count optimization\n",
        "        target_chars = self.length_limits.get(self.target_length, 600)\n",
        "        char_score = max(0, 25 - abs(char_count - target_chars) / target_chars * 25)\n",
        "        engagement_score += char_score\n",
        "\n",
        "        # Hashtag optimization (3-5 is optimal)\n",
        "        if 3 <= hashtag_count <= 5:\n",
        "            engagement_score += 20\n",
        "        elif hashtag_count > 0:\n",
        "            engagement_score += 10\n",
        "\n",
        "        # Emoji optimization (2-4 is optimal)\n",
        "        if 2 <= emoji_count <= 4:\n",
        "            engagement_score += 15\n",
        "        elif emoji_count > 0:\n",
        "            engagement_score += 7\n",
        "\n",
        "        # Engagement triggers\n",
        "        if any(trigger.lower() in post.lower() for trigger in self.engagement_triggers):\n",
        "            engagement_score += 20\n",
        "\n",
        "        # Question marks\n",
        "        if post.count('?') >= 1:\n",
        "            engagement_score += 10\n",
        "\n",
        "        # Line breaks for readability\n",
        "        if post.count('\\n') >= 2:\n",
        "            engagement_score += 10\n",
        "\n",
        "        # Readability level\n",
        "        avg_word_length = sum(len(word) for word in post.split()) / max(word_count, 1)\n",
        "        if avg_word_length <= 5:\n",
        "            readability_level = \"Easy\"\n",
        "        elif avg_word_length <= 7:\n",
        "            readability_level = \"Medium\"\n",
        "        else:\n",
        "            readability_level = \"Complex\"\n",
        "\n",
        "        # CTA detection\n",
        "        has_cta = any(trigger.lower() in post.lower() for trigger in self.engagement_triggers) or post.count('?') > 0\n",
        "\n",
        "        return LinkedInMetrics(\n",
        "            character_count=char_count,\n",
        "            word_count=word_count,\n",
        "            hashtag_count=hashtag_count,\n",
        "            emoji_count=emoji_count,\n",
        "            engagement_score=engagement_score,\n",
        "            readability_level=readability_level,\n",
        "            call_to_action=has_cta\n",
        "        )\n",
        "\n",
        "    def _generate_hashtags(self, query: str, insights: Dict) -> List[str]:\n",
        "        \"\"\"Generate relevant hashtags for the post\"\"\"\n",
        "\n",
        "        hashtags = []\n",
        "\n",
        "        # Primary hashtag from query\n",
        "        primary_tag = \"#\" + query.replace(' ', '').title()\n",
        "        hashtags.append(primary_tag)\n",
        "\n",
        "        # Industry/topic specific hashtags\n",
        "        query_words = query.lower().split()\n",
        "        industry_hashtags = {\n",
        "            'ai': ['#ArtificialIntelligence', '#MachineLearning', '#Technology', '#Innovation'],\n",
        "            'artificial': ['#AI', '#MachineLearning', '#Technology', '#Future'],\n",
        "            'marketing': ['#DigitalMarketing', '#Marketing', '#GrowthHacking', '#Business'],\n",
        "            'business': ['#Entrepreneurship', '#Leadership', '#Growth', '#Strategy'],\n",
        "            'leadership': ['#Leadership', '#Management', '#Professional', '#Growth'],\n",
        "            'technology': ['#Tech', '#Innovation', '#Digital', '#Future'],\n",
        "            'data': ['#DataScience', '#Analytics', '#BigData', '#Technology'],\n",
        "            'finance': ['#Finance', '#Investment', '#Business', '#Economy'],\n",
        "            'health': ['#Health', '#Wellness', '#Healthcare', '#Medical'],\n",
        "            'education': ['#Education', '#Learning', '#Teaching', '#Knowledge']\n",
        "        }\n",
        "\n",
        "        # Add relevant industry hashtags\n",
        "        for word in query_words:\n",
        "            if word in industry_hashtags:\n",
        "                hashtags.extend(industry_hashtags[word][:2])\n",
        "\n",
        "        # Generic professional hashtags\n",
        "        generic_hashtags = [\n",
        "            '#Professional', '#CareerGrowth', '#Success', '#Motivation',\n",
        "            '#Productivity', '#Skills', '#Network', '#Industry',\n",
        "            '#Insights', '#Tips', '#Strategy', '#Growth'\n",
        "        ]\n",
        "\n",
        "        # Add generic hashtags to reach target count\n",
        "        while len(hashtags) < 8:\n",
        "            remaining_generic = [tag for tag in generic_hashtags if tag not in hashtags]\n",
        "            if remaining_generic:\n",
        "                hashtags.append(random.choice(remaining_generic))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Remove duplicates and limit to 8\n",
        "        unique_hashtags = []\n",
        "        for tag in hashtags:\n",
        "            if tag not in unique_hashtags:\n",
        "                unique_hashtags.append(tag)\n",
        "\n",
        "        return unique_hashtags[:8]\n",
        "\n",
        "    def _create_post_variations(self, base_post: str, hashtags: List[str]) -> Dict[str, str]:\n",
        "        \"\"\"Create different variations of the post\"\"\"\n",
        "\n",
        "        variations = {}\n",
        "\n",
        "        # Short version (for Twitter cross-posting)\n",
        "        short_post = self._create_short_version(base_post)\n",
        "        variations['short'] = short_post\n",
        "\n",
        "        # With different hashtag combinations\n",
        "        variations['minimal_hashtags'] = base_post + '\\n\\n' + ' '.join(hashtags[:3])\n",
        "        variations['max_hashtags'] = base_post + '\\n\\n' + ' '.join(hashtags[:8])\n",
        "\n",
        "        # With different CTAs\n",
        "        alternative_ctas = [\n",
        "            \"What's your experience with this?\",\n",
        "            \"Drop your thoughts in the comments ðŸ‘‡\",\n",
        "            \"Agree or disagree? Let me know!\",\n",
        "            \"What would you add to this list?\"\n",
        "        ]\n",
        "\n",
        "        for i, cta in enumerate(alternative_ctas[:2]):\n",
        "            # Replace existing CTA with alternative\n",
        "            variation = re.sub(r'(What do you think\\?|Share your experience|Agree or disagree\\?)',\n",
        "                             cta, base_post, flags=re.IGNORECASE)\n",
        "            variations[f'cta_variation_{i+1}'] = variation + '\\n\\n' + ' '.join(hashtags[:5])\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def _create_short_version(self, post: str) -> str:\n",
        "        \"\"\"Create a short version of the post\"\"\"\n",
        "\n",
        "        # Extract main message (first paragraph)\n",
        "        paragraphs = post.split('\\n\\n')\n",
        "        main_message = paragraphs[0] if paragraphs else post[:130]\n",
        "\n",
        "        # Add a simple CTA\n",
        "        short_cta = \"Thoughts? ðŸ¤”\"\n",
        "\n",
        "        short_post = main_message[:100] + \"...\\n\\n\" + short_cta\n",
        "\n",
        "        return short_post\n",
        "\n",
        "    def _generate_engagement_tips(self) -> List[str]:\n",
        "        \"\"\"Generate tips for maximizing engagement\"\"\"\n",
        "\n",
        "        tips = [\n",
        "            \"Post during peak hours (8-10 AM or 12-2 PM on weekdays)\",\n",
        "            \"Respond to comments within the first hour for maximum reach\",\n",
        "            \"Ask specific questions to encourage detailed responses\",\n",
        "            \"Share personal experiences to build authentic connections\",\n",
        "            \"Use relevant hashtags but don't exceed 8 per post\",\n",
        "            \"Include a clear call-to-action in every post\",\n",
        "            \"Post consistently to build audience expectations\",\n",
        "            \"Engage with others' content before posting your own\"\n",
        "        ]\n",
        "\n",
        "        return random.sample(tips, 4)  # Return 4 random tips\n",
        "\n",
        "# Google Colab Optimization Functions\n",
        "def setup_linkedin_colab_environment():\n",
        "    \"\"\"Set up the environment for Google Colab\"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Setting up LinkedIn Post Writer environment in Google Colab...\")\n",
        "\n",
        "    # Install required packages\n",
        "    packages = [\n",
        "        \"openai\",\n",
        "        \"google-generativeai\",\n",
        "        \"transformers\",\n",
        "        \"torch\",\n",
        "        \"emoji\",\n",
        "        \"textstat\",\n",
        "        \"nltk\",\n",
        "        \"requests\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            if package == \"transformers\":\n",
        "                __import__(package)\n",
        "            elif package == \"google-generativeai\":\n",
        "                __import__(\"google.generativeai\")\n",
        "            elif package in [\"textstat\", \"emoji\"]:\n",
        "                __import__(package)\n",
        "            else:\n",
        "                __import__(package.replace('-', '_'))\n",
        "            print(f\"âœ… {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"ðŸ“¦ Installing {package}...\")\n",
        "            os.system(f\"pip install {package}\")\n",
        "\n",
        "    print(\"âœ… Environment setup complete!\")\n",
        "\n",
        "def create_linkedin_writer(\n",
        "    llm_provider: str = \"openai\",\n",
        "    model_name: str = \"gpt-3.5-turbo\",\n",
        "    post_style: str = \"professional\",\n",
        "    target_length: str = \"medium\",\n",
        "    api_key: str = None\n",
        ") -> callable:\n",
        "    \"\"\"\n",
        "    Factory function to create LinkedIn post writer for different environments\n",
        "\n",
        "    Usage in Colab:\n",
        "    ```python\n",
        "    # Setup environment\n",
        "    setup_linkedin_colab_environment()\n",
        "\n",
        "    # Set API key\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
        "\n",
        "    # Create writer\n",
        "    linkedin_writer = create_linkedin_writer(\n",
        "        llm_provider=\"openai\",\n",
        "        post_style=\"professional\",\n",
        "        target_length=\"medium\"\n",
        "    )\n",
        "\n",
        "    # Use in your workflow\n",
        "    result = linkedin_writer(state)\n",
        "    ```\n",
        "\n",
        "    Parameters:\n",
        "    - llm_provider: \"openai\", \"google\", \"huggingface\", \"template\"\n",
        "    - model_name: specific model to use\n",
        "    - post_style: \"professional\", \"casual\", \"thought_leadership\", \"personal\"\n",
        "    - target_length: \"short\" (130 chars), \"medium\" (600 chars), \"long\" (1300 chars)\n",
        "    - api_key: API key (optional if set in environment)\n",
        "    \"\"\"\n",
        "\n",
        "    agent = LinkedInPostWriterAgent(\n",
        "        llm_provider=llm_provider,\n",
        "        model_name=model_name,\n",
        "        api_key=api_key,\n",
        "        post_style=post_style,\n",
        "        target_length=target_length\n",
        "    )\n",
        "\n",
        "    return agent.linkedin_post_writer_agent\n",
        "\n",
        "# Analysis and Optimization Tools\n",
        "class LinkedInAnalyzer:\n",
        "    \"\"\"Tools for analyzing and optimizing LinkedIn posts\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def analyze_post_performance(post: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze a LinkedIn post for potential performance\"\"\"\n",
        "\n",
        "        analysis = {\n",
        "            \"character_count\": len(post),\n",
        "            \"word_count\": len(post.split()),\n",
        "            \"paragraph_count\": len([p for p in post.split('\\n\\n') if p.strip()]),\n",
        "            \"hashtag_count\": len(re.findall(r'#\\w+', post)),\n",
        "            \"emoji_count\": len(re.findall(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', post)),\n",
        "            \"question_count\": post.count('?'),\n",
        "            \"has_call_to_action\": any(cta in post.lower() for cta in [\"what do you think\", \"share your\", \"let me know\", \"thoughts?\"]),\n",
        "            \"readability\": \"Easy\" if sum(len(w) for w in post.split()) / max(len(post.split()), 1) <= 5 else \"Medium\"\n",
        "        }\n",
        "\n",
        "        # Performance prediction\n",
        "        score = 0\n",
        "\n",
        "        # Optimal character count (400-600 for medium posts)\n",
        "        if 400 <= analysis[\"character_count\"] <= 600:\n",
        "            score += 25\n",
        "        elif 300 <= analysis[\"character_count\"] <= 800:\n",
        "            score += 15\n",
        "\n",
        "        # Engagement elements\n",
        "        if analysis[\"has_call_to_action\"]:\n",
        "            score += 20\n",
        "        if analysis[\"question_count\"] >= 1:\n",
        "            score += 15\n",
        "        if 2 <= analysis[\"emoji_count\"] <= 4:\n",
        "            score += 15\n",
        "        if 3 <= analysis[\"hashtag_count\"] <= 7:\n",
        "            score += 15\n",
        "        if analysis[\"paragraph_count\"] >= 3:\n",
        "            score += 10\n",
        "\n",
        "        analysis[\"predicted_score\"] = min(score, 100)\n",
        "        analysis[\"performance_tier\"] = (\n",
        "            \"High\" if score >= 70 else\n",
        "            \"Medium\" if score >= 50 else\n",
        "            \"Low\"\n",
        "        )\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    @staticmethod\n",
        "    def suggest_improvements(post: str) -> List[str]:\n",
        "        \"\"\"Suggest specific improvements for a LinkedIn post\"\"\"\n",
        "\n",
        "        suggestions = []\n",
        "        analysis = LinkedInAnalyzer.analyze_post_performance(post)\n",
        "\n",
        "        # Length suggestions\n",
        "        if analysis[\"character_count\"] < 300:\n",
        "            suggestions.append(\"ðŸ“ Consider expanding the post - longer posts (400-600 chars) typically get better engagement\")\n",
        "        elif analysis[\"character_count\"] > 1000:\n",
        "            suggestions.append(\"âœ‚ï¸ Consider shortening the post - very long posts may lose reader attention\")\n",
        "\n",
        "        # Engagement suggestions\n",
        "        if not analysis[\"has_call_to_action\"]:\n",
        "            suggestions.append(\"â“ Add a call-to-action or question to encourage comments\")\n",
        "\n",
        "        if analysis[\"emoji_count\"] == 0:\n",
        "            suggestions.append(\"ðŸ˜Š Add 2-3 relevant emojis to make the post more visually appealing\")\n",
        "        elif analysis[\"emoji_count\"] > 5:\n",
        "            suggestions.append(\"ðŸ“‰ Reduce emoji usage - too many can appear unprofessional\")\n",
        "\n",
        "        if analysis[\"hashtag_count\"] < 3:\n",
        "            suggestions.append(\"#ï¸âƒ£ Add more relevant hashtags (aim for 5-7) to increase discoverability\")\n",
        "        elif analysis[\"hashtag_count\"] > 8:\n",
        "            suggestions.append(\"ðŸ”½ Reduce hashtags - LinkedIn posts perform best with 5-7 hashtags\")\n",
        "\n",
        "        if analysis[\"paragraph_count\"] < 3:\n",
        "            suggestions.append(\"ðŸ“„ Break content into more paragraphs for better readability\")\n",
        "\n",
        "        if analysis[\"question_count\"] == 0:\n",
        "            suggestions.append(\"ðŸ¤” Include at least one question to boost engagement\")\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "# Batch processing for multiple posts\n",
        "def generate_linkedin_campaign(\n",
        "    queries: List[str],\n",
        "    research_data_list: List[Dict],\n",
        "    writer_config: Dict = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generate a campaign of LinkedIn posts for multiple topics\n",
        "\n",
        "    Usage:\n",
        "    ```python\n",
        "    queries = [\"AI in healthcare\", \"Remote work trends\", \"Digital marketing\"]\n",
        "    research_data = [research1, research2, research3]\n",
        "\n",
        "    campaign = generate_linkedin_campaign(queries, research_data)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    if writer_config is None:\n",
        "        writer_config = {\n",
        "            \"llm_provider\": \"openai\",\n",
        "            \"post_style\": \"professional\",\n",
        "            \"target_length\": \"medium\"\n",
        "        }\n",
        "\n",
        "    # Create writer\n",
        "    writer = create_linkedin_writer(**writer_config)\n",
        "\n",
        "    campaign_results = {\n",
        "        \"posts\": {},\n",
        "        \"campaign_metrics\": {},\n",
        "        \"generated_at\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    class CampaignState:\n",
        "        def __init__(self, query, research_data):\n",
        "            self.query = query\n",
        "            self.research_data = research_data\n",
        "            self.blog_content = \"\"\n",
        "\n",
        "    total_chars = 0\n",
        "    total_hashtags = []\n",
        "\n",
        "    print(f\"ðŸ“± Generating LinkedIn campaign for {len(queries)} topics...\")\n",
        "\n",
        "    for i, (query, research_data) in enumerate(zip(queries, research_data_list), 1):\n",
        "        print(f\"ðŸš€ Generating post {i}/{len(queries)}: {query}\")\n",
        "\n",
        "        state = CampaignState(query, research_data)\n",
        "        result = writer(state)\n",
        "\n",
        "        campaign_results[\"posts\"][query] = result\n",
        "\n",
        "        # Collect campaign metrics\n",
        "        if \"metrics\" in result:\n",
        "            total_chars += result[\"metrics\"].get(\"character_count\", 0)\n",
        "            if \"hashtags\" in result:\n",
        "                total_hashtags.extend(result[\"hashtags\"])\n",
        "\n",
        "    # Calculate campaign-level metrics\n",
        "    avg_length = total_chars / len(queries) if queries else 0\n",
        "    unique_hashtags = len(set(total_hashtags))\n",
        "\n",
        "    campaign_results[\"campaign_metrics\"] = {\n",
        "        \"total_posts\": len(queries),\n",
        "        \"average_length\": avg_length,\n",
        "        \"total_hashtags\": len(total_hashtags),\n",
        "        \"unique_hashtags\": unique_hashtags,\n",
        "        \"completion_rate\": len([p for p in campaign_results[\"posts\"].values() if \"error\" not in p]) / len(queries) * 100\n",
        "    }\n",
        "\n",
        "    print(f\"âœ… Campaign complete! Generated {len(queries)} posts\")\n",
        "    print(f\"ðŸ“Š Average length: {avg_length:.0f} characters\")\n",
        "    print(f\"ðŸ·ï¸ Used {unique_hashtags} unique hashtags\")\n",
        "\n",
        "    return campaign_results\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Mock state for testing\n",
        "    class MockState:\n",
        "        def __init__(self, query, research_data=None, blog_content=\"\"):\n",
        "            self.query = query\n",
        "            self.research_data = research_data or {\n",
        "                \"search_results\": [\n",
        "                    {\"snippet\": \"AI revolutionizes business processes with 40% efficiency gains\", \"url\": \"tech-news.com\"},\n",
        "                    {\"snippet\": \"Machine learning adoption grows 65% in enterprise\", \"url\": \"business-ai.com\"}\n",
        "                ],\n",
        "                \"detailed_content\": [\n",
        "                    {\"content\": \"Artificial intelligence transforms industries by automating complex processes and providing data-driven insights...\"}\n",
        "                ],\n",
        "                \"summary\": \"AI technology adoption accelerates across industries with significant ROI improvements\"\n",
        "            }\n",
        "            self.blog_content = blog_content\n",
        "\n",
        "    # Test the agent\n",
        "    print(\"ðŸ§ª Testing LinkedIn Post Writer Agent...\")\n",
        "\n",
        "    # Create agent with template mode for testing\n",
        "    writer = create_linkedin_writer(\n",
        "        llm_provider=\"template\",  # Use template for testing\n",
        "        post_style=\"professional\",\n",
        "        target_length=\"medium\"\n",
        "    )\n",
        "\n",
        "    # Test with sample data\n",
        "    test_state = MockState(\n",
        "        query=\"artificial intelligence in business\",\n",
        "        blog_content=\"AI is transforming how businesses operate. From automating routine tasks to providing predictive analytics, artificial intelligence offers unprecedented opportunities for growth. Companies implementing AI solutions report 40% improvements in efficiency and 25% cost reductions. The key to successful AI implementation lies in strategic planning and employee training.\"\n",
        "    )\n",
        "\n",
        "    # Generate LinkedIn post\n",
        "    result = writer(test_state)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ“± LINKEDIN POST GENERATION RESULTS:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if \"error\" not in result:\n",
        "        print(\"ðŸ“ Generated Post:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(result.get(\"linkedin_content\", \"\"))\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if \"metrics\" in result:\n",
        "            metrics = result[\"metrics\"]\n",
        "            print(f\"\\nðŸ“Š Engagement Metrics:\")\n",
        "            print(f\"   Character Count: {metrics.get('character_count', 0)}\")\n",
        "            print(f\"   Word Count: {metrics.get('word_count', 0)}\")\n",
        "            print(f\"   Hashtags: {metrics.get('hashtag_count', 0)}\")\n",
        "            print(f\"   Emojis: {metrics.get('emoji_count', 0)}\")\n",
        "            print(f\"   Engagement Score: {metrics.get('engagement_score', 0):.1f}/100\")\n",
        "            print(f\"   Has CTA: {metrics.get('call_to_action', False)}\")\n",
        "\n",
        "        if \"hashtags\" in result:\n",
        "            print(f\"\\nðŸ·ï¸ Suggested Hashtags:\")\n",
        "            print(\"   \" + \" \".join(result[\"hashtags\"][:5]))\n",
        "\n",
        "        if \"engagement_tips\" in result:\n",
        "            print(f\"\\nðŸ’¡ Engagement Tips:\")\n",
        "            for tip in result[\"engagement_tips\"][:3]:\n",
        "                print(f\"   â€¢ {tip}\")\n",
        "\n",
        "        # Analyze post performance\n",
        "        analysis = LinkedInAnalyzer.analyze_post_performance(result.get(\"linkedin_content\", \"\"))\n",
        "        print(f\"\\nðŸŽ¯ Performance Prediction: {analysis['performance_tier']} ({analysis['predicted_score']}/100)\")\n",
        "\n",
        "        suggestions = LinkedInAnalyzer.suggest_improvements(result.get(\"linkedin_content\", \"\"))\n",
        "        if suggestions:\n",
        "            print(f\"\\nðŸ”§ Improvement Suggestions:\")\n",
        "            for suggestion in suggestions[:3]:\n",
        "                print(f\"   â€¢ {suggestion}\")\n",
        "    else:\n",
        "        print(f\"âŒ Error: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ¨ Post Style: {result.get('post_style', 'Unknown')}\")\n",
        "    print(f\"ðŸ“ Target Length: {result.get('target_length', 'Unknown')}\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "KxNwCQABeNmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Install required library (run once per Colab session)\n",
        "!pip install --quiet openai\n",
        "\n",
        "# âœ… Import libraries\n",
        "from openai import OpenAI\n",
        "\n",
        "# âœ… Initialize OpenAI client\n",
        "# Make sure you set your API key securely in Colab, e.g.:\n",
        "# import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
        "client = OpenAI()\n",
        "\n",
        "class State:\n",
        "    \"\"\"\n",
        "    Simple container to simulate agent state.\n",
        "    Replace this with your actual state object if different.\n",
        "    \"\"\"\n",
        "    def __init__(self, query=None, blog_content=None, linkedin_content=None):\n",
        "        self.query = query\n",
        "        self.blog_content = blog_content\n",
        "        self.linkedin_content = linkedin_content\n",
        "\n",
        "def image_generation_agent(state):\n",
        "    \"\"\"\n",
        "    Image Generation Agent for Google Colab.\n",
        "    Produces custom visuals with lightweight prompt optimization.\n",
        "    \"\"\"\n",
        "    print(\"---IMAGE GENERATION AGENT---\")\n",
        "\n",
        "    query = state.query\n",
        "    blog_content = state.blog_content\n",
        "    linkedin_content = state.linkedin_content\n",
        "\n",
        "    # Pick best available content\n",
        "    content_for_prompt = blog_content or linkedin_content or query\n",
        "\n",
        "    if not content_for_prompt:\n",
        "        print(\"âš ï¸ No content available to generate an image.\")\n",
        "        return {\n",
        "            \"image_prompt\": \"Error: No content available for image generation.\",\n",
        "            \"generated_image\": None,\n",
        "        }\n",
        "\n",
        "    print(f\"ðŸ“Œ Using content for image generation: {content_for_prompt}\")\n",
        "\n",
        "    try:\n",
        "        # Optional: Prompt optimization step with GPT\n",
        "        opt_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a creative assistant optimizing image prompts.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Optimize this for image generation: {content_for_prompt}\"}\n",
        "            ]\n",
        "        )\n",
        "        optimized_prompt = opt_response.choices[0].message.content.strip()\n",
        "\n",
        "        # Call DALL-E 3 (Image generation)\n",
        "        response = client.images.generate(\n",
        "            model=\"gpt-image-1\",  # Use OpenAI's latest image model (Colab-friendly)\n",
        "            prompt=optimized_prompt,\n",
        "            size=\"1024x1024\",\n",
        "            n=1\n",
        "        )\n",
        "        generated_image_url = response.data[0].url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        return {\n",
        "            \"image_prompt\": f\"Error optimizing or generating image: {e}\",\n",
        "            \"generated_image\": None,\n",
        "        }\n",
        "\n",
        "    print(\"âœ… Image generation complete.\")\n",
        "    return {\n",
        "        \"image_prompt\": optimized_prompt,\n",
        "        \"generated_image\": generated_image_url,\n",
        "    }\n",
        "\n",
        "# ðŸ”¹ Example usage in Colab:\n",
        "state = State(query=\"AI transforming healthcare through futuristic hospitals\")\n",
        "result = image_generation_agent(state)\n",
        "\n",
        "print(\"Prompt:\", result[\"image_prompt\"])\n",
        "print(\"Image URL:\", result[\"generated_image\"])\n"
      ],
      "metadata": {
        "id": "hEbfEr_2jmYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Import OpenAI client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "class State:\n",
        "    \"\"\"\n",
        "    Simple container to simulate agent state.\n",
        "    Extend this with attributes as needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, query=None, research_data=None, blog_content=None,\n",
        "                 linkedin_content=None, image_prompt=None, generated_image=None):\n",
        "        self.query = query\n",
        "        self.research_data = research_data\n",
        "        self.blog_content = blog_content\n",
        "        self.linkedin_content = linkedin_content\n",
        "        self.image_prompt = image_prompt\n",
        "        self.generated_image = generated_image\n",
        "\n",
        "def content_strategist_agent(state):\n",
        "    \"\"\"\n",
        "    Content Strategist Agent for Google Colab.\n",
        "    Formats and organizes research and generated content into a polished output.\n",
        "    \"\"\"\n",
        "    print(\"---CONTENT STRATEGIST AGENT---\")\n",
        "\n",
        "    query = state.query\n",
        "    research_data = state.research_data\n",
        "    blog_content = state.blog_content\n",
        "    linkedin_content = state.linkedin_content\n",
        "    image_prompt = state.image_prompt\n",
        "    generated_image = state.generated_image\n",
        "\n",
        "    print(\"ðŸ“Œ Organizing and formatting final content...\")\n",
        "\n",
        "    try:\n",
        "        # Call GPT to organize and polish the content\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",  # Lightweight, cost-efficient for formatting\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert content strategist. \"\n",
        "                                              \"Organize and format the provided materials into a professional, engaging final output.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Initial Query: {query}\n",
        "\n",
        "Research Data:\n",
        "{research_data if research_data else \"No research data available.\"}\n",
        "\n",
        "Blog Content:\n",
        "{blog_content if blog_content else \"No blog content available.\"}\n",
        "\n",
        "LinkedIn Content:\n",
        "{linkedin_content if linkedin_content else \"No LinkedIn content available.\"}\n",
        "\n",
        "Image Prompt:\n",
        "{image_prompt if image_prompt else \"No image prompt available.\"}\n",
        "\n",
        "Generated Image:\n",
        "{generated_image if generated_image else \"No image generated.\"}\n",
        "\n",
        "Please organize this into a final deliverable with:\n",
        "- Intro/summary\n",
        "- Key insights\n",
        "- Blog/long-form content\n",
        "- LinkedIn-style short content\n",
        "- Suggested image caption\n",
        "\"\"\"}\n",
        "            ]\n",
        "        )\n",
        "        final_content_output = response.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during formatting: {e}\")\n",
        "        final_content_output = f\"Error formatting content: {e}\"\n",
        "\n",
        "    print(\"âœ… Final content organization complete.\")\n",
        "    print(\"\\n---FINAL CONTENT OUTPUT---\\n\")\n",
        "    print(final_content_output)\n",
        "\n",
        "    # Optionally return structured results\n",
        "    return {\n",
        "        \"final_output\": final_content_output\n",
        "    }\n",
        "\n",
        "# ðŸ”¹ Example usage\n",
        "state = State(\n",
        "    query=\"AI transforming healthcare\",\n",
        "    research_data=\"Market is projected to grow 35% CAGR. Hospitals adopting AI for diagnostics.\",\n",
        "    blog_content=\"AI is revolutionizing healthcare with predictive analytics and automation.\",\n",
        "    linkedin_content=\"AI in healthcare = faster diagnosis + better outcomes ðŸš€\",\n",
        "    image_prompt=\"Futuristic hospital powered by AI technology\",\n",
        "    generated_image=\"https://example.com/fake_image_url\"\n",
        ")\n",
        "\n",
        "result = content_strategist_agent(state)\n"
      ],
      "metadata": {
        "id": "L4Mp9a_qj3yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3357286",
        "outputId": "01db68ed-f8d3-4827-85a2-be6b3867ffb5"
      },
      "source": [
        "# âœ… Install LangGraph (if not already installed in Colab)\n",
        "!pip install --quiet langgraph\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# âœ… Define the GraphState class (shared state between agents)\n",
        "class GraphState:\n",
        "    def __init__(self, query=None, research_data=None, blog_content=None,\n",
        "                 linkedin_content=None, image_prompt=None, generated_image=None):\n",
        "        self.query = query\n",
        "        self.research_data = research_data\n",
        "        self.blog_content = blog_content\n",
        "        self.linkedin_content = linkedin_content\n",
        "        self.image_prompt = image_prompt\n",
        "        self.generated_image = generated_image\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"GraphState(query='{self.query}', research_data='{self.research_data}', \"\n",
        "                f\"blog_content='{self.blog_content}', linkedin_content='{self.linkedin_content}', \"\n",
        "                f\"image_prompt='{self.image_prompt}', generated_image='{self.generated_image}')\")\n",
        "\n",
        "# âœ… Initialize the StateGraph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# âœ… Define conditional routing from query_handler\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\",\n",
        "    lambda state: \"deep_research\" if state.research_data else (\n",
        "        \"seo_blog_writer\" if state.blog_content else (\n",
        "            \"linkedin_post_writer\" if state.linkedin_content else (\n",
        "                \"image_generation\" if state.query and \"image\" in state.query.lower() else \"content_strategist\"\n",
        "            )\n",
        "        )\n",
        "    ),\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# âœ… Define the workflow edges (linear flows after agent execution)\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# âœ… Define the end of the workflow\n",
        "workflow.add_edge(\"content_strategist\", END)\n",
        "\n",
        "# âœ… Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"âœ… LangGraph workflow compiled successfully!\")\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LangGraph workflow compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "00e54eae",
        "outputId": "5492613d-26f8-4787-d2fd-e9435c742eb6"
      },
      "source": [
        "# Run the compiled graph with a sample query\n",
        "\n",
        "# Define a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "\n",
        "\n",
        "# Initialize the GraphState with the sample query\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "# Invoke the compiled graph\n",
        "# The output will be the state after the workflow has run\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n--- Final State ---\")\n",
        "print(final_state)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running workflow for query: research about the benefits of content marketing ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3840493209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Invoke the compiled graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# The output will be the state after the workflow has run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Final State ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3027\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         ]\n\u001b[0;32m---> 87\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 )\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb2d7095"
      },
      "source": [
        "# Task\n",
        "Implement the actual logic within the agents (e.g., add the real API calls for SERP API and DALL-E 3, and integrate the LLM calls for content generation using OpenAI GPT-4) and test the compiled graph with a sample query to test the workflow and see how the agents interact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ff85fc2"
      },
      "source": [
        "## Set up api keys\n",
        "\n",
        "### Subtask:\n",
        "Add a cell to securely handle and load the necessary API keys for OpenAI and SERP API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6a79615"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to add a cell to securely handle and load the necessary API keys. This involves importing `os` and `dotenv`, loading environment variables, and setting `os.environ` for the API keys. I will create a new code cell to perform these actions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "d14a25a0",
        "outputId": "77b2fa6b-87af-4689-ff70-937102872482"
      },
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access and set the API keys as environment variables\n",
        "# Ensure your .env file contains lines like:\n",
        "# OPENAI_API_KEY='your_openai_api_key'\n",
        "# SERPAPI_API_KEY='your_serpapi_api_key'\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "print(\"API keys loaded.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "str expected, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-618716742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# OPENAI_API_KEY='your_openai_api_key'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# SERPAPI_API_KEY='your_serpapi_api_key'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SERPAPI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SERPAPI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(value)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13a42174"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `os.getenv()` returned `None`, indicating that the environment variables were not loaded correctly. This is likely because the `.env` file does not exist or the keys are not present in the file. I will add a check to ensure the keys are loaded and provide feedback if they are not found. I will also make sure to only set the environment variables if the keys are found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jh35lj-lKR7",
        "outputId": "806e5dee-2d37-4d9d-f930-831834ee35bd"
      },
      "source": [
        "# Check if the API keys were loaded successfully\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "if openai_key is None:\n",
        "    print(\"Error: OPENAI_API_KEY not found in environment variables. Make sure you have a .env file with OPENAI_API_KEY set.\")\n",
        "else:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "    print(\"OPENAI_API_KEY loaded.\")\n",
        "\n",
        "if serpapi_key is None:\n",
        "    print(\"Error: SERPAPI_API_KEY not found in environment variables. Make sure you have a .env file with SERPAPI_API_KEY set.\")\n",
        "else:\n",
        "    os.environ[\"SERPAPI_API_KEY\"] = serpapi_key\n",
        "    print(\"SERPAPI_API_KEY loaded.\")\n",
        "\n",
        "if openai_key is not None and serpapi_key is not None:\n",
        "    print(\"All required API keys loaded successfully.\")\n",
        "else:\n",
        "    print(\"Not all required API keys were loaded.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: OPENAI_API_KEY not found in environment variables. Make sure you have a .env file with OPENAI_API_KEY set.\n",
            "Error: SERPAPI_API_KEY not found in environment variables. Make sure you have a .env file with SERPAPI_API_KEY set.\n",
            "Not all required API keys were loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7f2c073"
      },
      "source": [
        "## Update deep research agent\n",
        "\n",
        "### Subtask:\n",
        "Modify the `deep_research_agent` function to use the SERP API to perform actual web searches based on the query and store the results in the state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "700e3a5d"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the deep_research_agent function to use the SERP API for actual web searches based on the query and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe409959",
        "outputId": "12079553-c8f3-4d3e-8081-899b008cda39"
      },
      "source": [
        "try:\n",
        "    from serpapi import SerpApiClient\n",
        "    # Check if SerpApiClient can perform a Google search, or if there's a specific GoogleSearch client\n",
        "    # Based on common usage, SerpApiClient should be sufficient\n",
        "    SerpAPI_Client_Available = True\n",
        "except ImportError:\n",
        "    print(\"Could not import SerpApiClient from serpapi. Please check the library documentation and installation.\")\n",
        "    SerpAPI_Client_Available = False\n",
        "\n",
        "import os\n",
        "\n",
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "def deep_research_agent(state):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        return {\"research_data\": \"Error: No query provided for research.\"}\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key:\n",
        "        print(\"Error: SERPAPI_API_KEY not found in environment variables.\")\n",
        "        return {\"research_data\": \"Error: SERPAPI_API_KEY not configured.\"}\n",
        "\n",
        "    if not SerpAPI_Client_Available:\n",
        "         print(\"Error: SerpApiClient not imported successfully.\")\n",
        "         return {\"research_data\": \"Error: SerpAPI library not correctly configured/installed.\"}\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        # Use SerpApiClient to perform the search\n",
        "        client = SerpApiClient(params)\n",
        "        results = client.get_dict()\n",
        "\n",
        "\n",
        "        # Extract relevant data - this might need adjustment based on SERPAPI results structure\n",
        "        # For example, you might want to extract 'organic_results', 'knowledge_graph', etc.\n",
        "        # Let's store the entire results dictionary for now, or a summary\n",
        "        research_data = results # Or process results to get specific data\n",
        "\n",
        "\n",
        "        print(\"Research complete. Returning update.\")\n",
        "        return {\"research_data\": research_data} # Return a dictionary with updates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        return {\"research_data\": f\"Error during SERP API search: {e}\"}"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not import SerpApiClient from serpapi. Please check the library documentation and installation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd445f0c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `GoogleSearch` cannot be imported directly from `serpapi`. I need to import it from `serpapi.google_search`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "tHzKYdZUlQYC",
        "outputId": "d2b4ae70-f5f0-4fb6-e5d1-5ee2be62cd4d"
      },
      "source": [
        "from serpapi.google_search import GoogleSearch\n",
        "import os\n",
        "\n",
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "def deep_research_agent(state):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        return {\"research_data\": \"Error: No query provided for research.\"}\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key:\n",
        "        print(\"Error: SERPAPI_API_KEY not found in environment variables.\")\n",
        "        return {\"research_data\": \"Error: SERPAPI_API_KEY not configured.\"}\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        search = GoogleSearch(params)\n",
        "        results = search.get_dict()\n",
        "\n",
        "        # Extract relevant data - this might need adjustment based on SERPAPI results structure\n",
        "        # For example, you might want to extract 'organic_results', 'knowledge_graph', etc.\n",
        "        # Let's store the entire results dictionary for now, or a summary\n",
        "        research_data = results # Or process results to get specific data\n",
        "\n",
        "        print(\"Research complete. Returning update.\")\n",
        "        return {\"research_data\": research_data} # Return a dictionary with updates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        return {\"research_data\": f\"Error during SERP API search: {e}\"}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'serpapi.google_search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1377969389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mserpapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Deep Research Agent: Conducts comprehensive web research and analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'serpapi.google_search'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93db9229"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous import failed again. It seems the structure of the installed `serpapi` library doesn't match the expected import path. I will try importing `GoogleSearch` directly from the top-level `serpapi` package as it was in the first attempt, but will re-verify the documentation if this also fails.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "RdcPIp4wlSbz",
        "outputId": "2ac91990-6c9f-4491-a8c7-da8c07902c60"
      },
      "source": [
        "from serpapi import GoogleSearch\n",
        "import os\n",
        "\n",
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "def deep_research_agent(state):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        return {\"research_data\": \"Error: No query provided for research.\"}\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key:\n",
        "        print(\"Error: SERPAPI_API_KEY not found in environment variables.\")\n",
        "        return {\"research_data\": \"Error: SERPAPI_API_KEY not configured.\"}\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        search = GoogleSearch(params)\n",
        "        results = search.get_dict()\n",
        "\n",
        "        # Extract relevant data - this might need adjustment based on SERPAPI results structure\n",
        "        # For example, you might want to extract 'organic_results', 'knowledge_graph', etc.\n",
        "        # Let's store the entire results dictionary for now, or a summary\n",
        "        research_data = results # Or process results to get specific data\n",
        "\n",
        "        print(\"Research complete. Returning update.\")\n",
        "        return {\"research_data\": research_data} # Return a dictionary with updates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        return {\"research_data\": f\"Error during SERP API search: {e}\"}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'GoogleSearch' from 'serpapi' (/usr/local/lib/python3.12/dist-packages/serpapi/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2458647243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mserpapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Deep Research Agent: Conducts comprehensive web research and analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'GoogleSearch' from 'serpapi' (/usr/local/lib/python3.12/dist-packages/serpapi/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8990a98"
      },
      "source": [
        "**Reasoning**:\n",
        "The import error persists. It seems the `serpapi` library might not have a class named `GoogleSearch` directly importable this way, or there might be an issue with the installed version. I will try a different approach based on common usage patterns for the `serpapi` library, which often involves instantiating a client and then calling a search method, or directly using a search function provided by the library. I will attempt to import `GoogleSearch` from `serpapi.google_search_api`. If this fails, I will search for the correct way to import and use the Google Search functionality within the installed `serpapi` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZtXos_lVRi",
        "outputId": "a892b78b-4b6e-4eb3-f757-3f331bffff4c"
      },
      "source": [
        "try:\n",
        "    from serpapi.google_search_api import GoogleSearch\n",
        "except ImportError:\n",
        "    # Fallback or alternative import if the above fails\n",
        "    try:\n",
        "        from serpapi import GoogleSearch\n",
        "    except ImportError:\n",
        "        # If all else fails, provide a clear error message\n",
        "        print(\"Could not import GoogleSearch from serpapi. Please check the library documentation and installation.\")\n",
        "        GoogleSearch = None # Set to None to handle gracefully in the agent function\n",
        "\n",
        "import os\n",
        "\n",
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "def deep_research_agent(state):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        return {\"research_data\": \"Error: No query provided for research.\"}\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key:\n",
        "        print(\"Error: SERPAPI_API_KEY not found in environment variables.\")\n",
        "        return {\"research_data\": \"Error: SERPAPI_API_KEY not configured.\"}\n",
        "\n",
        "    if GoogleSearch is None:\n",
        "         print(\"Error: GoogleSearch class not imported successfully.\")\n",
        "         return {\"research_data\": \"Error: SerpAPI library not correctly configured/installed.\"}\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        search = GoogleSearch(params)\n",
        "        results = search.get_dict()\n",
        "\n",
        "        # Extract relevant data - this might need adjustment based on SERPAPI results structure\n",
        "        # For example, you might want to extract 'organic_results', 'knowledge_graph', etc.\n",
        "        # Let's store the entire results dictionary for now, or a summary\n",
        "        research_data = results # Or process results to get specific data\n",
        "\n",
        "        print(\"Research complete. Returning update.\")\n",
        "        return {\"research_data\": research_data} # Return a dictionary with updates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        return {\"research_data\": f\"Error during SERP API search: {e}\"}"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not import GoogleSearch from serpapi. Please check the library documentation and installation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40aa2bb6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to import `GoogleSearch` from `serpapi` failed. The error message suggests the class might not be directly importable or the library structure is different. I will try importing the `SerpApiClient` and using its `search` method, which is another common pattern for this library. I will update the `deep_research_agent` function to use this approach if the import is successful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e80a735b",
        "outputId": "a7ab468d-0bc1-4351-9fb7-9b802d5bb13f"
      },
      "source": [
        "try:\n",
        "    from serpapi import SerpApiClient\n",
        "    # Check if SerpApiClient can perform a Google search, or if there's a specific GoogleSearch client\n",
        "    # Based on common usage, SerpApiClient should be sufficient\n",
        "    SerpAPI_Client_Available = True\n",
        "except ImportError:\n",
        "    print(\"Could not import SerpApiClient from serpapi. Please check the library documentation and installation.\")\n",
        "    SerpAPI_Client_Available = False\n",
        "\n",
        "import os\n",
        "\n",
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "def deep_research_agent(state):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        return {\"research_data\": \"Error: No query provided for research.\"}\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key:\n",
        "        print(\"Error: SERPAPI_API_KEY not found in environment variables.\")\n",
        "        return {\"research_data\": \"Error: SERPAPI_API_KEY not configured.\"}\n",
        "\n",
        "    if not SerpAPI_Client_Available:\n",
        "         print(\"Error: SerpApiClient not imported successfully.\")\n",
        "         return {\"research_data\": \"Error: SerpAPI library not correctly configured/installed.\"}\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        # Use SerpApiClient to perform the search\n",
        "        client = SerpApiClient(params)\n",
        "        results = client.get_dict()\n",
        "\n",
        "\n",
        "        # Extract relevant data - this might need adjustment based on SERPAPI results structure\n",
        "        # For example, you might want to extract 'organic_results', 'knowledge_graph', etc.\n",
        "        # Let's store the entire results dictionary for now, or a summary\n",
        "        research_data = results # Or process results to get specific data\n",
        "\n",
        "\n",
        "        print(\"Research complete. Returning update.\")\n",
        "        return {\"research_data\": research_data} # Return a dictionary with updates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        return {\"research_data\": f\"Error during SERP API search: {e}\"}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not import SerpApiClient from serpapi. Please check the library documentation and installation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b86de3b4"
      },
      "source": [
        "## Update seo blog writer agent\n",
        "\n",
        "### Subtask:\n",
        "Modify the `seo_blog_writer_agent` function to use OpenAI's GPT-4 (or a suitable LLM) to generate SEO-optimized blog content based on the research data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9c591ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `seo_blog_writer_agent` function to use OpenAI's GPT-4 for generating blog content based on research data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7481010"
      },
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# SEO Blog Writer Agent: Creates search-optimized long-form content\n",
        "\n",
        "def seo_blog_writer_agent(state):\n",
        "    \"\"\"\n",
        "    Creates search-optimized long-form content based on research data.\n",
        "    \"\"\"\n",
        "    print(\"---SEO BLOG WRITER AGENT---\")\n",
        "    query = state.query\n",
        "    research_data = state.research_data\n",
        "\n",
        "    if not research_data:\n",
        "        print(\"No research data available. Cannot generate blog content.\")\n",
        "        return {\"blog_content\": \"Error: No research data available.\"} # Return error in dictionary\n",
        "\n",
        "    print(f\"Generating blog content for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        return {\"blog_content\": \"Error: OPENAI_API_KEY not configured.\"}\n",
        "\n",
        "    # Construct a detailed prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert SEO blog writer. Your task is to write a comprehensive and SEO-optimized blog post based on the following research data and the user's query.\n",
        "\n",
        "    User Query: {query}\n",
        "\n",
        "    Research Data:\n",
        "    {research_data}\n",
        "\n",
        "    Instructions:\n",
        "    1. Write a compelling title for the blog post.\n",
        "    2. Write an engaging introduction that hooks the reader and summarizes the main points.\n",
        "    3. Structure the blog post with clear headings and subheadings.\n",
        "    4. Incorporate relevant keywords naturally throughout the content (use keywords that appear in the research data if possible).\n",
        "    5. Provide detailed explanations and insights based on the research data.\n",
        "    6. Write a concluding paragraph that summarizes the key takeaways and includes a call to action (e.g., share, comment).\n",
        "    7. Ensure the content is well-written, grammatically correct, and easy to read.\n",
        "    8. The blog post should be at least 500 words long.\n",
        "\n",
        "    Generate the full blog post content now.\n",
        "    \"\"\"\n",
        "\n",
        "    generated_blog_content = \"Error: Blog content generation failed.\" # Default error message\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Chat Completions API\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", # Or another suitable model like \"gpt-3.5-turbo\"\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert SEO blog writer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_blog_content = response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating blog content with OpenAI API: {e}\")\n",
        "        generated_blog_content = f\"Error generating blog content: {e}\"\n",
        "\n",
        "    print(\"Blog content generation complete. Returning update.\")\n",
        "    return {\"blog_content\": generated_blog_content} # Return a dictionary with updates"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "841d21e8"
      },
      "source": [
        "## Update linkedin post writer agent\n",
        "\n",
        "### Subtask:\n",
        "Modify the `linkedin_post_writer_agent` function to use OpenAI's GPT-4 (or a suitable LLM) to generate an engaging LinkedIn post based on the blog content or research data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78fa6efd"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `linkedin_post_writer_agent` function to use OpenAI's GPT-4 (or a suitable LLM) to generate an engaging LinkedIn post based on the blog content or research data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6b8ecbc"
      },
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# LinkedIn Post Writer Agent: Generates engaging professional social content\n",
        "\n",
        "def linkedin_post_writer_agent(state):\n",
        "    \"\"\"\n",
        "    Generates engaging professional social content based on available content.\n",
        "    \"\"\"\n",
        "    print(\"---LINKEDIN POST WRITER AGENT---\")\n",
        "    query = state.query\n",
        "    research_data = state.research_data\n",
        "    blog_content = state.blog_content\n",
        "\n",
        "    # Use either blog content or research data if blog content is not available\n",
        "    content_to_summarize = blog_content if blog_content else research_data\n",
        "\n",
        "    if not content_to_summarize:\n",
        "        print(\"No content available to generate LinkedIn post.\")\n",
        "        return {\"linkedin_content\": \"Error: No content available to generate LinkedIn post.\"} # Return error in dictionary\n",
        "\n",
        "    print(f\"Generating LinkedIn post for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        return {\"linkedin_content\": \"Error: OPENAI_API_KEY not configured.\"}\n",
        "\n",
        "    # Construct a detailed prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert LinkedIn post writer. Your task is to create an engaging and concise professional LinkedIn post based on the following information.\n",
        "\n",
        "    User Query: {query}\n",
        "\n",
        "    Available Content (either blog post or research data):\n",
        "    {content_to_summarize}\n",
        "\n",
        "    Instructions:\n",
        "    1. Write a compelling and attention-grabbing opening line.\n",
        "    2. Summarize the key insights or takeaways from the provided content.\n",
        "    3. Use professional yet engaging language.\n",
        "    4. Include relevant hashtags (e.g., #marketing #contentmarketing #AI).\n",
        "    5. Keep the post concise and easy to read on a mobile feed.\n",
        "    6. Encourage engagement (e.g., ask a question, invite comments).\n",
        "\n",
        "    Generate the LinkedIn post content now.\n",
        "    \"\"\"\n",
        "\n",
        "    generated_linkedin_content = \"Error: LinkedIn post generation failed.\" # Default error message\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Chat Completions API\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", # Or another suitable model like \"gpt-3.5-turbo\"\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert LinkedIn post writer. Create engaging and concise professional posts.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_linkedin_content = response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating LinkedIn post with OpenAI API: {e}\")\n",
        "        generated_linkedin_content = f\"Error generating LinkedIn post: {e}\"\n",
        "\n",
        "\n",
        "    print(\"LinkedIn post generation complete. Returning update.\")\n",
        "    return {\"linkedin_content\": generated_linkedin_content} # Return a dictionary with updates"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19a69ba"
      },
      "source": [
        "## Update image generation agent\n",
        "\n",
        "### Subtask:\n",
        "Modify the `image_generation_agent` function to use OpenAI's DALL-E 3 (or a suitable image generation model) to create an image based on an optimized prompt derived from the query or content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5215bc38"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `image_generation_agent` function to use OpenAI's DALL-E 3 for image generation, including prompt optimization and error handling, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae7faed6"
      },
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Image Generation Agent: Produces custom visuals with prompt optimization\n",
        "\n",
        "def image_generation_agent(state):\n",
        "    \"\"\"\n",
        "    Produces custom visuals with prompt optimization based on the query or content.\n",
        "    \"\"\"\n",
        "    print(\"---IMAGE GENERATION AGENT---\")\n",
        "    query = state.query\n",
        "    blog_content = state.blog_content\n",
        "    linkedin_content = state.linkedin_content\n",
        "\n",
        "    # Use query, blog content, or LinkedIn content to generate an image prompt\n",
        "    content_for_prompt = blog_content if blog_content else linkedin_content if linkedin_content else query\n",
        "\n",
        "    if not content_for_prompt:\n",
        "        print(\"No content available to generate an image.\")\n",
        "        return {\"image_prompt\": \"Error: No content available for image generation.\", \"generated_image\": None}\n",
        "\n",
        "    print(f\"Generating image prompt for query/content: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        return {\"image_prompt\": \"Error: OPENAI_API_KEY not configured.\", \"generated_image\": None}\n",
        "\n",
        "    optimized_prompt = \"Error: Image prompt optimization failed.\" # Default error message\n",
        "    generated_image_data = \"Error: Image generation failed.\" # Default error message\n",
        "\n",
        "    try:\n",
        "        # Optional: Use an LLM to optimize the prompt before sending to DALL-E 3\n",
        "        print(\"Optimizing image prompt using LLM...\")\n",
        "        prompt_optimization_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", # Or another suitable model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at creating concise and effective prompts for image generation models like DALL-E 3. Your goal is to take provided content and turn it into a visually compelling image description.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Create a detailed and creative prompt for an image generation model based on the following content:\\n\\n{content_for_prompt}\\n\\nEnsure the prompt is visually descriptive and suitable for generating a professional and engaging image.\"}\n",
        "            ],\n",
        "             temperature=0.7,\n",
        "             max_tokens=150\n",
        "        )\n",
        "        optimized_prompt = prompt_optimization_response.choices[0].message['content']\n",
        "        print(f\"Optimized Prompt: {optimized_prompt}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error optimizing image prompt with OpenAI API: {e}\")\n",
        "        optimized_prompt = f\"Error optimizing prompt for image: {e}\"\n",
        "        # Continue to try image generation with a basic prompt if optimization fails\n",
        "        if optimized_prompt.startswith(\"Error\"):\n",
        "             optimized_prompt = f\"Image related to: {content_for_prompt[:200]}...\" # Fallback prompt\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Image Generation API (DALL-E 3)\n",
        "        print(\"Generating image using DALL-E 3...\")\n",
        "        image_response = openai.Image.create(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=optimized_prompt,\n",
        "            size=\"1024x1024\", # Or other desired size: \"1792x1024\" or \"1024x1792\"\n",
        "            quality=\"standard\", # Or \"hd\"\n",
        "            n=1 # Number of images to generate (DALL-E 3 currently supports n=1)\n",
        "        )\n",
        "        generated_image_data = image_response.data[0].url # Get the image URL\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image with DALL-E 3 API: {e}\")\n",
        "        generated_image_data = f\"Error generating image: {e}\"\n",
        "\n",
        "\n",
        "    print(\"Image generation complete. Returning update.\")\n",
        "    return {\"image_prompt\": optimized_prompt, \"generated_image\": generated_image_data} # Return a dictionary with updates"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1cfb4a0"
      },
      "source": [
        "## Update content strategist agent\n",
        "\n",
        "### Subtask:\n",
        "Modify the `content_strategist_agent` function to refine the formatting and organization of the final output using an LLM if necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562b1ac3"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `content_strategist_agent` function to use an LLM for formatting and organizing the final output, including checking for the OpenAI API key and handling API calls and errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15219be0"
      },
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Content Strategist Agent: Formats and organizes content\n",
        "\n",
        "def content_strategist_agent(state):\n",
        "    \"\"\"\n",
        "    Formats and organizes research and generated content into a final output using an LLM.\n",
        "    \"\"\"\n",
        "    print(\"---CONTENT STRATEGIST AGENT---\")\n",
        "    query = state.query\n",
        "    research_data = state.research_data\n",
        "    blog_content = state.blog_content\n",
        "    linkedin_content = state.linkedin_content\n",
        "    image_prompt = state.image_prompt\n",
        "    generated_image = state.generated_image\n",
        "\n",
        "    print(\"Organizing and formatting final content using LLM.\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        print(\"\\n---FINAL CONTENT OUTPUT (Raw State Data)---\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Research Data: {research_data}\")\n",
        "        print(f\"Blog Content: {blog_content}\")\n",
        "        print(f\"LinkedIn Content: {linkedin_content}\")\n",
        "        print(f\"Image Prompt: {image_prompt}\")\n",
        "        print(f\"Generated Image Data: {generated_image}\")\n",
        "        return {} # Indicate end of processing for this path\n",
        "\n",
        "    # Construct a detailed prompt for the LLM to format the output\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert content strategist and formatter. Your task is to take the following raw content components generated by different agents and organize and format them into a cohesive, professional, and easy-to-read final output.\n",
        "\n",
        "    Original User Query: {query}\n",
        "\n",
        "    Research Data:\n",
        "    {research_data if research_data else \"No research data available.\"}\n",
        "\n",
        "    Blog Content:\n",
        "    {blog_content if blog_content else \"No blog content generated.\"}\n",
        "\n",
        "    LinkedIn Post:\n",
        "    {linkedin_content if linkedin_content else \"No LinkedIn post generated.\"}\n",
        "\n",
        "    Image Information:\n",
        "    Image Prompt used: {image_prompt if image_prompt else \"No image prompt generated.\"}\n",
        "    Generated Image Data (URL or info): {generated_image if generated_image else \"No image generated.\"}\n",
        "\n",
        "    Instructions:\n",
        "    1. Start with a clear heading indicating the content is for the original query.\n",
        "    2. Present the Research Data in a structured way (e.g., bullet points, summary).\n",
        "    3. Present the Blog Content, ensuring it's clearly labeled.\n",
        "    4. Present the LinkedIn Post, clearly labeled.\n",
        "    5. Present the Image Information, including the prompt used and the generated image data (e.g., URL).\n",
        "    6. Ensure the overall output is well-formatted with clear sections and easy to read.\n",
        "    7. Add a concluding remark.\n",
        "\n",
        "    Format the final content output now.\n",
        "    \"\"\"\n",
        "\n",
        "    final_content_output = \"Error: Final content formatting failed.\" # Default error message\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Chat Completions API\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", # Or another suitable model like \"gpt-3.5-turbo\"\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert content strategist and formatter.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        final_content_output = response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting content with OpenAI API: {e}\")\n",
        "        final_content_output = f\"Error formatting content: {e}\\n\\nRaw State Data:\\nQuery: {query}\\nResearch Data: {research_data}\\nBlog Content: {blog_content}\\nLinkedIn Content: {linkedin_content}\\nImage Prompt: {image_prompt}\\nGenerated Image Data: {generated_image}\"\n",
        "\n",
        "\n",
        "    print(\"\\n---FINAL CONTENT OUTPUT---\")\n",
        "    print(final_content_output)\n",
        "\n",
        "    # The content strategist is the last step in this example, so it doesn't return a next node.\n",
        "    # Returning an empty dictionary signifies the end of this branch of the graph and updates the state\n",
        "    # with any changes made within this node (though in this case, we're just formatting for output).\n",
        "    return {}"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be5df0c7"
      },
      "source": [
        "## Test the workflow\n",
        "\n",
        "### Subtask:\n",
        "Run the compiled graph with different queries to test the implemented logic and ensure the agents are working correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2967b2f8"
      },
      "source": [
        "**Reasoning**:\n",
        "The compiled graph `app` is available from previous steps. I need to define a sample query, initialize the GraphState, invoke the graph, and print the final state. I will do this for one query first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "d964c258",
        "outputId": "ad8fb644-3b6a-4aa7-cdbe-dce96302d960"
      },
      "source": [
        "# Run the compiled graph with a sample query\n",
        "\n",
        "# Define a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "\n",
        "# Initialize the GraphState with the sample query\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "# Invoke the compiled graph\n",
        "# The output will be the state after the workflow has run\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n--- Final State ---\")\n",
        "print(final_state)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running workflow for query: research about the benefits of content marketing ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1137651403.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Invoke the compiled graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# The output will be the state after the workflow has run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Final State ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3027\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         ]\n\u001b[0;32m---> 87\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 )\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58740172"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to run the graph failed because the `query_handler_agent` function was returning a string (\"deep_research\", \"seo_blog_writer\", etc.) instead of a dictionary to update the state as expected by LangGraph. The `query_handler_agent` function should return a dictionary that indicates the next node to transition to, not just the node name as a string. I need to fix the return value of the `query_handler_agent` function to return a dictionary. Then I will try running the graph again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "fqEKCeeilu1_",
        "outputId": "5924ccc4-825e-4fd6-e93e-522e3c75e406"
      },
      "source": [
        "# Define the Query Handler Agent\n",
        "def query_handler_agent(state):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to.\n",
        "    \"\"\"\n",
        "    print(\"---QUERY HANDLER AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    # Basic routing logic (can be expanded)\n",
        "    if \"research\" in query.lower():\n",
        "        print(\"Routing to Deep Research Agent\")\n",
        "        # Return a dictionary with the next node key\n",
        "        return {\"next\": \"deep_research\"}\n",
        "    elif \"blog\" in query.lower():\n",
        "        print(\"Routing to SEO Blog Writer Agent\")\n",
        "        # Return a dictionary with the next node key\n",
        "        return {\"next\": \"seo_blog_writer\"}\n",
        "    elif \"linkedin\" in query.lower():\n",
        "        print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "        # Return a dictionary with the next node key\n",
        "        return {\"next\": \"linkedin_post_writer\"}\n",
        "    elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "        print(\"Routing to Image Generation Agent\")\n",
        "        # Return a dictionary with the next node key\n",
        "        return {\"next\": \"image_generation\"}\n",
        "    else:\n",
        "        print(\"Routing to Content Strategist Agent (default)\")\n",
        "        # Return a dictionary with the next node key\n",
        "        return {\"next\": \"content_strategist\"}\n",
        "\n",
        "# Re-compile the graph with the corrected agent function\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# Define the edges (transitions)\n",
        "\n",
        "# The query handler conditionally routes to different agents\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\", # From the query_handler node\n",
        "    lambda state: state[\"next\"], # The condition is based on the \"next\" key in the state update\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# After Deep Research, proceed to Content Strategist (or another relevant agent/decision point)\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "\n",
        "# After SEO Blog Writer, proceed to Content Strategist (or another relevant agent/decision point, maybe image generation?)\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\") # Or could go to \"image_generation\"\n",
        "\n",
        "# After LinkedIn Post Writer, proceed to Content Strategist (or maybe image generation?)\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\") # Or could go to \"image_generation\"\n",
        "\n",
        "# After Image Generation, proceed to Content Strategist\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Define the end point(s)\n",
        "# The content strategist node implicitly ends the main workflow for this example\n",
        "# If you had multiple end points, you would define them like:\n",
        "# workflow.add_edge(\"content_strategist\", END)\n",
        "\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with corrected query_handler!\")\n",
        "\n",
        "# Run the compiled graph with a sample query again\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n--- Final State ---\")\n",
        "print(final_state)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with corrected query_handler!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-621662207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Running workflow for query: {sample_query} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Final State ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3027\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         ]\n\u001b[0;32m---> 87\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 )\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7632daa"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists even after correcting the `query_handler_agent` to return a dictionary. The traceback indicates the error is still happening when processing the return value of the `query_handler_agent`. Looking closely at the `add_conditional_edges` definition, the `lambda state: state` is incorrect for the condition. It should be accessing the key returned by the `query_handler_agent` that determines the next state, which I've now set to be `\"next\"`. I need to correct the lambda function in `add_conditional_edges` to access `state[\"next\"]`. Then I will re-compile the graph and run it with the sample query again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "YupELDb_lxp_",
        "outputId": "91755265-0f60-438c-c82c-3c23bd8b2c3b"
      },
      "source": [
        "# Re-compile the graph with the corrected conditional edge logic\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# Define the edges (transitions)\n",
        "\n",
        "# The query handler conditionally routes to different agents\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\", # From the query_handler node\n",
        "    lambda state: state[\"next\"], # The condition is based on the \"next\" key in the state update\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# After Deep Research, proceed to Content Strategist (or another relevant agent/decision point)\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "\n",
        "# After SEO Blog Writer, proceed to Content Strategist (or another relevant agent/decision point, maybe image generation?)\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\") # Or could go to \"image_generation\"\n",
        "\n",
        "# After LinkedIn Post Writer, proceed to Content Strategist (or maybe image generation?)\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\") # Or could go to \"image_generation\"\n",
        "\n",
        "# After Image Generation, proceed to Content Strategist\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Define the end point(s)\n",
        "# The content strategist node implicitly ends the main workflow for this example\n",
        "# If you had multiple end points, you would define them like:\n",
        "# workflow.add_edge(\"content_strategist\", END)\n",
        "\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with corrected conditional edge!\")\n",
        "\n",
        "# Run the compiled graph with a sample query again\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n--- Final State ---\")\n",
        "print(final_state)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with corrected conditional edge!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2860517131.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Running workflow for query: {sample_query} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Final State ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3027\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         ]\n\u001b[0;32m---> 87\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 )\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93faf89e"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists, indicating that the `query_handler_agent` is still not returning a dictionary in a way that LangGraph expects for conditional edges, even though the code appears to be returning `{\"next\": \"...\"}`. This might be related to how LangGraph's `StateGraph` handles state updates and transitions, particularly with the conditional edges. The `query_handler_agent` is intended *only* to determine the next node, not update the main state with general information. LangGraph's conditional edges work by the node returning a string corresponding to the next node name, not a dictionary. The previous fix was based on a misunderstanding of how conditional edges use the node's return value. The `query_handler_agent` should return the string of the next node name directly. I need to revert the `query_handler_agent` to return just the string name of the next node and keep the `add_conditional_edges` lambda function accessing `state[\"next\"]`. This is confusing, but the error message strongly suggests the *return value* of the node itself is the issue, not just how the conditional edge reads the state. Let's try returning the string again, but re-examine the conditional edge logic if that doesn't work.\n",
        "\n",
        "Let's try returning the string name of the next node from the `query_handler_agent` and see if that resolves the error. I will also keep the conditional edge logic as it was in the first attempt, which uses `lambda state: state`. This was the original setup before the error occurred, and the error message specifically pointed to the return value of the node.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "sGb6fo0El2TL",
        "outputId": "7d430ebf-022d-4e52-b301-306193048d07"
      },
      "source": [
        "# Define the Query Handler Agent - Reverting to returning a string\n",
        "def query_handler_agent(state):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to.\n",
        "    \"\"\"\n",
        "    print(\"---QUERY HANDLER AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    # Basic routing logic (can be expanded)\n",
        "    if \"research\" in query.lower():\n",
        "        print(\"Routing to Deep Research Agent\")\n",
        "        # Return the string name of the next node\n",
        "        return \"deep_research\"\n",
        "    elif \"blog\" in query.lower():\n",
        "        print(\"Routing to SEO Blog Writer Agent\")\n",
        "        # Return the string name of the next node\n",
        "        return \"seo_blog_writer\"\n",
        "    elif \"linkedin\" in query.lower():\n",
        "        print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "        # Return the string name of the next node\n",
        "        return \"linkedin_post_writer\"\n",
        "    elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "        print(\"Routing to Image Generation Agent\")\n",
        "        # Return the string name of the next node\n",
        "        return \"image_generation\"\n",
        "    else:\n",
        "        print(\"Routing to Content Strategist Agent (default)\")\n",
        "        # Return the string name of the next node\n",
        "        return \"content_strategist\"\n",
        "\n",
        "# Re-compile the graph with the reverted query_handler and original conditional edge logic\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# Define the edges (transitions)\n",
        "\n",
        "# The query handler conditionally routes to different agents\n",
        "# The condition should evaluate the return value of the query_handler_agent\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\", # From the query_handler node\n",
        "    lambda x: x, # The condition is the return value of the query_handler_agent (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# After Deep Research, proceed to Content Strategist (or another relevant agent/decision point)\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "\n",
        "# After SEO Blog Writer, proceed to Content Strategist (or another relevant agent/decision point, maybe image generation?)\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\") # Or could go to \"image_generation\"\n",
        "\n",
        "# After LinkedIn Post Writer, proceed to Content Strategist (or maybe image generation?)\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\") # Or could go to \"image_generation\"\n",
        "\n",
        "# After Image Generation, proceed to Content Strategist\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Define the end point(s)\n",
        "# The content strategist node implicitly ends the main workflow for this example\n",
        "# If you had multiple end points, you would define them like:\n",
        "# workflow.add_edge(\"content_strategist\", END)\n",
        "\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with potentially corrected query_handler and conditional edge!\")\n",
        "\n",
        "# Run the compiled graph with a sample query again\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n--- Final State ---\")\n",
        "print(final_state)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with potentially corrected query_handler and conditional edge!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1689432680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Running workflow for query: {sample_query} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Final State ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3027\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         ]\n\u001b[0;32m---> 87\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 )\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7602027"
      },
      "source": [
        "## Test the workflow retry 1\n",
        "\n",
        "### Subtask:\n",
        "Retry running the compiled graph with a sample query to test the implemented logic and ensure the agents are working correctly, addressing the `InvalidUpdateError`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8912cb4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to run the graph failed with an `InvalidUpdateError`. The error message indicates that the `query_handler_agent` is returning a `GraphState` object when a dictionary or string (depending on the edge type) is expected. The `query_handler_agent` function as defined currently returns a string which is correct for the conditional edge. The issue might be in how LangGraph handles the initial state passed to `app.invoke` and the subsequent processing within the graph's first node. Let's explicitly return a dictionary from `query_handler_agent` that updates the state with the determined next node and then use a regular edge from `query_handler` to a new routing node, or re-examine the conditional edge logic to ensure it's correctly interpreting the string output as the next node name.\n",
        "\n",
        "Let's re-examine the conditional edge definition and the `query_handler_agent`. The `query_handler_agent` returns a string which is the name of the next node. The `add_conditional_edges` function is designed to use this string return value to determine the next step. The lambda `lambda x: x` is correct for this purpose as it simply passes the output of the `query_handler_agent` as the key for the conditional routing dictionary. The error message \"Expected dict, got GraphState(...)\" suggests the error might be occurring *after* the conditional edge has determined the next node, specifically when the state is being updated by the subsequent node. However, the traceback points to the `_assemble_writes` function within LangGraph's internal state handling, which is triggered by the return value of a node.\n",
        "\n",
        "Let's carefully review the `query_handler_agent` function again. It does not modify the state and correctly returns a string. The issue might be with how the initial state is being handled when the graph starts. The `app.invoke` function takes the initial state. LangGraph might be trying to process this initial state as an update from the first node before even calling the first node.\n",
        "\n",
        "A common pattern in LangGraph is for the first node to receive the initial state and then return a dictionary to update the state for the next step. However, the `query_handler_agent`'s purpose is purely routing.\n",
        "\n",
        "Let's try a slight modification to the `query_handler_agent` to ensure it explicitly returns a dictionary, even if that dictionary only contains information about the next step, which can then be used by a subsequent node or a different type of edge. This deviates from the standard conditional edge pattern but might help isolate the source of the `InvalidUpdateError`.\n",
        "\n",
        "Alternatively, the error might be related to how the `GraphState` class is defined or initialized. Let's ensure the `GraphState` class is correctly defined and that its attributes are initialized properly (which they appear to be).\n",
        "\n",
        "Let's try to simplify the graph execution and the `query_handler_agent` return value to see if we can pinpoint the issue. We will modify the `query_handler_agent` to return a dictionary that indicates the chosen route, and then use a regular edge to a separate routing logic node if necessary, or see if returning a dictionary directly from the first node resolves the `InvalidUpdateError`.\n",
        "\n",
        "Given the traceback points to the state update mechanism (`_assemble_writes`), it's highly likely that the issue is with the return type of the node function not being a dictionary as expected by the state update logic in certain scenarios, even when using conditional edges. Although the documentation suggests a string return is valid for conditional edges, there might be an interaction with the initial state or other graph configurations causing this error.\n",
        "\n",
        "Let's modify the `query_handler_agent` to return a dictionary containing the determined next node name under a specific key, say `'next_node'`, and then adjust the conditional edge to read this key from the dictionary. This ensures the node always returns a dictionary, which is the standard way nodes update the state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "238b364a",
        "outputId": "9c954767-4ed6-4d13-ec78-572ffbd4e414"
      },
      "source": [
        "# Define the Query Handler Agent - Modified to return a dictionary\n",
        "def query_handler_agent(state):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns the next node name in a dictionary.\n",
        "    \"\"\"\n",
        "    print(\"---QUERY HANDLER AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "\n",
        "    print(f\"Query Handler returning next_node: {next_node}\")\n",
        "    # Return a dictionary containing the next node name\n",
        "    return {\"next_node\": next_node}\n",
        "\n",
        "# Re-compile the graph with the modified query_handler and updated conditional edge logic\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# Define the edges (transitions)\n",
        "\n",
        "# The query handler conditionally routes to different agents based on the 'next_node' key in the returned dictionary\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\", # From the query_handler node\n",
        "    lambda state: state.get(\"next_node\"), # The condition is the value of the 'next_node' key in the state update from the query_handler\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with modified query_handler and conditional edge!\")\n",
        "\n",
        "# Run the compiled graph with a sample query again\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the GraphState with the sample query\n",
        "# When invoking the graph, the initial state is passed to the entry point node.\n",
        "# The entry point node's return value should be a dictionary to update the state.\n",
        "# The query_handler_agent now returns a dictionary, which should be compatible.\n",
        "initial_state_dict = {\"query\": sample_query} # Pass initial data as a dictionary\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial state dictionary\n",
        "    final_state = app.invoke(initial_state_dict)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # LangGraph invoke returns the final state as a dictionary\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n",
        "    # We can try to access the state managed by the graph if needed for debugging,\n",
        "    # but app.invoke generally returns the final state on successful completion.\n",
        "    # For errors during execution, examining the traceback is key.\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with modified query_handler and conditional edge!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---QUERY HANDLER AGENT---\n",
            "Query Handler returning next_node: content_strategist\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: AttributeError\n",
            "Error Message: 'GraphState' object has no attribute 'get'\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6453d2c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt resulted in an `AttributeError: 'GraphState' object has no attribute 'get'`. This error occurs in the lambda function `lambda state: state.get(\"next_node\")` used in `add_conditional_edges`. The `state` object passed to this lambda function is the `GraphState` object itself, not the dictionary returned by the `query_handler_agent`. The `GraphState` object does not have a `.get()` method like a dictionary does.\n",
        "\n",
        "The conditional edge needs to read the return value of the `query_handler_agent` to decide the next node. The return value of a node in LangGraph is used to update the state. In this case, the `query_handler_agent` returns `{\"next_node\": next_node}`, which updates the `GraphState` by adding/updating the `next_node` key. The `add_conditional_edges` function, when used with a lambda on the *source* node (in this case, `query_handler`), should be evaluating the *output* of that node, not the full state *after* the update.\n",
        "\n",
        "Let's revert the `query_handler_agent` back to returning a string, which is the standard way to use conditional edges for routing based on the node's direct output. The lambda in `add_conditional_edges` then simply needs to return this string output. The `InvalidUpdateError` from before might have been related to how the initial state was being processed, or a subtle interaction within LangGraph. Let's try returning the string again, as it's the intended pattern for conditional routing based on node output.\n",
        "\n",
        "Let's first try reverting the `query_handler_agent` to return a string and use the simple `lambda x: x` for the conditional edge, as this is the documented pattern. If this brings back the `InvalidUpdateError`, we will need to investigate further or consider a different approach to the initial routing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6787216",
        "outputId": "39eb4b25-e9e0-4c30-8601-1a7348c03f40"
      },
      "source": [
        "# Define the Query Handler Agent - Reverted to returning a string node name\n",
        "def query_handler_agent(state):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns the next node name as a string.\n",
        "    \"\"\"\n",
        "    print(\"---QUERY HANDLER AGENT---\")\n",
        "    query = state.query\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "\n",
        "    print(f\"Query Handler returning next_node: {next_node}\")\n",
        "    # Return the string name of the next node\n",
        "    return next_node\n",
        "\n",
        "# Re-compile the graph with the reverted query_handler and original conditional edge logic\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# Define the edges (transitions)\n",
        "\n",
        "# The query handler conditionally routes to different agents based on the string returned by query_handler_agent\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\", # From the query_handler node\n",
        "    lambda x: x, # The condition is the return value of the query_handler_agent (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with reverted query_handler and conditional edge!\")\n",
        "\n",
        "# Run the compiled graph with a sample query again\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the GraphState with the sample query\n",
        "# The initial state is passed to the entry point node.\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial state object\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # LangGraph invoke returns the final state as a dictionary\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback for more details\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n",
        "    # We can try to access the state managed by the graph if needed for debugging,\n",
        "    # but app.invoke generally returns the final state on successful completion.\n",
        "    # For errors during execution, examining the traceback is key.\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with reverted query_handler and conditional edge!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: InvalidUpdateError\n",
            "Error Message: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-972448263.py\", line 87, in <cell line: 0>\n",
            "    final_state = app.invoke(initial_state)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
            "    for chunk in self.stream(\n",
            "                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
            "    for _ in runner.tick(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
            "    run_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
            "    return task.proc.invoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
            "    ret = self.func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 87, in _write\n",
            "    self.do_write(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 129, in do_write\n",
            "    write(_assemble_writes(writes))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 184, in _assemble_writes\n",
            "    if ww := w.mapper(w.value):\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\", line 983, in _get_updates\n",
            "    raise InvalidUpdateError(msg)\n",
            "langgraph.errors.InvalidUpdateError: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "During task with name '__start__' and id 'f69c9faf-5374-fbe0-3e15-6a0b7a3abf92'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3405e4c"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `InvalidUpdateError: Expected dict, got GraphState(...)` persists even after reverting the `query_handler_agent` to return a string and using the standard conditional edge logic (`lambda x: x`). The traceback indicates the error is happening during the state update process, specifically in `_assemble_writes` within LangGraph's pregel execution. This is happening at the `__start__` task, which suggests the very first step of processing the initial state is causing the issue.\n",
        "\n",
        "This error usually means that a node returned something other than a dictionary when it was expected to return a dictionary to update the state. In this case, the `query_handler_agent` returns a string, which *should* be handled by the conditional edge logic for routing and not directly used to update the state. However, the error message explicitly says it got a `GraphState` object, not a string. This is very strange, as the `query_handler_agent`'s code clearly returns a string.\n",
        "\n",
        "This might indicate a misunderstanding of how LangGraph handles the initial state when it's a custom object like `GraphState` and how it interacts with the first node's return value. It's possible that when the graph starts, it implicitly tries to use the initial `GraphState` object to perform an 'update' before the first node's output is processed by the conditional edge, leading to this error because a `GraphState` object is not a dictionary of updates.\n",
        "\n",
        "Let's try initializing the graph with a dictionary containing the query instead of a `GraphState` object. LangGraph often works more seamlessly when the state is managed as a dictionary internally, and the custom `GraphState` class is used to define the schema and potentially add methods. By initializing with a dictionary, we ensure the initial state is in the expected format for state updates. The `GraphState` class can still be used within the nodes for type hinting and easier attribute access (LangGraph can often work with attribute access on dictionary-based states if the schema is defined).\n",
        "\n",
        "We will modify the invocation part to pass a dictionary `{ \"query\": sample_query }` instead of `GraphState(query=sample_query)`. The agent functions should still be able to access state attributes like `state.query` if LangGraph's state management handles this for dictionary-backed states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwp9rakjmKOk",
        "outputId": "f77c54f8-3409-4490-94b2-c37e7ed478d6"
      },
      "source": [
        "# Re-compile the graph (using the query_handler_agent that returns a string)\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes (agents) to the graph\n",
        "workflow.add_node(\"query_handler\", query_handler_agent)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "workflow.set_entry_point(\"query_handler\")\n",
        "\n",
        "# Define the edges (transitions)\n",
        "# The query handler conditionally routes to different agents based on the string returned by query_handler_agent\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_handler\", # From the query_handler node\n",
        "    lambda x: x, # The condition is the return value of the query_handler_agent (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully!\")\n",
        "\n",
        "# Run the compiled graph with a sample query again\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "# LangGraph can often work with dictionary inputs which are then used to initialize the state.\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    # The state will be initialized with this dictionary.\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # LangGraph invoke generally returns the final state as a dictionary when using dict-based state\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback for more details\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---QUERY HANDLER AGENT---\n",
            "Query Handler returning next_node: content_strategist\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: InvalidUpdateError\n",
            "Error Message: Expected dict, got content_strategist\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-108444900.py\", line 58, in <cell line: 0>\n",
            "    final_state = app.invoke(initial_input_data)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
            "    for chunk in self.stream(\n",
            "                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
            "    for _ in runner.tick(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
            "    run_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
            "    return task.proc.invoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 659, in invoke\n",
            "    input = step.invoke(input, config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
            "    ret = self.func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 87, in _write\n",
            "    self.do_write(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 129, in do_write\n",
            "    write(_assemble_writes(writes))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 184, in _assemble_writes\n",
            "    if ww := w.mapper(w.value):\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\", line 983, in _get_updates\n",
            "    raise InvalidUpdateError(msg)\n",
            "langgraph.errors.InvalidUpdateError: Expected dict, got content_strategist\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "During task with name 'query_handler' and id '9c4688dc-2ea9-2963-4421-39a86cef112e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93889785"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message has changed slightly, now saying `Expected dict, got content_strategist` and the traceback points to the `query_handler` task. This confirms that the `query_handler_agent` is correctly returning the string \"content_strategist\", but LangGraph's state update mechanism is trying to interpret this string as a state update dictionary, which is incorrect for how conditional edges are supposed to work based on the node's return value.\n",
        "\n",
        "This behavior is unexpected based on the standard usage of `add_conditional_edges` where the node's return value is a string representing the next node name. It suggests there might be a specific requirement or nuance when using a custom `GraphState` class or with the specific version of LangGraph being used in this environment that is causing this conflict.\n",
        "\n",
        "Since directly returning the string for conditional routing is causing an `InvalidUpdateError` by being misinterpreted as a state update, and returning a dictionary for routing caused an `AttributeError`, we need a different approach to handle the initial routing based on the query.\n",
        "\n",
        "A robust way to handle initial routing when the standard conditional edge on the entry node is problematic is to introduce a dedicated \"router\" node immediately after the entry point. The entry point node will receive the initial state (either `GraphState` or a dictionary), potentially perform some initial processing if needed (though not necessary for just routing), and return the full state (or a dictionary representing the state). A subsequent \"router\" node will then receive this state, determine the next step based on the query, and return a string representing the next node name. The conditional edge will then be on this \"router\" node.\n",
        "\n",
        "Let's add a simple `initial_processing_node` as the entry point that just passes the state along, and a `router_node` that replicates the logic of the original `query_handler_agent` but is not the entry point. The conditional edge will be from this `router_node`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeGyZYvlmQMZ",
        "outputId": "98ee727d-9199-401d-b04e-47056a12c781"
      },
      "source": [
        "# Define a simple initial processing node that just passes the state\n",
        "def initial_processing_node(state):\n",
        "    \"\"\"\n",
        "    A simple node that receives the initial state and passes it along.\n",
        "    \"\"\"\n",
        "    print(\"---INITIAL PROCESSING NODE---\")\n",
        "    # In a real scenario, this might perform initial validation or setup\n",
        "    # For now, it just returns the current state as a dictionary for the next node\n",
        "    # LangGraph nodes should return dictionaries to update the state.\n",
        "    # If state is a GraphState object, convert it to a dictionary.\n",
        "    if isinstance(state, GraphState):\n",
        "         # Convert GraphState to dict - assuming attributes match state keys\n",
        "         state_dict = state.__dict__\n",
        "         # Remove private attributes if any, although our GraphState doesn't have them\n",
        "         state_dict = {k: v for k, v in state_dict.items() if not k.startswith('_')}\n",
        "         return state_dict\n",
        "    elif isinstance(state, dict):\n",
        "         return state # If already a dict, return as is\n",
        "    else:\n",
        "         print(f\"Warning: initial_processing_node received unexpected state type: {type(state)}\")\n",
        "         return {\"query\": None} # Return a default state if type is unexpected\n",
        "\n",
        "\n",
        "# Define a dedicated Router Agent node\n",
        "def router_node(state):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns the next node name as a string.\n",
        "    This node is placed after the entry point.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Access the query from the state dictionary\n",
        "    query = state.get(\"query\")\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "\n",
        "    print(f\"Router node returning next_node: {next_node}\")\n",
        "    # Return the string name of the next node for conditional routing\n",
        "    return next_node\n",
        "\n",
        "\n",
        "# Re-compile the graph with the new structure: initial_processing_node -> router_node -> ...\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes\n",
        "workflow.add_node(\"initial_processing\", initial_processing_node)\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the initial processing node\n",
        "workflow.set_entry_point(\"initial_processing\")\n",
        "\n",
        "# Add a regular edge from initial_processing to the router\n",
        "workflow.add_edge(\"initial_processing\", \"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on its string return value\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda x: x, # The condition is the return value of the router_node (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with initial processing and router nodes!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "# This dictionary will be passed to the initial_processing_node\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # LangGraph invoke generally returns the final state as a dictionary when using dict-based state\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback for more details\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with initial processing and router nodes!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---INITIAL PROCESSING NODE---\n",
            "---ROUTER NODE---\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: AttributeError\n",
            "Error Message: 'GraphState' object has no attribute 'get'\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3935307993.py\", line 118, in <cell line: 0>\n",
            "    final_state = app.invoke(initial_input_data)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
            "    for chunk in self.stream(\n",
            "                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
            "    for _ in runner.tick(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
            "    run_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
            "    return task.proc.invoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
            "    input = context.run(step.invoke, input, config, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
            "    ret = self.func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3935307993.py\", line 32, in router_node\n",
            "    query = state.get(\"query\")\n",
            "            ^^^^^^^^^\n",
            "AttributeError: 'GraphState' object has no attribute 'get'\n",
            "During task with name 'router' and id '58e6d3d6-4156-ed30-3113-ca582b4a363d'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ce8e67"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback shows an `AttributeError: 'GraphState' object has no attribute 'get'` occurring in the `router_node`. This means that even though we are initializing the graph with a dictionary `{\"query\": sample_query}`, the state object being passed to the `router_node` is still a `GraphState` object, not a dictionary. This is surprising, as the `initial_processing_node` was designed to return a dictionary representation of the state.\n",
        "\n",
        "The `GraphState` class is defined with attributes like `query`, `research_data`, etc. When LangGraph uses a custom state class with `StateGraph`, it manages an instance of this class. The nodes receive and can modify this instance. When a node returns a dictionary, LangGraph uses that dictionary to update the attributes of the `GraphState` instance.\n",
        "\n",
        "The `AttributeError` in `router_node` indicates that `state` within that function is the `GraphState` object. The previous attempt to convert `GraphState` to a dictionary in `initial_processing_node` seems not to have resulted in a dictionary being passed to the `router_node`. This might be due to how LangGraph's state management merges updates back into the `GraphState` instance.\n",
        "\n",
        "The most reliable way for nodes to interact with the state when using a custom `GraphState` is to access and modify its attributes directly, rather than treating it as a dictionary. The `router_node` should access the query using `state.query` instead of `state.get(\"query\")`. Similarly, other agents like `deep_research_agent`, `seo_blog_writer_agent`, etc., should also access state using attribute access (`state.query`, `state.research_data`, etc.) and return dictionaries to update the state. All agent functions are already doing this correctly, except for the `router_node` which was modified in the last step.\n",
        "\n",
        "Let's revert the `router_node` to access the query using `state.query`. The `initial_processing_node` might still be useful to ensure the initial dictionary input correctly populates the `GraphState` instance's attributes, although it might not need to return a dictionary itself if LangGraph handles the initial state mapping automatically. Let's keep the `initial_processing_node` as is for now, as it correctly handles both dictionary and `GraphState` inputs, and focus on fixing the `router_node`.\n",
        "\n",
        "We will modify the `router_node` to use attribute access for the state. The graph structure with `initial_processing` -> `router` -> ... and the conditional edge on `router` returning a string should be correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I66rBlIomWRm",
        "outputId": "cda75647-1baa-4abe-ba6b-814382c0f499"
      },
      "source": [
        "# Define a simple initial processing node that just passes the state\n",
        "# Keeping this node as it might help with initializing the GraphState from a dictionary input\n",
        "def initial_processing_node(state):\n",
        "    \"\"\"\n",
        "    A simple node that receives the initial state and passes it along.\n",
        "    Ensures state is in a consistent format (dictionary) for subsequent nodes.\n",
        "    \"\"\"\n",
        "    print(\"---INITIAL PROCESSING NODE---\")\n",
        "    # LangGraph should handle mapping dictionary input to GraphState attributes\n",
        "    # This node can simply return the state as is, or explicitly return a dictionary\n",
        "    # Let's try returning the state as is, relying on LangGraph's state management\n",
        "    print(f\"Initial state received by initial_processing_node: {state}\")\n",
        "    # No explicit return needed if the node just receives and the next node reads the state\n",
        "    # However, returning the state explicitly can sometimes clarify flow.\n",
        "    # Let's explicitly return the state as a dictionary to be safe, as done previously.\n",
        "    if isinstance(state, GraphState):\n",
        "         state_dict = state.__dict__\n",
        "         state_dict = {k: v for k, v in state_dict.items() if not k.startswith('_')}\n",
        "         return state_dict\n",
        "    elif isinstance(state, dict):\n",
        "         return state\n",
        "    else:\n",
        "         print(f\"Warning: initial_processing_node received unexpected state type: {type(state)}\")\n",
        "         return {\"query\": None}\n",
        "\n",
        "\n",
        "# Define a dedicated Router Agent node - Modified to use attribute access\n",
        "def router_node(state: GraphState): # Add type hint for clarity\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns the next node name as a string.\n",
        "    This node is placed after the entry point.\n",
        "    Accesses state using attribute access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Access the query from the state using attribute access\n",
        "    query = state.query\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning next_node: {next_node}\")\n",
        "    # Return the string name of the next node for conditional routing\n",
        "    return next_node\n",
        "\n",
        "\n",
        "# Re-compile the graph with the modified router_node\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes\n",
        "workflow.add_node(\"initial_processing\", initial_processing_node)\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the initial processing node\n",
        "workflow.set_entry_point(\"initial_processing\")\n",
        "\n",
        "# Add a regular edge from initial_processing to the router\n",
        "workflow.add_edge(\"initial_processing\", \"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on its string return value\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda x: x, # The condition is the return value of the router_node (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with router_node using attribute access!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "# This dictionary will be passed to the initial_processing_node\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # LangGraph invoke generally returns the final state as a dictionary when using dict-based state\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback for more details\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with router_node using attribute access!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---INITIAL PROCESSING NODE---\n",
            "Initial state received by initial_processing_node: GraphState(query='None', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\n",
            "---ROUTER NODE---\n",
            "No query in state, routing to Content Strategist (default)\n",
            "Router node returning next_node: content_strategist\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: InvalidUpdateError\n",
            "Error Message: Expected dict, got content_strategist\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-458013153.py\", line 121, in <cell line: 0>\n",
            "    final_state = app.invoke(initial_input_data)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
            "    for chunk in self.stream(\n",
            "                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
            "    for _ in runner.tick(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
            "    run_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
            "    return task.proc.invoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 659, in invoke\n",
            "    input = step.invoke(input, config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
            "    ret = self.func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 87, in _write\n",
            "    self.do_write(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 129, in do_write\n",
            "    write(_assemble_writes(writes))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 184, in _assemble_writes\n",
            "    if ww := w.mapper(w.value):\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\", line 983, in _get_updates\n",
            "    raise InvalidUpdateError(msg)\n",
            "langgraph.errors.InvalidUpdateError: Expected dict, got content_strategist\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "During task with name 'router' and id '823914d0-3424-9bf0-b1f9-162d868e2acd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d5c34d3"
      },
      "source": [
        "**Reasoning**:\n",
        "We are still encountering the `InvalidUpdateError: Expected dict, got content_strategist` originating from the `router` node, even though the `router_node` is correctly returning a string for conditional routing and accessing the state via attribute access. The traceback continues to point to the state update mechanism trying to interpret the string return as a dictionary update.\n",
        "\n",
        "The output from `initial_processing_node` shows `Initial state received by initial_processing_node: GraphState(query='None', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')`. This is the root cause of the `router_node` not having the query: the initial dictionary input `{\"query\": sample_query}` is not correctly populating the `GraphState` instance when the graph is invoked. The `GraphState` is initialized with its default `None` values.\n",
        "\n",
        "The `initial_processing_node` was intended to help with this, but its return value might not be correctly updating the state for the *next* node in the way we expect when using a custom `StateGraph`.\n",
        "\n",
        "A more direct way to ensure the initial state is correctly set when using a `StateGraph` with a custom class is to initialize the `GraphState` object *before* invoking the graph and pass that object to `app.invoke`. We tried this initially, which led to the first `InvalidUpdateError`. However, the structure of the graph was different then (the `query_handler` was the entry point, and it was trying to route directly).\n",
        "\n",
        "With the current structure (`initial_processing` -> `router` -> ...), the `initial_processing_node` receives the initial state passed to `invoke`. If we pass a `GraphState` object to `invoke`, the `initial_processing_node` will receive this already-initialized `GraphState` object. We can then ensure the `initial_processing_node` correctly populates the `GraphState`'s `query` attribute from the initial input dictionary *if* the state was not already initialized with a query.\n",
        "\n",
        "Let's try initializing the graph with a `GraphState` object created from the initial dictionary input and pass that object to `app.invoke`. We will also modify the `initial_processing_node` to explicitly set the `query` attribute of the `GraphState` if it's `None`, using the query from the initial input dictionary. This ensures the `query` is available in the `GraphState` instance when it reaches the `router_node`.\n",
        "\n",
        "We will revert the invocation back to using a `GraphState` object and modify `initial_processing_node` to handle setting the query attribute. The rest of the graph structure and node logic (router returning string, conditional edge on router) should remain the same as they are the correct pattern.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-em4JP2mdcl",
        "outputId": "33a3173c-9c04-47b8-c298-e35bc666b06d"
      },
      "source": [
        "# Define a simple initial processing node that ensures the query is set in the state\n",
        "def initial_processing_node(state: GraphState):\n",
        "    \"\"\"\n",
        "    Ensures the query is set in the GraphState object, potentially from initial invocation input.\n",
        "    \"\"\"\n",
        "    print(\"---INITIAL PROCESSING NODE---\")\n",
        "    print(f\"Initial state received by initial_processing_node: {state}\")\n",
        "\n",
        "    # Check if the query is already set in the GraphState\n",
        "    if state.query is None:\n",
        "        print(\"Query is None in GraphState. Attempting to set from initial input.\")\n",
        "        # When invoked with a dictionary, LangGraph might pass a GraphState with default values first.\n",
        "        # We need to access the initial input that was used to create this state.\n",
        "        # LangGraph often makes the initial input available, but the mechanism can vary.\n",
        "        # A common pattern is to pass the initial input directly to the entry node.\n",
        "        # Let's assume the initial `state` object passed to this node *contains* the initial input data,\n",
        "        # which is the case when invoke is called with a GraphState object initialized from a dictionary.\n",
        "        # However, if invoke is called with a dictionary directly, the entry node receives that dictionary first.\n",
        "        # Let's simplify: if invoke is called with a GraphState, we expect the query to be there.\n",
        "        # If invoke is called with a dictionary, the initial_processing_node receives the dictionary.\n",
        "\n",
        "        # Let's redefine initial_processing_node to be robust to initial input type\n",
        "        # (Although LangGraph typically passes the *state* object to nodes after the first one)\n",
        "\n",
        "        # Given the error, let's assume `initial_processing_node` receives the `GraphState` object\n",
        "        # initialized by LangGraph from the initial input. We need to ensure the query is copied over.\n",
        "        # When `app.invoke(initial_input_data)` was called, LangGraph likely created a `GraphState`\n",
        "        # and then tried to map `initial_input_data` to it. The previous output showed it was not mapped.\n",
        "\n",
        "        # Let's go back to initializing with a GraphState object and ensure its query is set.\n",
        "        # The `initial_processing_node` doesn't need to do much if the GraphState is already correctly initialized.\n",
        "        # Its primary purpose in the previous attempt was to transition to the router.\n",
        "        pass # No action needed if GraphState is correctly initialized before invoke\n",
        "\n",
        "    # Return the state as a dictionary for consistency, although returning None/GraphState might also work\n",
        "    # Let's return None as this node's primary role is just to be an entry point before routing\n",
        "    # Returning None or the state object itself doesn't update the state, it just passes it along.\n",
        "    # The state update happens from the return value of nodes that return dictionaries.\n",
        "    # The router node will read the state and return a string for routing.\n",
        "\n",
        "    # Reverting initial_processing_node to just pass the state along, relying on pre-initialization\n",
        "    return state # Pass the GraphState object to the next node (router)\n",
        "\n",
        "\n",
        "# Define a dedicated Router Agent node - Uses attribute access, returns string\n",
        "def router_node(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns the next node name as a string.\n",
        "    Accesses state using attribute access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Access the query from the state using attribute access\n",
        "    query = state.query\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning next_node: {next_node}\")\n",
        "    # Return the string name of the next node for conditional routing\n",
        "    return next_node\n",
        "\n",
        "\n",
        "# Re-compile the graph with the current node definitions\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the nodes\n",
        "workflow.add_node(\"initial_processing\", initial_processing_node)\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the initial processing node\n",
        "workflow.set_entry_point(\"initial_processing\")\n",
        "\n",
        "# Add a regular edge from initial_processing to the router\n",
        "workflow.add_edge(\"initial_processing\", \"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on its string return value\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda x: x, # The condition is the return value of the router_node (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the GraphState object directly with the query\n",
        "initial_state = GraphState(query=sample_query)\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initialized GraphState object\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # When using a custom StateGraph, invoke returns the final GraphState object\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback for more details\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: InvalidUpdateError\n",
            "Error Message: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-770350283.py\", line 137, in <cell line: 0>\n",
            "    final_state = app.invoke(initial_state)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
            "    for chunk in self.stream(\n",
            "                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
            "    for _ in runner.tick(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
            "    run_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
            "    return task.proc.invoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
            "    ret = self.func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 87, in _write\n",
            "    self.do_write(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 129, in do_write\n",
            "    write(_assemble_writes(writes))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\", line 184, in _assemble_writes\n",
            "    if ww := w.mapper(w.value):\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\", line 983, in _get_updates\n",
            "    raise InvalidUpdateError(msg)\n",
            "langgraph.errors.InvalidUpdateError: Expected dict, got GraphState(query='research about the benefits of content marketing', research_data='None', blog_content='None', linkedin_content='None', image_prompt='None', generated_image='None')\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
            "During task with name '__start__' and id '871b7373-baf2-a665-c582-966704415f68'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d198850"
      },
      "source": [
        "**Reasoning**:\n",
        "The `InvalidUpdateError: Expected dict, got GraphState(...)` is still occurring at the `__start__` task, even when invoking the graph with a pre-initialized `GraphState` object. The error message and traceback are identical to the first time we encountered this error. This strongly suggests that the issue is with how LangGraph's `StateGraph` specifically handles the *very first* step of processing the initial state when a custom state class is used. It seems to be trying to perform a state update using the initial `GraphState` object itself, which it expects to be a dictionary of updates.\n",
        "\n",
        "The `initial_processing_node` was intended to be the entry point and handle the initial state, but the error is happening *before* the code inside `initial_processing_node` even executes. This is evident from the traceback pointing to the `__start__` task and the state update logic.\n",
        "\n",
        "Given this persistent error at the graph's entry point with a custom `GraphState`, the most likely solution is to redefine the state of the `StateGraph` to be a simple dictionary (`dict`) instead of our custom `GraphState` class. LangGraph's dictionary-based state management is more fundamental and less likely to have issues with the initial state processing.\n",
        "\n",
        "We can still conceptually use the attributes defined in `GraphState` (query, research_data, etc.) as keys in the dictionary state. The agent functions will need to be slightly modified to access state using dictionary key access (`state.get(\"query\")` or `state[\"query\"]`) instead of attribute access (`state.query`). Returning dictionaries from agents to update the state is already the correct pattern for dictionary-based state.\n",
        "\n",
        "Let's redefine the `StateGraph` to use `dict` for its state. We will also update all agent functions to use dictionary access for state variables. The graph structure with `initial_processing` -> `router` and the conditional edge on `router` should still work, but we might be able to simplify and remove the `initial_processing_node` if the router can now be the entry point and correctly handle the initial dictionary input.\n",
        "\n",
        "Let's try making the `router_node` the entry point again and define the `StateGraph` with `dict` as the state type. We will update the `router_node` and all other agent nodes to use dictionary access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agsp5-xVmoIg",
        "outputId": "7a8089b6-24bc-46ca-8194-6e59db7b36c4"
      },
      "source": [
        "# Define the state for the graph as a dictionary\n",
        "# No custom class needed for the StateGraph definition\n",
        "# class GraphState: ... (will not be used for StateGraph state type)\n",
        "\n",
        "# Define the Router Agent node - Now using dictionary access, can be the entry point\n",
        "def router_node(state: dict): # Type hint is now dict\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns the next node name as a string.\n",
        "    Accesses state using dictionary access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Access the query from the state using dictionary access\n",
        "    query = state.get(\"query\") # Use .get() for safety\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning next_node: {next_node}\")\n",
        "    # Return the string name of the next node for conditional routing\n",
        "    return next_node\n",
        "\n",
        "# Deep Research Agent: Conducts comprehensive web research and analysis - Update to dict access\n",
        "def deep_research_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query (dict state).\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.get(\"query\") # Use .get() for safety\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        return {\"research_data\": \"Error: No query provided for research.\"}\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key:\n",
        "        print(\"Error: SERPAPI_API_KEY not found in environment variables.\")\n",
        "        return {\"research_data\": \"Error: SERPAPI_API_KEY not configured.\"}\n",
        "\n",
        "    if not SerpAPI_Client_Available:\n",
        "         print(\"Error: SerpApiClient not imported successfully.\")\n",
        "         return {\"research_data\": \"Error: SerpAPI library not correctly configured/installed.\"}\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        client = SerpApiClient(params)\n",
        "        results = client.get_dict()\n",
        "        research_data = results\n",
        "        print(\"Research complete. Returning update.\")\n",
        "        return {\"research_data\": research_data} # Return a dictionary with updates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        return {\"research_data\": f\"Error during SERP API search: {e}\"}\n",
        "\n",
        "\n",
        "# SEO Blog Writer Agent: Creates search-optimized long-form content - Update to dict access\n",
        "def seo_blog_writer_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Creates search-optimized long-form content based on research data (dict state).\n",
        "    \"\"\"\n",
        "    print(\"---SEO BLOG WRITER AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "\n",
        "    if not research_data:\n",
        "        print(\"No research data available. Cannot generate blog content.\")\n",
        "        return {\"blog_content\": \"Error: No research data available.\"}\n",
        "\n",
        "    print(f\"Generating blog content for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        return {\"blog_content\": \"Error: OPENAI_API_KEY not configured.\"}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert SEO blog writer... (prompt remains the same)\n",
        "    User Query: {query}\n",
        "    Research Data: {research_data}\n",
        "    Instructions: ...\n",
        "    \"\"\" # Truncated for brevity\n",
        "\n",
        "    generated_blog_content = \"Error: Blog content generation failed.\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert SEO blog writer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_blog_content = response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating blog content with OpenAI API: {e}\")\n",
        "        generated_blog_content = f\"Error generating blog content: {e}\"\n",
        "\n",
        "    print(\"Blog content generation complete. Returning update.\")\n",
        "    return {\"blog_content\": generated_blog_content}\n",
        "\n",
        "\n",
        "# LinkedIn Post Writer Agent: Generates engaging professional social content - Update to dict access\n",
        "def linkedin_post_writer_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Generates engaging professional social content based on available content (dict state).\n",
        "    \"\"\"\n",
        "    print(\"---LINKEDIN POST WRITER AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "\n",
        "    content_to_summarize = blog_content if blog_content else research_data\n",
        "\n",
        "    if not content_to_summarize:\n",
        "        print(\"No content available to generate LinkedIn post.\")\n",
        "        return {\"linkedin_content\": \"Error: No content available to generate LinkedIn post.\"}\n",
        "\n",
        "    print(f\"Generating LinkedIn post for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        return {\"linkedin_content\": \"Error: OPENAI_API_KEY not configured.\"}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert LinkedIn post writer... (prompt remains the same)\n",
        "    User Query: {query}\n",
        "    Available Content: {content_to_summarize}\n",
        "    Instructions: ...\n",
        "    \"\"\" # Truncated for brevity\n",
        "\n",
        "    generated_linkedin_content = \"Error: LinkedIn post generation failed.\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert LinkedIn post writer. Create engaging and concise professional posts.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_linkedin_content = response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating LinkedIn post with OpenAI API: {e}\")\n",
        "        generated_linkedin_content = f\"Error generating LinkedIn post: {e}\"\n",
        "\n",
        "    print(\"LinkedIn post generation complete. Returning update.\")\n",
        "    return {\"linkedin_content\": generated_linkedin_content}\n",
        "\n",
        "\n",
        "# Image Generation Agent: Produces custom visuals with prompt optimization - Update to dict access\n",
        "def image_generation_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Produces custom visuals with prompt optimization based on the query or content (dict state).\n",
        "    \"\"\"\n",
        "    print(\"---IMAGE GENERATION AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "    linkedin_content = state.get(\"linkedin_content\")\n",
        "\n",
        "    content_for_prompt = blog_content if blog_content else linkedin_content if linkedin_content else query\n",
        "\n",
        "    if not content_for_prompt:\n",
        "        print(\"No content available to generate an image.\")\n",
        "        return {\"image_prompt\": \"Error: No content available for image generation.\", \"generated_image\": None}\n",
        "\n",
        "    print(f\"Generating image prompt for query/content: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        return {\"image_prompt\": \"Error: OPENAI_API_KEY not configured.\", \"generated_image\": None}\n",
        "\n",
        "    optimized_prompt = \"Error: Image prompt optimization failed.\"\n",
        "    generated_image_data = \"Error: Image generation failed.\"\n",
        "\n",
        "    try:\n",
        "        print(\"Optimizing image prompt using LLM...\")\n",
        "        prompt_optimization_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at creating concise and effective prompts for image generation models like DALL-E 3. Your goal is to take provided content and turn it into a visually compelling image description.\"},\n",
        "                {\"role_content\": f\"Create a detailed and creative prompt for an image generation model based on the following content:\\n\\n{content_for_prompt}\\n\\nEnsure the prompt is visually descriptive and suitable for generating a professional and engaging image.\"}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        optimized_prompt = prompt_optimization_response.choices[0].message['content']\n",
        "        print(f\"Optimized Prompt: {optimized_prompt}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error optimizing image prompt with OpenAI API: {e}\")\n",
        "        optimized_prompt = f\"Error optimizing prompt for image: {e}\"\n",
        "        if optimized_prompt.startswith(\"Error\"):\n",
        "             optimized_prompt = f\"Image related to: {content_for_prompt[:200]}...\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(\"Generating image using DALL-E 3...\")\n",
        "        image_response = openai.Image.create(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=optimized_prompt,\n",
        "            size=\"1024x1024\",\n",
        "            quality=\"standard\",\n",
        "            n=1\n",
        "        )\n",
        "        generated_image_data = image_response.data[0].url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image with DALL-E 3 API: {e}\")\n",
        "        generated_image_data = f\"Error generating image: {e}\"\n",
        "\n",
        "    print(\"Image generation complete. Returning update.\")\n",
        "    return {\"image_prompt\": optimized_prompt, \"generated_image\": generated_image_data}\n",
        "\n",
        "\n",
        "# Content Strategist Agent: Formats and organizes content - Update to dict access\n",
        "def content_strategist_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Formats and organizes research and generated content into a final output using an LLM (dict state).\n",
        "    \"\"\"\n",
        "    print(\"---CONTENT STRATEGIST AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "    linkedin_content = state.get(\"linkedin_content\")\n",
        "    image_prompt = state.get(\"image_prompt\")\n",
        "    generated_image = state.get(\"generated_image\")\n",
        "\n",
        "    print(\"Organizing and formatting final content using LLM.\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key:\n",
        "        print(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
        "        print(\"\\n---FINAL CONTENT OUTPUT (Raw State Data)---\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Research Data: {research_data}\")\n",
        "        print(f\"Blog Content: {blog_content}\")\n",
        "        print(f\"LinkedIn Content: {linkedin_content}\")\n",
        "        print(f\"Image Prompt: {image_prompt}\")\n",
        "        print(f\"Generated Image Data: {generated_image}\")\n",
        "        return {}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert content strategist and formatter... (prompt remains the same)\n",
        "    Original User Query: {query}\n",
        "    Research Data: {research_data if research_data else \"No research data available.\"}\n",
        "    Blog Content: {blog_content if blog_content else \"No blog content generated.\"}\n",
        "    LinkedIn Post: {linkedin_content if linkedin_content else \"No LinkedIn post generated.\"}\n",
        "    Image Information: Image Prompt used: {image_prompt if image_prompt else \"No image prompt generated.\"}\n",
        "    Generated Image Data (URL or info): {generated_image if generated_image else \"No image generated.\"}\n",
        "    Instructions: ...\n",
        "    \"\"\" # Truncated for brevity\n",
        "\n",
        "    final_content_output = \"Error: Final content formatting failed.\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert content strategist and formatter.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        final_content_output = response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting content with OpenAI API: {e}\")\n",
        "        final_content_output = f\"Error formatting content: {e}\\n\\nRaw State Data:\\nQuery: {query}\\nResearch Data: {research_data}\\nBlog Content: {blog_content}\\nLinkedIn Content: {linkedin_content}\\nImage Prompt: {image_prompt}\\nGenerated Image Data: {generated_image}\"\n",
        "\n",
        "    print(\"\\n---FINAL CONTENT OUTPUT---\")\n",
        "    print(final_content_output)\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "# Build the LangGraph with dictionary state\n",
        "# Initialize the StateGraph with dict as the state type\n",
        "workflow = StateGraph(dict) # Use dict for state\n",
        "\n",
        "# Add the nodes (agents) - router is now the entry point\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the router node\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on its string return value\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda x: x, # The condition is the return value of the router_node (the string node name)\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with dictionary state and router as entry point!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "# This dictionary will be passed directly to the entry point node (router_node)\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    # When using a dict StateGraph, invoke returns the final state as a dictionary\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback for more details\n",
        "    print(\"\\n--- Final State (if available) ---\")\n",
        "    # In case of an error, the final state might not be fully populated\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with dictionary state and router as entry point!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---ROUTER NODE---\n",
            "Routing to Deep Research Agent\n",
            "Router node returning next_node: deep_research\n",
            "---DEEP RESEARCH AGENT---\n",
            "\n",
            "--- Error during workflow execution ---\n",
            "Error Type: AttributeError\n",
            "Error Message: 'str' object has no attribute 'get'\n",
            "\n",
            "--- Final State (if available) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1461493210.py\", line 362, in <cell line: 0>\n",
            "    final_state = app.invoke(initial_input_data)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
            "    for chunk in self.stream(\n",
            "                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
            "    for _ in runner.tick(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
            "    run_with_retry(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
            "    return task.proc.invoke(task.input, config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
            "    input = context.run(step.invoke, input, config, **kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
            "    ret = self.func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1461493210.py\", line 44, in deep_research_agent\n",
            "    query = state.get(\"query\") # Use .get() for safety\n",
            "            ^^^^^^^^^\n",
            "AttributeError: 'str' object has no attribute 'get'\n",
            "During task with name 'deep_research' and id 'acb25d93-a654-f763-c267-da7d0bba0c70'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9890078e"
      },
      "source": [
        "**Reasoning**:\n",
        "We've successfully resolved the initial `InvalidUpdateError` by switching to a dictionary-based state. The graph now starts, and the `router_node` executes correctly, returning the string \"deep_research\". The conditional edge then correctly routes to the `deep_research_agent`. However, a new error occurs within the `deep_research_agent`: `AttributeError: 'str' object has no attribute 'get'`. The traceback shows this happens on the line `query = state.get(\"query\")`. This means that the `state` object being passed to the `deep_research_agent` is not the full dictionary state as expected, but rather the *string output* from the previous node (`router_node`), which was \"deep_research\". This reveals another crucial aspect of LangGraph's `StateGraph` with dictionary state: the return value of a node is used to update the state *for the next node*, but the next node receives the *entire, updated state*, not just the output of the previous node. In our case, the `router_node` returns a string (\"deep_research\"). This string is used by the conditional edge for routing, but it is *also* being used to update the state before passing it to `deep_research_agent`. Since the return value is not a dictionary, LangGraph seems to be replacing the entire state with this string, leading to the `deep_research_agent` receiving a string instead of the expected state dictionary. To fix this, the `router_node` should return a dictionary that updates the state *without* overwriting the entire state with the routing string. The routing decision should still be based on the router's output, but the output itself needs to be a dictionary. We can modify the `router_node` to return a dictionary like `{\"route\": next_node}`. Then, the conditional edge needs to be updated to read this `'route'` key from the state *after* the `router_node` has updated it. Let's redefine the `router_node` to return a dictionary containing the next node name under a key, and update the conditional edge to use this key for routing. The state is now a dictionary, so `state.get(\"route\")` in the conditional edge lambda is appropriate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pnw_ZvfmxfG",
        "outputId": "06f3293e-aacb-4940-b97e-dd4f65b1de72"
      },
      "source": [
        "# Define the Router Agent node - Modified to return a dictionary with the route key\n",
        "def router_node(state: dict): # Type hint is dict\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns a dictionary\n",
        "    containing the next node name under the 'route' key.\n",
        "    Accesses state using dictionary access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Access the query from the state using dictionary access\n",
        "    query = state.get(\"query\") # Use .get() for safety\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning route: {next_node}\")\n",
        "    # Return a dictionary containing the next node name\n",
        "    return {\"route\": next_node} # Return a dictionary\n",
        "\n",
        "\n",
        "# Re-compile the graph with the modified router_node and conditional edge\n",
        "workflow = StateGraph(dict) # State is a dictionary\n",
        "\n",
        "# Add the nodes (agents) - router is the entry point\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent) # Uses dict access\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent) # Uses dict access\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent) # Uses dict access\n",
        "workflow.add_node(\"image_generation\", image_generation_agent) # Uses dict access\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent) # Uses dict access\n",
        "\n",
        "# Set the entry point to the router node\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on the 'route' key in the state\n",
        "# after the router node's return value has updated the state.\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda state: state.get(\"route\"), # The condition is the value of the 'route' key in the state\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with router node returning dict for route!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n--- Final State (if available) ---\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with router node returning dict for route!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---ROUTER NODE---\n",
            "Routing to Deep Research Agent\n",
            "Router node returning route: deep_research\n",
            "---DEEP RESEARCH AGENT---\n",
            "No query provided for research.\n",
            "---CONTENT STRATEGIST AGENT---\n",
            "Organizing and formatting final content using LLM.\n",
            "Error: OPENAI_API_KEY not found in environment variables.\n",
            "\n",
            "---FINAL CONTENT OUTPUT (Raw State Data)---\n",
            "Query: None\n",
            "Research Data: Error: No query provided for research.\n",
            "Blog Content: None\n",
            "LinkedIn Content: None\n",
            "Image Prompt: None\n",
            "Generated Image Data: None\n",
            "\n",
            "--- Final State ---\n",
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e4f21bb"
      },
      "source": [
        "## Test the workflow retry 2\n",
        "\n",
        "### Subtask:\n",
        "Retry running the compiled graph with a sample query to test the implemented logic and ensure the agents are working correctly, this time ensuring the necessary API keys are available to the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6bb2d7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to run the graph failed because the necessary API keys were not set in the environment. The subtask requires ensuring these keys are available and retrying the graph execution. While the notebook history shows attempts to load from `.env`, the execution logs indicate the keys were not found. Directly setting `os.environ` with dummy keys (as actual keys cannot be provided in this environment) within the code block will allow the agents to proceed past the API key check, even if the external API calls will still fail with authentication errors. This will at least allow us to test the workflow's structure and agent interaction flow without the immediate `KeyError` or `None` value issues within the agents. We will then re-compile and run the graph using the dictionary state and the router node, as this configuration successfully bypassed the `InvalidUpdateError` in the previous step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb694642",
        "outputId": "a24032a2-0277-49ba-e9c4-613445cbec6c"
      },
      "source": [
        "import os\n",
        "# Ensure API keys are set for testing purposes.\n",
        "# In a real scenario, these would be loaded securely from environment variables or a secret management system.\n",
        "# Using dummy values here to bypass the agent's API key check.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"dummy_openai_key_for_testing\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"dummy_serpapi_key_for_testing\"\n",
        "\n",
        "print(\"Dummy API keys set in environment variables for testing.\")\n",
        "\n",
        "# Re-define the router node - Modified to return a dictionary with the route key\n",
        "# This definition is copied from the previous successful attempt to avoid errors\n",
        "def router_node(state: dict): # Type hint is dict\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns a dictionary\n",
        "    containing the next node name under the 'route' key.\n",
        "    Accesses state using dictionary access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Access the query from the state using dictionary access\n",
        "    query = state.get(\"query\") # Use .get() for safety\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning route: {next_node}\")\n",
        "    # Return a dictionary containing the next node name\n",
        "    return {\"route\": next_node} # Return a dictionary\n",
        "\n",
        "\n",
        "# Re-compile the graph with the modified router_node and conditional edge\n",
        "workflow = StateGraph(dict) # State is a dictionary\n",
        "\n",
        "# Add the nodes (agents) - router is the entry point\n",
        "workflow.add_node(\"router\", router_node)\n",
        "# Assuming deep_research_agent, seo_blog_writer_agent, linkedin_post_writer_agent,\n",
        "# image_generation_agent, and content_strategist_agent are defined in previous cells\n",
        "# and are compatible with dictionary state access (using state.get())\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the router node\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on the 'route' key in the state\n",
        "# after the router node's return value has updated the state.\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda state: state.get(\"route\"), # The condition is the value of the 'route' key in the state\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with router node returning dict for route!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    # This should now proceed past the initial state handling and router node\n",
        "    # but will likely encounter API errors in the subsequent agents\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n--- Final State (if available) ---\")\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy API keys set in environment variables for testing.\n",
            "LangGraph workflow re-compiled successfully with router node returning dict for route!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---ROUTER NODE---\n",
            "Routing to Deep Research Agent\n",
            "Router node returning route: deep_research\n",
            "---DEEP RESEARCH AGENT---\n",
            "No query provided for research.\n",
            "---CONTENT STRATEGIST AGENT---\n",
            "Organizing and formatting final content using LLM.\n",
            "Error formatting content with OpenAI API: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "\n",
            "---FINAL CONTENT OUTPUT---\n",
            "Error formatting content: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "\n",
            "Raw State Data:\n",
            "Query: None\n",
            "Research Data: Error: No query provided for research.\n",
            "Blog Content: None\n",
            "LinkedIn Content: None\n",
            "Image Prompt: None\n",
            "Generated Image Data: None\n",
            "\n",
            "--- Final State ---\n",
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48c46873"
      },
      "source": [
        "**Reasoning**:\n",
        "The workflow execution proceeded past the router node and into the agents, confirming the state management and routing structure are working. However, the `deep_research_agent` reported \"No query provided for research\", indicating the state wasn't correctly passed or accessed. Additionally, the `content_strategist_agent` failed with an `AttributeError` related to accessing `openai.ChatCompletion`, which indicates an incompatibility with the installed OpenAI library version (`openai>=1.0.0`). The next step is to address the state access issue in the agents and update the OpenAI API calls in all relevant agents (`seo_blog_writer_agent`, `linkedin_post_writer_agent`, `image_generation_agent`, `content_strategist_agent`) to use the new API syntax (`openai.chat.completions.create`, `openai.images.generate`). We will then re-compile the graph and retry the invocation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46e8a652",
        "outputId": "2766eed1-236a-478b-8fe6-806d12b467b3"
      },
      "source": [
        "import openai\n",
        "\n",
        "# Update Deep Research Agent to correctly access query from dictionary state\n",
        "def deep_research_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.get(\"query\") # Access query using .get() from dictionary state\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        # Update state with error message\n",
        "        state[\"research_data\"] = \"Error: No query provided for research.\"\n",
        "        return state # Return the updated state dictionary\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key or serpapi_key == \"dummy_serpapi_key_for_testing\":\n",
        "        print(\"Error: SERPAPI_API_KEY not found or is dummy key.\")\n",
        "        # Update state with error message\n",
        "        state[\"research_data\"] = \"Error: SERPAPI_API_KEY not configured or is dummy.\"\n",
        "        return state # Return the updated state dictionary\n",
        "\n",
        "\n",
        "    if 'SerpAPI_Client_Available' not in globals() or not SerpAPI_Client_Available:\n",
        "         print(\"Error: SerpAPI client not imported successfully.\")\n",
        "         # Update state with error message\n",
        "         state[\"research_data\"] = \"Error: SerpAPI library not correctly configured/installed.\"\n",
        "         return state # Return the updated state dictionary\n",
        "\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        # Use SerpApiClient to perform the search (assuming it was defined and imported previously)\n",
        "        client = SerpApiClient(params) # SerpApiClient should be available from previous cell\n",
        "        results = client.get_dict()\n",
        "\n",
        "\n",
        "        # Extract relevant data - this might need adjustment based on SERPAPI results structure\n",
        "        # For example, you might want to extract 'organic_results', 'knowledge_graph', etc.\n",
        "        # Let's store the entire results dictionary for now, or a summary\n",
        "        research_data = results # Or process results to get specific data\n",
        "\n",
        "        print(\"Research complete. Updating state.\")\n",
        "        # Update state with research data\n",
        "        state[\"research_data\"] = research_data\n",
        "        return state # Return the updated state dictionary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        # Update state with error message\n",
        "        state[\"research_data\"] = f\"Error during SERP API search: {e}\"\n",
        "        return state # Return the updated state dictionary\n",
        "\n",
        "\n",
        "# Update SEO Blog Writer Agent to use new OpenAI API syntax\n",
        "def seo_blog_writer_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Creates search-optimized long-form content based on research data.\n",
        "    \"\"\"\n",
        "    print(\"---SEO BLOG WRITER AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "\n",
        "    if not research_data:\n",
        "        print(\"No research data available. Cannot generate blog content.\")\n",
        "        state[\"blog_content\"] = \"Error: No research data available.\"\n",
        "        return state\n",
        "\n",
        "    print(f\"Generating blog content for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        state[\"blog_content\"] = \"Error: OPENAI_API_KEY not configured or is dummy.\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    # Construct a detailed prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert SEO blog writer. Your task is to write a comprehensive and SEO-optimized blog post based on the following research data and the user's query.\n",
        "\n",
        "    User Query: {query}\n",
        "\n",
        "    Research Data:\n",
        "    {research_data}\n",
        "\n",
        "    Instructions:\n",
        "    1. Write a compelling title for the blog post.\n",
        "    2. Write an engaging introduction that hooks the reader and summarizes the main points.\n",
        "    3. Structure the blog post with clear headings and subheadings.\n",
        "    4. Incorporate relevant keywords naturally throughout the content (use keywords that appear in the research data if possible).\n",
        "    5. Provide detailed explanations and insights based on the research data.\n",
        "    6. Write a concluding paragraph that summarizes the key takeaways and includes a call to action (e.g., share, comment).\n",
        "    7. Ensure the content is well-written, grammatically correct, and easy to read.\n",
        "    8. The blog post should be at least 500 words long.\n",
        "\n",
        "    Generate the full blog post content now.\n",
        "    \"\"\"\n",
        "\n",
        "    generated_blog_content = \"Error: Blog content generation failed.\"\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Chat Completions API using the new syntax\n",
        "        client = openai.OpenAI(api_key=openai_api_key) # Initialize client\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # Using a newer model if available, or \"gpt-4\" or \"gpt-3.5-turbo\"\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert SEO blog writer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_blog_content = response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating blog content with OpenAI API: {e}\")\n",
        "        generated_blog_content = f\"Error generating blog content: {e}\"\n",
        "\n",
        "    print(\"Blog content generation complete. Updating state.\")\n",
        "    state[\"blog_content\"] = generated_blog_content\n",
        "    return state # Return the updated state dictionary\n",
        "\n",
        "\n",
        "# Update LinkedIn Post Writer Agent to use new OpenAI API syntax\n",
        "def linkedin_post_writer_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Generates engaging professional social content based on available content.\n",
        "    \"\"\"\n",
        "    print(\"---LINKEDIN POST WRITER AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "\n",
        "    content_to_summarize = blog_content if blog_content else research_data\n",
        "\n",
        "    if not content_to_summarize:\n",
        "        print(\"No content available to generate LinkedIn post.\")\n",
        "        state[\"linkedin_content\"] = \"Error: No content available to generate LinkedIn post.\"\n",
        "        return state\n",
        "\n",
        "    print(f\"Generating LinkedIn post for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        state[\"linkedin_content\"] = \"Error: OPENAI_API_KEY not configured or is dummy.\"\n",
        "        return state\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert LinkedIn post writer. Your task is to create an engaging and concise professional LinkedIn post based on the following information.\n",
        "\n",
        "    User Query: {query}\n",
        "\n",
        "    Available Content (either blog post or research data):\n",
        "    {content_to_summarize}\n",
        "\n",
        "    Instructions:\n",
        "    1. Write a compelling and attention-grabbing opening line.\n",
        "    2. Summarize the key insights or takeaways from the provided content.\n",
        "    3. Use professional yet engaging language.\n",
        "    4. Include relevant hashtags (e.g., #marketing #contentmarketing #AI).\n",
        "    5. Keep the post concise and easy to read on a mobile feed.\n",
        "    6. Encourage engagement (e.g., ask a question, invite comments).\n",
        "\n",
        "    Generate the LinkedIn post content now.\n",
        "    \"\"\"\n",
        "\n",
        "    generated_linkedin_content = \"Error: LinkedIn post generation failed.\"\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Chat Completions API using the new syntax\n",
        "        client = openai.OpenAI(api_key=openai_api_key) # Initialize client\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # Or another suitable model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert LinkedIn post writer. Create engaging and concise professional posts.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_linkedin_content = response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating LinkedIn post with OpenAI API: {e}\")\n",
        "        generated_linkedin_content = f\"Error generating LinkedIn post: {e}\"\n",
        "\n",
        "    print(\"LinkedIn post generation complete. Updating state.\")\n",
        "    state[\"linkedin_content\"] = generated_linkedin_content\n",
        "    return state # Return the updated state dictionary\n",
        "\n",
        "\n",
        "# Update Image Generation Agent to use new OpenAI API syntax\n",
        "def image_generation_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Produces custom visuals with prompt optimization based on the query or content.\n",
        "    \"\"\"\n",
        "    print(\"---IMAGE GENERATION AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "    linkedin_content = state.get(\"linkedin_content\")\n",
        "\n",
        "    content_for_prompt = blog_content if blog_content else linkedin_content if linkedin_content else query\n",
        "\n",
        "    if not content_for_prompt:\n",
        "        print(\"No content available to generate an image.\")\n",
        "        state[\"image_prompt\"] = \"Error: No content available for image generation.\"\n",
        "        state[\"generated_image\"] = None\n",
        "        return state\n",
        "\n",
        "    print(f\"Generating image prompt for query/content: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        state[\"image_prompt\"] = \"Error: OPENAI_API_KEY not configured or is dummy.\"\n",
        "        state[\"generated_image\"] = None\n",
        "        return state\n",
        "\n",
        "    optimized_prompt = \"Error: Image prompt optimization failed.\"\n",
        "    generated_image_data = \"Error: Image generation failed.\"\n",
        "\n",
        "    try:\n",
        "        # Use an LLM to optimize the prompt using new OpenAI API syntax\n",
        "        print(\"Optimizing image prompt using LLM...\")\n",
        "        client = openai.OpenAI(api_key=openai_api_key) # Initialize client\n",
        "        prompt_optimization_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # Or another suitable model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at creating concise and effective prompts for image generation models like DALL-E 3. Your goal is to take provided content and turn it into a visually compelling image description.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Create a detailed and creative prompt for an image generation model based on the following content:\\n\\n{content_for_prompt}\\n\\nEnsure the prompt is visually descriptive and suitable for generating a professional and engaging image.\"}\n",
        "            ],\n",
        "             temperature=0.7,\n",
        "             max_tokens=150\n",
        "        )\n",
        "        optimized_prompt = prompt_optimization_response.choices[0].message.content\n",
        "        print(f\"Optimized Prompt: {optimized_prompt}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error optimizing image prompt with OpenAI API: {e}\")\n",
        "        optimized_prompt = f\"Error optimizing prompt for image: {e}\"\n",
        "        # Continue to try image generation with a basic prompt if optimization fails\n",
        "        if optimized_prompt.startswith(\"Error\"):\n",
        "             optimized_prompt = f\"Image related to: {content_for_prompt[:200]}...\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Image Generation API (DALL-E 3) using the new syntax\n",
        "        print(\"Generating image using DALL-E 3...\")\n",
        "        client = openai.OpenAI(api_key=openai_api_key) # Initialize client\n",
        "        image_response = client.images.generate(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=optimized_prompt,\n",
        "            size=\"1024x1024\",\n",
        "            quality=\"standard\",\n",
        "            n=1\n",
        "        )\n",
        "        generated_image_data = image_response.data[0].url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image with DALL-E 3 API: {e}\")\n",
        "        generated_image_data = f\"Error generating image: {e}\"\n",
        "\n",
        "    print(\"Image generation complete. Updating state.\")\n",
        "    state[\"image_prompt\"] = optimized_prompt\n",
        "    state[\"generated_image\"] = generated_image_data\n",
        "    return state # Return the updated state dictionary\n",
        "\n",
        "\n",
        "# Update Content Strategist Agent to use new OpenAI API syntax\n",
        "def content_strategist_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Formats and organizes research and generated content into a final output using an LLM.\n",
        "    \"\"\"\n",
        "    print(\"---CONTENT STRATEGIST AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "    linkedin_content = state.get(\"linkedin_content\")\n",
        "    image_prompt = state.get(\"image_prompt\")\n",
        "    generated_image = state.get(\"generated_image\")\n",
        "\n",
        "    print(\"Organizing and formatting final content using LLM.\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        print(\"\\n---FINAL CONTENT OUTPUT (Raw State Data)---\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Research Data: {research_data}\")\n",
        "        print(f\"Blog Content: {blog_content}\")\n",
        "        print(f\"LinkedIn Content: {linkedin_content}\")\n",
        "        print(f\"Image Prompt: {image_prompt}\")\n",
        "        print(f\"Generated Image Data: {generated_image}\")\n",
        "        return state # Return state even on error\n",
        "\n",
        "\n",
        "    # Construct a detailed prompt for the LLM to format the output\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert content strategist and formatter. Your task is to take the following raw content components generated by different agents and organize and format them into a cohesive, professional, and easy-to-read final output.\n",
        "\n",
        "    Original User Query: {query}\n",
        "\n",
        "    Research Data:\n",
        "    {research_data if research_data else \"No research data available.\"}\n",
        "\n",
        "    Blog Content:\n",
        "    {blog_content if blog_content else \"No blog content generated.\"}\n",
        "\n",
        "    LinkedIn Post:\n",
        "    {linkedin_content if linkedin_content else \"No LinkedIn post generated.\"}\n",
        "\n",
        "    Image Information:\n",
        "    Image Prompt used: {image_prompt if image_prompt else \"No image prompt generated.\"}\n",
        "    Generated Image Data (URL or info): {generated_image if generated_image else \"No image generated.\"}\n",
        "\n",
        "    Instructions:\n",
        "    1. Start with a clear heading indicating the content is for the original query.\n",
        "    2. Present the Research Data in a structured way (e.g., bullet points, summary).\n",
        "    3. Present the Blog Content, ensuring it's clearly labeled.\n",
        "    4. Present the LinkedIn Post, clearly labeled.\n",
        "    5. Present the Image Information, including the prompt used and the generated image data (e.g., URL).\n",
        "    6. Ensure the overall output is well-formatted with clear sections and easy to read.\n",
        "    7. Add a concluding remark.\n",
        "\n",
        "    Format the final content output now.\n",
        "    \"\"\"\n",
        "\n",
        "    final_content_output = \"Error: Final content formatting failed.\"\n",
        "\n",
        "    try:\n",
        "        # Call the OpenAI Chat Completions API using the new syntax\n",
        "        client = openai.OpenAI(api_key=openai_api_key) # Initialize client\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # Or another suitable model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert content strategist and formatter.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        final_content_output = response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting content with OpenAI API: {e}\")\n",
        "        final_content_output = f\"Error formatting content: {e}\\n\\nRaw State Data:\\nQuery: {query}\\nResearch Data: {research_data}\\nBlog Content: {blog_content}\\nLinkedIn Content: {linkedin_content}\\nImage Prompt: {image_prompt}\\nGenerated Image Data: {generated_image}\"\n",
        "\n",
        "    print(\"\\n---FINAL CONTENT OUTPUT---\")\n",
        "    print(final_content_output)\n",
        "\n",
        "    # The content strategist is the last step in this example, so it doesn't return a next node.\n",
        "    # Return the state to ensure it's updated in the graph, even if no further routing occurs.\n",
        "    return state\n",
        "\n",
        "\n",
        "# Re-compile the graph with the updated agents (assuming router_node is defined above and correct)\n",
        "workflow = StateGraph(dict) # State is a dictionary\n",
        "\n",
        "# Add the nodes (agents) - router is the entry point\n",
        "workflow.add_node(\"router\", router_node)\n",
        "# Add updated agents\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the router node\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on the 'route' key in the state\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\", # From the router node\n",
        "    lambda state: state.get(\"route\"), # The condition is the value of the 'route' key in the state\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\", # Default routing\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with updated agents and dictionary state handling!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "# sample_query = \"write a blog post about effective SEO strategies\"\n",
        "# sample_query = \"create a linkedin post about the importance of AI in marketing\"\n",
        "# sample_query = \"generate an image for a blog post about cloud computing\"\n",
        "# sample_query = \"summarize recent AI news\" # Test default routing\n",
        "\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    # Invoke the compiled graph with the initial input data (dictionary)\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n--- Final State (if available) ---\")\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow re-compiled successfully with updated agents and dictionary state handling!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---ROUTER NODE---\n",
            "Routing to Deep Research Agent\n",
            "Router node returning route: deep_research\n",
            "---DEEP RESEARCH AGENT---\n",
            "No query provided for research.\n",
            "---CONTENT STRATEGIST AGENT---\n",
            "Organizing and formatting final content using LLM.\n",
            "Error: OPENAI_API_KEY not found or is dummy key.\n",
            "\n",
            "---FINAL CONTENT OUTPUT (Raw State Data)---\n",
            "Query: None\n",
            "Research Data: Error: No query provided for research.\n",
            "Blog Content: None\n",
            "LinkedIn Content: None\n",
            "Image Prompt: None\n",
            "Generated Image Data: None\n",
            "\n",
            "--- Final State ---\n",
            "{'route': 'deep_research', 'research_data': 'Error: No query provided for research.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae54a1d"
      },
      "source": [
        "**Reasoning**:\n",
        "The deep research agent is still reporting \"No query provided for research\", indicating an issue with how the state is being passed or updated between the router and the deep research agent. The router returns `{\"route\": \"deep_research\"}`, which updates the state. However, the `query` key from the initial input `{\"query\": sample_query}` is not being preserved or merged correctly in the state that the `deep_research_agent` receives. Additionally, the workflow seems to be jumping directly from `deep_research` to `content_strategist` without the state from `deep_research` being fully available, as seen in the final state. LangGraph's `StateGraph` with a dictionary state is designed to merge updates. The `router_node` returning `{\"route\": next_node}` correctly updates the state with the `route` key. When the edge from `router` leads to `deep_research`, the `deep_research_agent` should receive the full state dictionary, including the original `query`. The error \"No query provided\" suggests this isn't happening as expected, or there's an issue with how the state is returned from `deep_research_agent`. The `deep_research_agent` should return the *entire* updated state dictionary, not just the `research_data`. Let's ensure all agent functions explicitly return the `state` dictionary after making updates to guarantee state propagation. We will also explicitly add `return state` at the end of each agent function to make the state propagation clear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0334e09a",
        "outputId": "740b5d3e-a1cc-49fa-f183-037dc2ff19bc"
      },
      "source": [
        "import os\n",
        "import openai # Ensure openai is imported\n",
        "\n",
        "# Ensure API keys are set for testing purposes.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"dummy_openai_key_for_testing\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"dummy_serpapi_key_for_testing\"\n",
        "\n",
        "print(\"Dummy API keys set in environment variables for testing.\")\n",
        "\n",
        "# Re-define the router node (copied for completeness, no changes needed here for this issue)\n",
        "def router_node(state: dict):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns a dictionary\n",
        "    containing the next node name under the 'route' key.\n",
        "    Accesses state using dictionary access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    query = state.get(\"query\")\n",
        "    next_node = \"content_strategist\"\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning route: {next_node}\")\n",
        "    return {\"route\": next_node}\n",
        "\n",
        "\n",
        "# Update Deep Research Agent to ensure it returns the full state dictionary\n",
        "def deep_research_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Conducts comprehensive web research and analysis based on the query.\n",
        "    \"\"\"\n",
        "    print(\"---DEEP RESEARCH AGENT---\")\n",
        "    query = state.get(\"query\") # Access query using .get()\n",
        "\n",
        "    if not query:\n",
        "        print(\"No query provided for research.\")\n",
        "        state[\"research_data\"] = \"Error: No query provided for research.\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "    print(f\"Performing deep research for: {query}\")\n",
        "\n",
        "    serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "    if not serpapi_key or serpapi_key == \"dummy_serpapi_key_for_testing\":\n",
        "        print(\"Error: SERPAPI_API_KEY not found or is dummy key.\")\n",
        "        state[\"research_data\"] = \"Error: SERPAPI_API_KEY not configured or is dummy.\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "    # Assuming SerpApiClient is available globally from a previous cell\n",
        "    if 'SerpApiClient' not in globals():\n",
        "         print(\"Error: SerpApiClient class not found.\")\n",
        "         state[\"research_data\"] = \"Error: SerpAPI library not correctly configured/installed.\"\n",
        "         return state # Explicitly return the updated state\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "          \"engine\": \"google\",\n",
        "          \"q\": query,\n",
        "          \"api_key\": serpapi_key\n",
        "        }\n",
        "        client = SerpApiClient(params)\n",
        "        results = client.get_dict()\n",
        "\n",
        "        research_data = results # Store raw results or processed data\n",
        "\n",
        "        print(\"Research complete. Updating state.\")\n",
        "        state[\"research_data\"] = research_data\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SERP API search: {e}\")\n",
        "        state[\"research_data\"] = f\"Error during SERP API search: {e}\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "# Update SEO Blog Writer Agent to ensure it returns the full state dictionary\n",
        "def seo_blog_writer_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Creates search-optimized long-form content based on research data.\n",
        "    \"\"\"\n",
        "    print(\"---SEO BLOG WRITER AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "\n",
        "    if not research_data or str(research_data).startswith(\"Error\"): # Check for actual data or error string\n",
        "        print(\"No valid research data available. Cannot generate blog content.\")\n",
        "        state[\"blog_content\"] = \"Error: No valid research data available.\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "    print(f\"Generating blog content for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        state[\"blog_content\"] = \"Error: OPENAI_API_KEY not configured or is dummy.\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "    prompt = f\"\"\"...\"\"\" # Keep the existing prompt structure\n",
        "\n",
        "    generated_blog_content = \"Error: Blog content generation failed.\"\n",
        "\n",
        "    try:\n",
        "        client = openai.OpenAI(api_key=openai_api_key)\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert SEO blog writer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_blog_content = response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating blog content with OpenAI API: {e}\")\n",
        "        generated_blog_content = f\"Error generating blog content: {e}\"\n",
        "\n",
        "    print(\"Blog content generation complete. Updating state.\")\n",
        "    state[\"blog_content\"] = generated_blog_content\n",
        "    return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "# Update LinkedIn Post Writer Agent to ensure it returns the full state dictionary\n",
        "def linkedin_post_writer_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Generates engaging professional social content based on available content.\n",
        "    \"\"\"\n",
        "    print(\"---LINKEDIN POST WRITER AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "\n",
        "    content_to_summarize = blog_content if blog_content and not str(blog_content).startswith(\"Error\") else research_data\n",
        "    if str(content_to_summarize).startswith(\"Error\"):\n",
        "        content_to_summarize = research_data # Fallback even if research had errors, might still contain query\n",
        "\n",
        "    if not content_to_summarize or (isinstance(content_to_summarize, str) and content_to_summarize.startswith(\"Error:\") and \"No query\" in content_to_summarize):\n",
        "        print(\"No valid content available to generate LinkedIn post.\")\n",
        "        state[\"linkedin_content\"] = \"Error: No valid content available to generate LinkedIn post.\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "    print(f\"Generating LinkedIn post for query: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        state[\"linkedin_content\"] = \"Error: OPENAI_API_KEY not configured or is dummy.\"\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "    prompt = f\"\"\"...\"\"\" # Keep the existing prompt structure\n",
        "\n",
        "    generated_linkedin_content = \"Error: LinkedIn post generation failed.\"\n",
        "\n",
        "    try:\n",
        "        client = openai.OpenAI(api_key=openai_api_key)\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert LinkedIn post writer. Create engaging and concise professional posts.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        generated_linkedin_content = response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating LinkedIn post with OpenAI API: {e}\")\n",
        "        generated_linkedin_content = f\"Error generating LinkedIn post: {e}\"\n",
        "\n",
        "    print(\"LinkedIn post generation complete. Updating state.\")\n",
        "    state[\"linkedin_content\"] = generated_linkedin_content\n",
        "    return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "# Update Image Generation Agent to ensure it returns the full state dictionary\n",
        "def image_generation_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Produces custom visuals with prompt optimization based on the query or content.\n",
        "    \"\"\"\n",
        "    print(\"---IMAGE GENERATION AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "    linkedin_content = state.get(\"linkedin_content\")\n",
        "\n",
        "    content_for_prompt = blog_content if blog_content and not str(blog_content).startswith(\"Error\") else linkedin_content\n",
        "    if str(content_for_prompt).startswith(\"Error\"):\n",
        "         content_for_prompt = linkedin_content if linkedin_content and not str(linkedin_content).startswith(\"Error\") else query\n",
        "\n",
        "\n",
        "    if not content_for_prompt or (isinstance(content_for_prompt, str) and content_for_prompt.startswith(\"Error:\") and \"No content\" in content_for_prompt):\n",
        "        print(\"No valid content available to generate an image.\")\n",
        "        state[\"image_prompt\"] = \"Error: No valid content available for image generation.\"\n",
        "        state[\"generated_image\"] = None\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "    print(f\"Generating image prompt for query/content: {query}\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        state[\"image_prompt\"] = \"Error: OPENAI_API_KEY not configured or is dummy.\"\n",
        "        state[\"generated_image\"] = None\n",
        "        return state # Explicitly return the updated state\n",
        "\n",
        "    optimized_prompt = \"Error: Image prompt optimization failed.\"\n",
        "    generated_image_data = \"Error: Image generation failed.\"\n",
        "\n",
        "    try:\n",
        "        print(\"Optimizing image prompt using LLM...\")\n",
        "        client = openai.OpenAI(api_key=openai_api_key)\n",
        "        prompt_optimization_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at creating concise and effective prompts for image generation models like DALL-E 3. Your goal is to take provided content and turn it into a visually compelling image description.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Create a detailed and creative prompt for an image generation model based on the following content:\\n\\n{content_for_prompt}\\n\\nEnsure the prompt is visually descriptive and suitable for generating a professional and engaging image.\"}\n",
        "            ],\n",
        "             temperature=0.7,\n",
        "             max_tokens=150\n",
        "        )\n",
        "        optimized_prompt = prompt_optimization_response.choices[0].message.content\n",
        "        print(f\"Optimized Prompt: {optimized_prompt}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error optimizing image prompt with OpenAI API: {e}\")\n",
        "        optimized_prompt = f\"Error optimizing prompt for image: {e}\"\n",
        "        if optimized_prompt.startswith(\"Error\"):\n",
        "             optimized_prompt = f\"Image related to: {content_for_prompt[:200]}...\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(\"Generating image using DALL-E 3...\")\n",
        "        client = openai.OpenAI(api_key=openai_api_key)\n",
        "        image_response = client.images.generate(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=optimized_prompt,\n",
        "            size=\"1024x1024\",\n",
        "            quality=\"standard\",\n",
        "            n=1\n",
        "        )\n",
        "        generated_image_data = image_response.data[0].url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image with DALL-E 3 API: {e}\")\n",
        "        generated_image_data = f\"Error generating image: {e}\"\n",
        "\n",
        "    print(\"Image generation complete. Updating state.\")\n",
        "    state[\"image_prompt\"] = optimized_prompt\n",
        "    state[\"generated_image\"] = generated_image_data\n",
        "    return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "# Update Content Strategist Agent to ensure it returns the full state dictionary\n",
        "def content_strategist_agent(state: dict):\n",
        "    \"\"\"\n",
        "    Formats and organizes research and generated content into a final output using an LLM.\n",
        "    \"\"\"\n",
        "    print(\"---CONTENT STRATEGIST AGENT---\")\n",
        "    query = state.get(\"query\")\n",
        "    research_data = state.get(\"research_data\")\n",
        "    blog_content = state.get(\"blog_content\")\n",
        "    linkedin_content = state.get(\"linkedin_content\")\n",
        "    image_prompt = state.get(\"image_prompt\")\n",
        "    generated_image = state.get(\"generated_image\")\n",
        "\n",
        "    print(\"Organizing and formatting final content using LLM.\")\n",
        "\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not openai_api_key or openai_api_key == \"dummy_openai_key_for_testing\":\n",
        "        print(\"Error: OPENAI_API_KEY not found or is dummy key.\")\n",
        "        print(\"\\n---FINAL CONTENT OUTPUT (Raw State Data)---\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Research Data: {research_data}\")\n",
        "        print(f\"Blog Content: {blog_content}\")\n",
        "        print(f\"LinkedIn Content: {linkedin_content}\")\n",
        "        print(f\"Image Prompt: {image_prompt}\")\n",
        "        print(f\"Generated Image Data: {generated_image}\")\n",
        "        return state # Explicitly return state even on error\n",
        "\n",
        "\n",
        "    # Construct a detailed prompt for the LLM to format the output\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert content strategist and formatter. Your task is to take the following raw content components generated by different agents and organize and format them into a cohesive, professional, and easy-to-read final output.\n",
        "\n",
        "    Original User Query: {query}\n",
        "\n",
        "    Research Data:\n",
        "    {research_data if research_data else \"No research data available.\"}\n",
        "\n",
        "    Blog Content:\n",
        "    {blog_content if blog_content else \"No blog content generated.\"}\n",
        "\n",
        "    LinkedIn Post:\n",
        "    {linkedin_content if linkedin_content else \"No LinkedIn post generated.\"}\n",
        "\n",
        "    Image Information:\n",
        "    Image Prompt used: {image_prompt if image_prompt else \"No image prompt generated.\"}\n",
        "    Generated Image Data (URL or info): {generated_image if generated_image else \"No image generated.\"}\n",
        "\n",
        "    Instructions:\n",
        "    1. Start with a clear heading indicating the content is for the original query.\n",
        "    2. Present the Research Data in a structured way (e.g., bullet points, summary).\n",
        "    3. Present the Blog Content, ensuring it's clearly labeled.\n",
        "    4. Present the LinkedIn Post, clearly labeled.\n",
        "    5. Present the Image Information, including the prompt used and the generated image data (e.g., URL).\n",
        "    6. Ensure the overall output is well-formatted with clear sections and easy to read.\n",
        "    7. Add a concluding remark.\n",
        "\n",
        "    Format the final content output now.\n",
        "    \"\"\"\n",
        "\n",
        "    final_content_output = \"Error: Final content formatting failed.\"\n",
        "\n",
        "    try:\n",
        "        client = openai.OpenAI(api_key=openai_api_key)\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert content strategist and formatter.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        final_content_output = response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting content with OpenAI API: {e}\")\n",
        "        final_content_output = f\"Error formatting content: {e}\\n\\nRaw State Data:\\nQuery: {query}\\nResearch Data: {research_data}\\nBlog Content: {blog_content}\\nLinkedIn Content: {linkedin_content}\\nImage Prompt: {image_prompt}\\nGenerated Image Data: {generated_image}\"\n",
        "\n",
        "    print(\"\\n---FINAL CONTENT OUTPUT---\")\n",
        "    print(final_content_output)\n",
        "\n",
        "    return state # Explicitly return the updated state\n",
        "\n",
        "\n",
        "# Re-compile the graph with the updated agents\n",
        "# StateGraph and nodes are assumed to be defined from previous cells\n",
        "workflow = StateGraph(dict) # State is a dictionary\n",
        "\n",
        "# Add the nodes (agents) - router is the entry point\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the router node\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on the 'route' key in the state\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state.get(\"route\"),\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with updated agents ensuring state return!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n--- Final State (if available) ---\")\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy API keys set in environment variables for testing.\n",
            "LangGraph workflow re-compiled successfully with updated agents ensuring state return!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---ROUTER NODE---\n",
            "Routing to Deep Research Agent\n",
            "Router node returning route: deep_research\n",
            "---DEEP RESEARCH AGENT---\n",
            "No query provided for research.\n",
            "---CONTENT STRATEGIST AGENT---\n",
            "Organizing and formatting final content using LLM.\n",
            "Error: OPENAI_API_KEY not found or is dummy key.\n",
            "\n",
            "---FINAL CONTENT OUTPUT (Raw State Data)---\n",
            "Query: None\n",
            "Research Data: Error: No query provided for research.\n",
            "Blog Content: None\n",
            "LinkedIn Content: None\n",
            "Image Prompt: None\n",
            "Generated Image Data: None\n",
            "\n",
            "--- Final State ---\n",
            "{'route': 'deep_research', 'research_data': 'Error: No query provided for research.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db2770c5"
      },
      "source": [
        "**Reasoning**:\n",
        "The workflow is still failing because the `deep_research_agent` is not receiving the query, and the state is not being correctly propagated. The issue is likely that the initial state dictionary `{\"query\": sample_query}` is *not* being merged into the state before the router node's return value updates the state. The router node receives the initial dictionary, adds the 'route' key, and returns `{'route': 'deep_research'}`. LangGraph's StateGraph with a dictionary state should merge updates, but it seems the initial dictionary input isn't being fully integrated into the state before the *first* node's return. A potential fix is to explicitly initialize the state with the query *within* the router node before determining the route. This ensures the query is always present in the state that is updated and passed to subsequent nodes. We will modify the `router_node` to explicitly include the initial query in its return dictionary, ensuring the query is part of the state passed to the next node. We will then re-compile and re-run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2607f532",
        "outputId": "1fa52021-3b6b-4e1b-ceca-9c53ac475d5f"
      },
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Ensure API keys are set for testing purposes.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"dummy_openai_key_for_testing\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"dummy_serpapi_key_for_testing\"\n",
        "\n",
        "print(\"Dummy API keys set in environment variables for testing.\")\n",
        "\n",
        "# Modify the router node to explicitly include the query in its return dictionary\n",
        "def router_node(state: dict):\n",
        "    \"\"\"\n",
        "    Determines which agent to route the query to and returns a dictionary\n",
        "    containing the next node name under the 'route' key, *including the original query*.\n",
        "    Accesses state using dictionary access.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTER NODE---\")\n",
        "    # Get the query from the current state (which should be the initial input)\n",
        "    query = state.get(\"query\")\n",
        "\n",
        "    next_node = \"content_strategist\" # Default routing\n",
        "    if query:\n",
        "        if \"research\" in query.lower():\n",
        "            print(\"Routing to Deep Research Agent\")\n",
        "            next_node = \"deep_research\"\n",
        "        elif \"blog\" in query.lower():\n",
        "            print(\"Routing to SEO Blog Writer Agent\")\n",
        "            next_node = \"seo_blog_writer\"\n",
        "        elif \"linkedin\" in query.lower():\n",
        "            print(\"Routing to LinkedIn Post Writer Agent\")\n",
        "            next_node = \"linkedin_post_writer\"\n",
        "        elif \"image\" in query.lower() or \"visual\" in query.lower():\n",
        "            print(\"Routing to Image Generation Agent\")\n",
        "            next_node = \"image_generation\"\n",
        "        else:\n",
        "            print(\"Query does not match specific routes, routing to Content Strategist Agent (default)\")\n",
        "    else:\n",
        "        print(\"No query in state, routing to Content Strategist (default)\")\n",
        "\n",
        "    print(f\"Router node returning route: {next_node} and query: {query}\")\n",
        "    # Return a dictionary containing both the next node name AND the original query\n",
        "    # This ensures the query is preserved in the state when the next node is called.\n",
        "    return {\"route\": next_node, \"query\": query}\n",
        "\n",
        "\n",
        "# The agent functions (deep_research_agent, seo_blog_writer_agent, etc.)\n",
        "# should be the updated versions from the previous cell that correctly access\n",
        "# state using dictionary methods (.get()) and return the full state dictionary.\n",
        "# Assuming these functions are defined in the cells above this one and are correct.\n",
        "\n",
        "# Re-compile the graph with the modified router_node\n",
        "workflow = StateGraph(dict) # State is a dictionary\n",
        "\n",
        "# Add the nodes (agents) - router is the entry point\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"deep_research\", deep_research_agent)\n",
        "workflow.add_node(\"seo_blog_writer\", seo_blog_writer_agent)\n",
        "workflow.add_node(\"linkedin_post_writer\", linkedin_post_writer_agent)\n",
        "workflow.add_node(\"image_generation\", image_generation_agent)\n",
        "workflow.add_node(\"content_strategist\", content_strategist_agent)\n",
        "\n",
        "# Set the entry point to the router node\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# The router node conditionally routes to different agents based on the 'route' key in the state\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state.get(\"route\"), # Condition still checks the 'route' key returned by the router\n",
        "    {\n",
        "        \"deep_research\": \"deep_research\",\n",
        "        \"seo_blog_writer\": \"seo_blog_writer\",\n",
        "        \"linkedin_post_writer\": \"linkedin_post_writer\",\n",
        "        \"image_generation\": \"image_generation\",\n",
        "        \"content_strategist\": \"content_strategist\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add edges from content generation nodes to the content strategist\n",
        "workflow.add_edge(\"deep_research\", \"content_strategist\")\n",
        "workflow.add_edge(\"seo_blog_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"linkedin_post_writer\", \"content_strategist\")\n",
        "workflow.add_edge(\"image_generation\", \"content_strategist\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully with router node explicitly returning query!\")\n",
        "\n",
        "# Run the compiled graph with a sample query\n",
        "sample_query = \"research about the benefits of content marketing\"\n",
        "\n",
        "# Initialize the graph's input data as a dictionary\n",
        "initial_input_data = {\"query\": sample_query}\n",
        "\n",
        "\n",
        "print(f\"--- Running workflow for query: {sample_query} ---\")\n",
        "\n",
        "try:\n",
        "    final_state = app.invoke(initial_input_data)\n",
        "\n",
        "    print(\"\\n--- Final State ---\")\n",
        "    print(final_state)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during workflow execution ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n--- Final State (if available) ---\")\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy API keys set in environment variables for testing.\n",
            "LangGraph workflow re-compiled successfully with router node explicitly returning query!\n",
            "--- Running workflow for query: research about the benefits of content marketing ---\n",
            "---ROUTER NODE---\n",
            "Routing to Deep Research Agent\n",
            "Router node returning route: deep_research and query: research about the benefits of content marketing\n",
            "---DEEP RESEARCH AGENT---\n",
            "Performing deep research for: research about the benefits of content marketing\n",
            "Error: SERPAPI_API_KEY not found or is dummy key.\n",
            "---CONTENT STRATEGIST AGENT---\n",
            "Organizing and formatting final content using LLM.\n",
            "Error: OPENAI_API_KEY not found or is dummy key.\n",
            "\n",
            "---FINAL CONTENT OUTPUT (Raw State Data)---\n",
            "Query: research about the benefits of content marketing\n",
            "Research Data: Error: SERPAPI_API_KEY not configured or is dummy.\n",
            "Blog Content: None\n",
            "LinkedIn Content: None\n",
            "Image Prompt: None\n",
            "Generated Image Data: None\n",
            "\n",
            "--- Final State ---\n",
            "{'route': 'deep_research', 'query': 'research about the benefits of content marketing', 'research_data': 'Error: SERPAPI_API_KEY not configured or is dummy.'}\n"
          ]
        }
      ]
    }
  ]
}