{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYWegk537VWhWAlrZ0HqmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harooniqbal4879/AgenticAI/blob/main/Capstone2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ike0YYCqG318"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiHrpebEGwHB",
        "outputId": "9f9fc22b-0998-47f0-d818-33c68e32c1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Requirement already satisfied: serpapi in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Requirement already satisfied: serpapi in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# 1. Installthe required\n",
        "!pip install openai serpapi requests python-dotenv\n",
        "!pip install pyngrok\n",
        "!pip install openai serpapi requests python-dotenv pyngrok\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata # Import userdata\n",
        "from pyngrok import ngrok # from google.colab import userdata # Keep this line commented out if using direct token\n",
        "import uuid # For generating state\n",
        "import http.server\n",
        "import socketserver\n",
        "import threading\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1.5: Start a simple HTTP server for ngrok to connect to ---\n",
        "# This server will just acknowledge requests, as the main logic is in the Colab notebook\n",
        "\n",
        "PORT = 8000\n",
        "\n",
        "class Handler(http.server.SimpleHTTPRequestHandler):\n",
        "    def do_GET(self):\n",
        "        # This is where LinkedIn will redirect with the authorization code\n",
        "        # In a real application, you would parse the 'code' and 'state' from self.path\n",
        "        # For this notebook, we just need the server to run so ngrok can connect\n",
        "        print(f\"Received request on {self.path}\")\n",
        "        self.send_response(200)\n",
        "        self.send_header(\"Content-type\", \"text/html\")\n",
        "        self.end_headers()\n",
        "        self.wfile.write(b\"<html><body><h1>Received request. Check Colab output for details.</h1></body></html>\")\n",
        "\n",
        "# Use threading to run the server in the background\n",
        "def run_server():\n",
        "    with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n",
        "        print(f\"Serving HTTP on port {PORT}\")\n",
        "        httpd.serve_forever()\n",
        "\n",
        "# Run the server in a separate thread so the Colab notebook can continue\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.daemon = True  # Allow the main program to exit even if the thread is running\n",
        "server_thread.start()\n",
        "\n",
        "print(\"Simple HTTP server started in a background thread on port 8000.\")\n",
        "print(\"You can now run the ngrok cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWhGftiPG5U2",
        "outputId": "3a1f66a1-9637-429e-a596-db4356298b54"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple HTTP server started in a background thread on port 8000.\n",
            "You can now run the ngrok cell.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata # Keep this line commented out if using direct token\n",
        "\n",
        "# Get the ngrok authtoken (using the provided token directly for now)\n",
        "# NGROK_AUTH_TOKEN = userdata.get(\"NGROK_AUTH_TOKEN\") # Comment out if using direct token\n",
        "NGROK_AUTH_TOKEN = \"329d7PaznWat7gpdKBNidRNI3u1_4prNmGXkz1KothxLMXDMX\" # Use the provided token\n",
        "\n",
        "# Authenticate ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Expose port 8000\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBVwQTH2G5jX",
        "outputId": "cbd3a9d3-36cf-465c-f293-30767801a6ea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Thread-7 (run_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-842775339.py\", line 19, in run_server\n",
            "  File \"/usr/lib/python3.12/socketserver.py\", line 457, in __init__\n",
            "    self.server_bind()\n",
            "  File \"/usr/lib/python3.12/socketserver.py\", line 478, in server_bind\n",
            "    self.socket.bind(self.server_address)\n",
            "OSError: [Errno 98] Address already in use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://63a132bfaf33.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API keys from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "NGROK_AUTH_TOKEN = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "# Initialize OpenAI Client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Set ngrok authtoken (using provided token directly for demonstration)\n",
        "# For security, prefer storing in Colab secrets and using userdata.get()\n",
        "ngrok.set_auth_token(\"329d7PaznWat7gpdKBNidRNI3u1_4prNmGXkz1KothxLMXDMX\")\n",
        "\n",
        "# Expose port 8000 using ngrok to get a public redirect URL\n",
        "# You can change the port if your local server runs on a different one\n",
        "try:\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"ngrok Public URL:\", public_url)\n",
        "    # Use this ngrok URL as your REDIRECT_URI in your LinkedIn Developer App settings\n",
        "    # Convert PyngrokTunnel object to string and append /callback\n",
        "    # REDIRECT_URI = str(public_url) + \"/callback\"\n",
        "    REDIRECT_URI = public_url.public_url + \"/callback\"\n",
        "    # Ensure the redirect URI in your LinkedIn app exactly matches this URL\n",
        "    print(f\"Use this as your Redirect URL in LinkedIn app settings: {REDIRECT_URI}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok: {e}\")\n",
        "    print(\"Please ensure you have added your NGROK_AUTH_TOKEN to Colab secrets.\")\n",
        "    # Fallback placeholder - make sure this matches your LinkedIn app if ngrok fails\n",
        "    REDIRECT_URI = \"YOUR_NGROK_OR_OTHER_REDIRECT_URL\"\n",
        "    print(f\"Using fallback REDIRECT_URI: {REDIRECT_URI}\")\n",
        "\n",
        "\n",
        "# Your LinkedIn Application Credentials\n",
        "CLIENT_ID = \"868ac47cwrbkde\"\n",
        "CLIENT_SECRET = \"WPL_AP1.Jo7dDtzbbx5Cum1r.OIzirg==\"\n",
        "\n",
        "# The scope for posting content (w_member_social is required for sharing)\n",
        "SCOPE = \"w_member_social\" # Use w_member_social for posting\n",
        "\n",
        "# The state parameter is used to maintain state between the request and callback\n",
        "STATE = str(uuid.uuid4())\n",
        "\n",
        "# Generate the authorization URL\n",
        "auth_url = f\"https://www.linkedin.com/oauth/v2/authorization?response_type=code&client_id={CLIENT_ID}&redirect_uri={REDIRECT_URI}&state={STATE}&scope={SCOPE}\"\n",
        "\n",
        "print(\"\\n--- Step 2: Authorize Application ---\")\n",
        "print(\"Please open the following URL in your browser and authorize your LinkedIn application:\")\n",
        "print(auth_url)\n",
        "\n",
        "# After authorizing, LinkedIn will redirect you to your REDIRECT_URI with a 'code' and 'state' query parameter.\n",
        "# You will need to manually capture the 'code' from the redirect URL in the next step."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ZNqg0FG6G0",
        "outputId": "f8fd295d-8d15-4ddf-d81f-26edb68bd272"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok Public URL: NgrokTunnel: \"https://d470ad0be463.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "Use this as your Redirect URL in LinkedIn app settings: https://d470ad0be463.ngrok-free.app/callback\n",
            "\n",
            "--- Step 2: Authorize Application ---\n",
            "Please open the following URL in your browser and authorize your LinkedIn application:\n",
            "https://www.linkedin.com/oauth/v2/authorization?response_type=code&client_id=868ac47cwrbkde&redirect_uri=https://d470ad0be463.ngrok-free.app/callback&state=de086106-3ffa-4c2a-bf08-0d7f783b68e2&scope=w_member_social\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Your LinkedIn app credentials\n",
        "CLIENT_ID = \"868ac47cwrbkde\"\n",
        "CLIENT_SECRET = \"WPL_AP1.Jo7dDtzbbx5Cum1r.OIzirg==\"\n",
        "REDIRECT_URI = \"https://0864f959e430.ngrok-free.app/callback\"  # must match LinkedIn app settings\n",
        "AUTH_CODE = \"AQSW7Lrjvz0XsMdRegFspzf0zskv2w4EYUWdFGlqCQLauD8AJcTW4NDOEM7LvDvJIKMmddK-ECrxAjyA--QjdQBvyACWH_fvQv372yWyiO97BgTZzPP6kfjxcxcgZfIi3H1AwOcsGwbmquAuEoM2suBhi4uEewjNC-93-7zyx5P8Tx-pUzK-U5sp1wLAe3YrBwUE7iPaNaSYwlxBHps&state=a69bd3df-a561-4915-bb92-8d3fd58fb1e5\"\n",
        "\n",
        "# Exchange code for access token\n",
        "token_url = \"https://www.linkedin.com/oauth/v2/accessToken\"\n",
        "data = {\n",
        "    \"grant_type\": \"authorization_code\",\n",
        "    \"code\": AUTH_CODE,\n",
        "    \"redirect_uri\": REDIRECT_URI,\n",
        "    \"client_id\": CLIENT_ID,\n",
        "    \"client_secret\": CLIENT_SECRET\n",
        "}\n",
        "\n",
        "response = requests.post(token_url, data=data, headers={\"Content-Type\": \"application/x-www-form-urlencoded\"})\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgPalZYCjD5p",
        "outputId": "fba423f5-cd35-42ec-f20f-791e2ccb467f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error': 'invalid_request', 'error_description': 'Unable to retrieve access token: authorization code not found'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste the authorization code you got from the redirect URL here\n",
        "AUTHORIZATION_CODE = \"AQQ2b2olVzjhLvL6eZAIcMfiGXNge2tokVvWNVa2N0bS7LTBXVjDPiKMbK4oIP3ty2PNgjbpbMRWT00kDUI8iPQN0nlW390uQTpoCUUUjSUt_jc2sLcxkgbjYYcDHkYr_aaSRIm0SeYX-VaQxC6c7XWuYpKKzuFjMMpYTCSSSK6km_yLSGzKymdUSBwr6Jx1jDDecvdm_plSd2u4SR8&state=a69bd3df-a561-4915-bb92-8d3fd58fb1e5\"\n",
        "\n",
        "# LinkedIn Token Endpoint\n",
        "TOKEN_URL = \"https://www.linkedin.com/oauth/v2/accessToken\"\n",
        "\n",
        "# Ensure REDIRECT_URI is defined (should be from the previous cell)\n",
        "# If running this cell independently, define REDIRECT_URI here:\n",
        "# REDIRECT_URI = \"https://your-ngrok-url/callback\" # Replace with your ngrok public URL + /callback\n",
        "\n",
        "# Prepare the data for the token exchange request\n",
        "token_data = {\n",
        "    \"grant_type\": \"authorization_code\",\n",
        "    \"code\": AUTHORIZATION_CODE,\n",
        "    \"redirect_uri\": REDIRECT_URI,\n",
        "    \"client_id\": CLIENT_ID, # Ensure CLIENT_ID is defined (should be from previous cell)\n",
        "    \"client_secret\": CLIENT_SECRET # Ensure CLIENT_SECRET is defined (should be from previous cell)\n",
        "}\n",
        "\n",
        "print(\"\\n--- Step 4: Exchange Authorization Code for Access Token ---\")\n",
        "# Make the POST request to exchange the authorization code for an access token\n",
        "try:\n",
        "    response = requests.post(TOKEN_URL, data=token_data)\n",
        "    token_response_data = response.json()\n",
        "\n",
        "    # Check for errors\n",
        "    if \"error\" in token_response_data:\n",
        "        print(\"Error exchanging authorization code for access token:\")\n",
        "        print(token_response_data)\n",
        "        ACCESS_TOKEN = None\n",
        "    else:\n",
        "        ACCESS_TOKEN = token_response_data.get(\"access_token\")\n",
        "        expires_in = token_response_data.get(\"expires_in\")\n",
        "        print(\"Successfully obtained Access Token:\")\n",
        "        print(f\"Access Token: {ACCESS_TOKEN[:10]}...\") # Print truncated token for security\n",
        "        print(f\"Expires in: {expires_in} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during token exchange: {e}\")\n",
        "    ACCESS_TOKEN = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUPayH-4G5vY",
        "outputId": "0a600cfa-6384-4022-87b6-6e030580a181"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 4: Exchange Authorization Code for Access Token ---\n",
            "Error exchanging authorization code for access token:\n",
            "{'error': 'invalid_request', 'error_description': 'Unable to retrieve access token: authorization code not found'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load env vars (you already set OPENAI_API_KEY, SERPAPI_API_KEY in Colab environment)\n",
        "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) # Original line\n",
        "client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\")) # Use userdata to get the key\n",
        "\n",
        "# --- Step 2: Blog Generation Function ---\n",
        "def generate_blog(content_idea: str):\n",
        "    \"\"\"Generates a LinkedIn blog post using OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert LinkedIn content writer.\n",
        "    Write a professional, engaging LinkedIn blog post from the following idea:\n",
        "    {content_idea}\n",
        "\n",
        "    Make sure to:\n",
        "    - Start with a hook\n",
        "    - Use short paragraphs\n",
        "    - Add bullet points where helpful\n",
        "    - Include a clear call-to-action\n",
        "    - Suggest relevant hashtags\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # using GPT-4o\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- Step 3: Placeholder LinkedIn Posting Function ---\n",
        "def post_to_linkedin(blog_text: str):\n",
        "    \"\"\"\n",
        "    Placeholder for LinkedIn API Posting.\n",
        "    You need:\n",
        "    - LinkedIn Developer App\n",
        "    - Client ID, Client Secret\n",
        "    - Access Token with w_member_social scope\n",
        "    \"\"\"\n",
        "    print(\"Simulating LinkedIn Post...\")\n",
        "    print(\"Post Content:\")\n",
        "    print(blog_text[:500], \"...\")  # preview first 500 chars\n",
        "    print(\"\\n[LinkedIn API credentials required to post automatically]\")\n",
        "\n",
        "# --- Step 4: Execution Example ---\n",
        "print(\"\\n--- Step 4: Generate and Simulate Posting Blog ---\")\n",
        "# Get content idea from user input\n",
        "content_idea = input(\"Enter the content idea for the LinkedIn blog post: \")\n",
        "\n",
        "blog_post = generate_blog(content_idea)\n",
        "\n",
        "# Show blog\n",
        "print(\"\\n=== Generated LinkedIn Blog ===\")\n",
        "print(blog_post)\n",
        "\n",
        "# Simulated LinkedIn posting\n",
        "post_to_linkedin(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDzUZtD5G57i",
        "outputId": "98e7db1c-c400-4b87-feaf-44942000ed39"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 4: Generate and Simulate Posting Blog ---\n",
            "Enter the content idea for the LinkedIn blog post: Is the AI future of business?\n",
            "\n",
            "=== Generated LinkedIn Blog ===\n",
            "**The AI Future of Business: Are We Ready?**\n",
            "\n",
            "Imagine a world where your coffee maker knows your morning routine better than you do, where your work emails are drafted before you even think about them, and where every decision is backed by data-driven insights. This isnâ€™t a scene from a sci-fi movieâ€”it's the rapidly approaching future of business, powered by Artificial Intelligence (AI).\n",
            "\n",
            "**Why AI Matters**\n",
            "\n",
            "AI is no longer a futuristic concept; itâ€™s a present reality thatâ€™s reshaping industries and redefining the way we work. From streamlining operations to enhancing customer experiences, the impact of AI is profound and pervasive.\n",
            "\n",
            "- **Efficiency and Productivity**: AI automates repetitive tasks, freeing up valuable time for employees to focus on strategic initiatives.\n",
            "- **Data-Driven Decisions**: With AI, businesses can analyze vast amounts of data in seconds, leading to more informed and timely decisions.\n",
            "- **Enhanced Customer Experience**: AI-driven chatbots and personalized marketing strategies create seamless and personalized customer interactions.\n",
            "\n",
            "**Challenges Ahead**\n",
            "\n",
            "While the benefits are undeniable, the integration of AI into business processes is not without its challenges:\n",
            "\n",
            "- **Ethical Considerations**: How do we ensure AI is used responsibly and fairly?\n",
            "- **Skill Gaps**: Are we prepared to upskill our workforce to thrive in an AI-driven environment?\n",
            "- **Security Concerns**: As AI systems handle sensitive data, how do we protect against cyber threats?\n",
            "\n",
            "**Embrace the AI Revolution**\n",
            "\n",
            "So, how can businesses prepare for this AI-driven future? Here are a few strategies:\n",
            "\n",
            "- **Invest in Training**: Equip your team with the skills they need to leverage AI tools effectively.\n",
            "- **Foster a Culture of Innovation**: Encourage creativity and experimentation to harness AIâ€™s full potential.\n",
            "- **Collaborate with AI Experts**: Partner with specialists to navigate the complexities of AI integration.\n",
            "\n",
            "The future is here, and itâ€™s powered by AI. Are you ready to embrace it? Engage with this post and share your thoughts on how AI is transforming your industry or business. Letâ€™s start the conversation and shape the future together.\n",
            "\n",
            "#AI #BusinessInnovation #FutureOfWork #DigitalTransformation #Leadership #DataDriven #TechTrends #AIRevolution\n",
            "\n",
            "Feel free to connect with me for more insights on leveraging AI for your business success!\n",
            "Simulating LinkedIn Post...\n",
            "Post Content:\n",
            "**The AI Future of Business: Are We Ready?**\n",
            "\n",
            "Imagine a world where your coffee maker knows your morning routine better than you do, where your work emails are drafted before you even think about them, and where every decision is backed by data-driven insights. This isnâ€™t a scene from a sci-fi movieâ€”it's the rapidly approaching future of business, powered by Artificial Intelligence (AI).\n",
            "\n",
            "**Why AI Matters**\n",
            "\n",
            "AI is no longer a futuristic concept; itâ€™s a present reality thatâ€™s reshaping industries ...\n",
            "\n",
            "[LinkedIn API credentials required to post automatically]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Blog Generation Function (from original code) ---\n",
        "def generate_blog(content_idea: str):\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert LinkedIn content writer.\n",
        "    Write a professional, engaging LinkedIn blog post from the following idea:\n",
        "    {content_idea}\n",
        "\n",
        "    Make sure to:\n",
        "    - Start with a hook\n",
        "    - Use short paragraphs\n",
        "    - Add bullet points where helpful\n",
        "    - Include a clear call-to-action\n",
        "    - Suggest relevant hashtags\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # using GPT-4o\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- Step 6: Generate Blog Content ---\n",
        "content_idea = \"How Agentic AI systems are transforming business workflows\"\n",
        "blog_post = generate_blog(content_idea)\n",
        "\n",
        "# Show blog\n",
        "print(\"--- Generated LinkedIn Blog ---\")\n",
        "print(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yfrYmqfG6R9",
        "outputId": "66bf78ca-df47-4bf1-857e-12c51eaa7ce4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generated LinkedIn Blog ---\n",
            "ðŸš€ **The Future is Here: How Agentic AI Systems Are Transforming Business Workflows**\n",
            "\n",
            "Imagine a world where your business processes operate with the efficiency of a well-oiled machine, predicting challenges and seizing opportunities before they even arise. Welcome to the era of agentic AI systemsâ€”a revolutionary force that is reshaping how businesses function, making operations smarter, faster, and more efficient than ever before.\n",
            "\n",
            "### The Power of Agentic AI\n",
            "\n",
            "Agentic AI systems are not just tools; they are intelligent agents capable of independent action. They analyze data, learn patterns, and make decisions without constant human intervention. This capability is transforming business workflows across industries.\n",
            "\n",
            "### Why Businesses Are Embracing Agentic AI\n",
            "\n",
            "- **Enhanced Efficiency**: By automating routine tasks, agentic AI liberates human resources, allowing employees to focus on strategic initiatives.\n",
            "  \n",
            "- **Improved Decision-Making**: With real-time data analysis, businesses can make informed decisions swiftly, adapting to market changes as they happen.\n",
            "\n",
            "- **Cost Reduction**: Automation reduces the likelihood of human error and operational costs, providing a significant return on investment.\n",
            "\n",
            "- **Scalability**: Agentic AI systems can easily scale operations up or down, meeting the dynamic needs of any business.\n",
            "\n",
            "### Real-World Applications\n",
            "\n",
            "Agentic AI is already making waves in various industries:\n",
            "\n",
            "- **Customer Service**: AI-driven chatbots provide personalized customer experiences, resolving queries 24/7.\n",
            "  \n",
            "- **Supply Chain Management**: AI optimizes inventory levels, predicts demand, and improves logistics.\n",
            "  \n",
            "- **Healthcare**: AI assists in diagnostics, patient management, and personalized treatment plans.\n",
            "\n",
            "### Embrace the Change\n",
            "\n",
            "The integration of agentic AI systems is not just a trend; it's a business imperative. To stay competitive, companies must explore how these systems can enhance their workflows. Start small, perhaps by automating a single process, and grow from there.\n",
            "\n",
            "### Call to Action\n",
            "\n",
            "Are you ready to transform your business workflows with agentic AI? Start today by assessing which of your processes could benefit from automation and intelligence. Reach out to AI experts, attend workshops, and invest in training your team to embrace this technology.\n",
            "\n",
            "ðŸ”— Connect with industry professionals and share your experiences. Let's lead the AI revolution together!\n",
            "\n",
            "### Relevant Hashtags\n",
            "\n",
            "#AgenticAI #BusinessTransformation #AIInnovation #FutureOfWork #WorkflowAutomation #ArtificialIntelligence #BusinessEfficiency\n",
            "\n",
            "---\n",
            "\n",
            "In this rapidly evolving technological landscape, those who adapt will thrive. Be a pioneer in your industry by harnessing the power of agentic AI. ðŸŒŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ACCESS_TOKEN:\n",
        "    # LinkedIn Sharing API Endpoint\n",
        "    POST_URL = \"https://api.linkedin.com/v2/ugcPosts\"\n",
        "\n",
        "    # Get the URN of the authenticated user\n",
        "    profile_url = \"https://api.linkedin.com/v2/me\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "        \"X-Restli-Protocol-Version\": \"2.0.0\"\n",
        "    }\n",
        "    try:\n",
        "        profile_response = requests.get(profile_url, headers=headers)\n",
        "        profile_response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        profile_data = profile_response.json()\n",
        "        # The person URN is typically in the 'id' field for the '/v2/me' endpoint\n",
        "        author_urn = f\"urn:linkedin:person:{profile_data.get('id')}\"\n",
        "        print(f\"Obtained Author URN: {author_urn}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching user profile: {e}\")\n",
        "        author_urn = None\n",
        "        print(\"Could not retrieve author URN. Cannot post to LinkedIn.\")\n",
        "\n",
        "\n",
        "    if author_urn:\n",
        "        # Prepare the post data\n",
        "        post_data = {\n",
        "            \"author\": author_urn,\n",
        "            \"lifecycleState\": \"PUBLISHED\",\n",
        "            \"specificContent\": {\n",
        "                \"com.linkedin.ugc.ShareContent\": {\n",
        "                    \"shareCommentary\": {\n",
        "                        \"text\": blog_post # Use the blog_post generated earlier\n",
        "                    },\n",
        "                    \"shareMediaCategory\": \"NONE\" # Or \"ARTICLE\", \"IMAGE\", etc.\n",
        "                }\n",
        "            },\n",
        "            \"visibility\": {\n",
        "                \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Make the POST request to publish the blog\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"X-Restli-Protocol-Version\": \"2.0.0\"\n",
        "        }\n",
        "        print(\"\\n--- Step 8: Publishing Blog Post ---\")\n",
        "        try:\n",
        "            post_response = requests.post(POST_URL, json=post_data, headers=headers)\n",
        "            post_response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "            if post_response.status_code == 201:\n",
        "                print(\"Blog post published successfully!\")\n",
        "                # The response contains the ID of the created post\n",
        "                print(\"Post Response:\", post_response.json())\n",
        "            else:\n",
        "                 # This part might be redundant due to raise_for_status, but kept for clarity\n",
        "                print(f\"Error publishing blog post. Status code: {post_response.status_code}\")\n",
        "                print(\"Response:\", post_response.text)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error publishing blog post: {e}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Access token not available. Cannot post to LinkedIn.\")\n",
        "    print(\"Please ensure you completed the authorization steps and obtained an access token.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4eH_GT0Jxxw",
        "outputId": "ffd8faa2-0a6f-4042-ca8b-d5d617a12850"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access token not available. Cannot post to LinkedIn.\n",
            "Please ensure you completed the authorization steps and obtained an access token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Handler Agent: Routes requests to appropriate specialized agents\n",
        "\n",
        "def route_query(query: str, client):\n",
        "    \"\"\"\n",
        "    Routes a user query to a hypothetical specialized agent based on its content.\n",
        "    Uses an LLM to categorize the query.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following user query and determine the primary task or intent.\n",
        "    Suggest which type of specialized agent or action would be best suited to handle it.\n",
        "    Possible categories/agents include:\n",
        "    - LinkedIn Posting (for requests related to creating or publishing LinkedIn content)\n",
        "    - Data Analysis (for requests involving data manipulation, analysis, or visualization)\n",
        "    - Code Generation (for requests asking for new code snippets)\n",
        "    - Code Explanation (for requests asking to explain existing code)\n",
        "    - API Interaction (for requests involving external APIs like LinkedIn, ngrok, etc.)\n",
        "    - General Question (for general inquiries not fitting other categories)\n",
        "    - Other (if none of the above fit well)\n",
        "\n",
        "    Respond with ONLY the suggested category/agent name.\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # Using GPT-4o as it's capable of understanding intent\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=50, # Keep the response concise\n",
        "            temperature=0.1 # Keep the response focused\n",
        "        )\n",
        "        suggested_agent = response.choices[0].message.content.strip()\n",
        "        return suggested_agent\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during query routing: {e}\")\n",
        "        return \"Error Routing\"\n",
        "\n",
        "# Example Usage (assuming 'client' is initialized from a previous cell)\n",
        "# if 'client' in globals():\n",
        "#     user_query = \"Generate a LinkedIn post about the benefits of cloud computing.\"\n",
        "#     suggested_handler = route_query(user_query, client)\n",
        "#     print(f\"Query: '{user_query}'\")\n",
        "#     print(f\"Suggested Handler: {suggested_handler}\")\n",
        "\n",
        "#     user_query_2 = \"Explain the code in cell XYZ.\"\n",
        "#     suggested_handler_2 = route_query(user_query_2, client)\n",
        "#     print(f\"Query: '{user_query_2}'\")\n",
        "#     print(f\"Suggested Handler: {suggested_handler_2}\")\n",
        "# else:\n",
        "#     print(\"OpenAI client not initialized. Cannot use query router.\")"
      ],
      "metadata": {
        "id": "UzoWlpsuJxau"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3dc4968"
      },
      "source": [
        "# Deep Research Agent: Conducts comprehensive web research and analysis\n",
        "\n",
        "class DeepResearchAgent:\n",
        "    \"\"\"\n",
        "    An agent designed to conduct comprehensive web research and analyze findings.\n",
        "    \"\"\"\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "        # Initialize any tools or resources needed for research here\n",
        "        # e.g., search API client, scraping tools, analysis modules\n",
        "\n",
        "    def conduct_research(self, query: str):\n",
        "        \"\"\"\n",
        "        Conducts web research based on the query and analyzes the results.\n",
        "\n",
        "        Args:\n",
        "            query: The research query.\n",
        "\n",
        "        Returns:\n",
        "            A summary or structured analysis of the research findings.\n",
        "        \"\"\"\n",
        "        print(f\"Deep Research Agent: Conducting research for query: '{query}'\")\n",
        "        # Placeholder for web research logic\n",
        "        # This would involve using search APIs (like SERP API),\n",
        "        # potentially scraping, parsing information, and synthesizing it.\n",
        "        # Example: Using a hypothetical search tool\n",
        "        # search_results = self.search_tool.perform_search(query)\n",
        "        # analysis = self.analyze_results(search_results)\n",
        "        # return analysis\n",
        "\n",
        "        return \"Deep Research Agent: Research function placeholder.\"\n",
        "\n",
        "    # def analyze_results(self, results):\n",
        "    #     \"\"\"\n",
        "    #     Analyzes the raw research results.\n",
        "    #     \"\"\"\n",
        "    #     # Placeholder for analysis logic\n",
        "    #     pass\n",
        "\n",
        "# Example Usage (assuming 'client' is initialized)\n",
        "# if 'client' in globals():\n",
        "#     research_agent = DeepResearchAgent(client)\n",
        "#     research_findings = research_agent.conduct_research(\"latest trends in AI\")\n",
        "#     print(research_findings)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bhjsjBC3n60D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SEO Blog Writer Agent: Creates search-optimized long-form content\n",
        "\n",
        "class SEOBlogWriterAgent:\n",
        "    \"\"\"\n",
        "    An agent designed to create search-optimized long-form blog content.\n",
        "    \"\"\"\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "        # Initialize any tools or resources needed for SEO analysis or content generation here\n",
        "        # e.g., keyword research tools, SEO analysis modules\n",
        "\n",
        "    def generate_seo_blog(self, topic: str, keywords: list = None):\n",
        "        \"\"\"\n",
        "        Generates long-form blog content optimized for search engines.\n",
        "\n",
        "        Args:\n",
        "            topic: The main topic of the blog post.\n",
        "            keywords: A list of keywords to incorporate for SEO.\n",
        "\n",
        "        Returns:\n",
        "            The generated SEO-optimized blog content.\n",
        "        \"\"\"\n",
        "        print(f\"SEO Blog Writer Agent: Generating blog post on '{topic}'...\")\n",
        "        if keywords:\n",
        "            print(f\"Optimizing for keywords: {', '.join(keywords)}\")\n",
        "\n",
        "        # Placeholder for advanced SEO content generation logic\n",
        "        # This would involve:\n",
        "        # 1. Using keywords to refine the prompt.\n",
        "        # 2. Structuring the content with headings, subheadings.\n",
        "        # 3. Incorporating keywords naturally throughout the text.\n",
        "        # 4. Potentially analyzing generated text for SEO score.\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Write a detailed, long-form blog post about \"{topic}\".\n",
        "        \"\"\"\n",
        "        if keywords:\n",
        "             prompt += f\"\\nEnsure the content is optimized for the following keywords: {', '.join(keywords)}.\"\n",
        "             prompt += \"\\nStructure the blog with a clear introduction, body paragraphs, and a conclusion. Use headings and subheadings where appropriate.\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",  # Using GPT-4o for detailed content\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.7, # Allow for some creativity\n",
        "                max_tokens=1500 # Allow for longer content\n",
        "            )\n",
        "            blog_content = response.choices[0].message.content.strip()\n",
        "            return blog_content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during blog generation: {e}\")\n",
        "            return \"SEO Blog Writer Agent: Blog generation failed.\"\n",
        "\n",
        "# Example Usage (assuming 'client' is initialized)\n",
        "# if 'client' in globals():\n",
        "#     seo_writer = SEOBlogWriterAgent(client)\n",
        "#     blog_post = seo_writer.generate_seo_blog(\n",
        "#         topic=\"The Impact of AI on Marketing\",\n",
        "#         keywords=[\"AI in marketing\", \"marketing automation\", \"AI tools for marketing\"]\n",
        "#     )\n",
        "#     print(\"--- Generated SEO Blog Post ---\")\n",
        "#     print(blog_post)"
      ],
      "metadata": {
        "id": "g4JC9KdrnogG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21483f3"
      },
      "source": [
        "# Task\n",
        "Set up a Streamlit chat interface to take user input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10364dad"
      },
      "source": [
        "## Install streamlit\n",
        "\n",
        "### Subtask:\n",
        "Add a code cell to install the Streamlit library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0925e531"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the Streamlit library. This requires adding a new code cell with a pip install command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43511ca0",
        "outputId": "d41d6e13-9f60-4e54-f354-009e44594705"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35b0ca3b"
      },
      "source": [
        "## Write streamlit app code\n",
        "\n",
        "### Subtask:\n",
        "Create a new code cell to write the Python code for the Streamlit application. This code will include setting up a basic chat interface to capture user input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf83bfeb"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a Streamlit application with a basic chat interface to capture user input. I need to write Python code in a new cell to achieve this, importing streamlit, setting a title, adding a text input and a button, and displaying the input when the button is clicked.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "102de2d7",
        "outputId": "4e63e737-eebb-4203-f2ba-27a1dd49c186"
      },
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Set the title of the application\n",
        "st.title(\"Agentic AI Chat Interface\")\n",
        "\n",
        "# Create a text input field for user messages\n",
        "user_input = st.text_input(\"Enter your message:\")\n",
        "\n",
        "# Add a button to send the message\n",
        "send_button = st.button(\"Send\")\n",
        "\n",
        "# Check if the button is clicked and the input is not empty\n",
        "if send_button and user_input:\n",
        "    # Display the user's input\n",
        "    st.info(f\"You entered: {user_input}\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-02 23:13:35.863 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.015 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-09-02 23:13:36.016 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.017 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.021 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.023 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.025 Session state does not function when running a script without `streamlit run`\n",
            "2025-09-02 23:13:36.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.028 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.029 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.031 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.032 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.034 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.035 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.036 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-02 23:13:36.038 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7157a11f"
      },
      "source": [
        "## Run streamlit app\n",
        "\n",
        "### Subtask:\n",
        "Add a code cell to run the Streamlit application. This will typically involve using a shell command like `streamlit run your_app.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31be2d24"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the Streamlit app code to a file and then run it using the streamlit command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834a3299",
        "outputId": "6ea68e4e-96a8-4282-db65-0a2ff1022add"
      },
      "source": [
        "# Save the Streamlit app code to a file named app.py\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "# Set the title of the application\n",
        "st.title(\"Agentic AI Chat Interface\")\n",
        "\n",
        "# Create a text input field for user messages\n",
        "user_input = st.text_input(\"Enter your message:\")\n",
        "\n",
        "# Add a button to send the message\n",
        "send_button = st.button(\"Send\")\n",
        "\n",
        "# Check if the button is clicked and the input is not empty\n",
        "if send_button and user_input:\n",
        "    # Display the user's input\n",
        "    st.info(f\"You entered: {user_input}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "# Run the Streamlit application\n",
        "!streamlit run app.py & npx colab-tunnel 8501"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.225.201.163:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94mcode\u001b[39m E404\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m Not Found - GET https://registry.npmjs.org/colab-tunnel - Not found\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m  'colab-tunnel@*' is not in this registry.\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m Note that you can also install from a\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m tarball, folder, http url, or git url.\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m A complete log of this run can be found in: /root/.npm/_logs/2025-09-02T23_13_48_958Z-debug-0.log\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b691707",
        "outputId": "481274c9-6faa-4b76-df73-727dc78cb573"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the parent directory to the sys.path to import functions from Colab cells\n",
        "# This assumes the functions like generate_blog and post_to_linkedin\n",
        "# are defined in the main notebook environment or a file that can be imported.\n",
        "# In a real-world app, these would typically be structured differently.\n",
        "# For demonstration in Colab, we'll assume they are accessible or re-defined here.\n",
        "\n",
        "# --- Re-define necessary functions or import from notebook context ---\n",
        "# This is a simplified approach for demonstration in Colab.\n",
        "# In a production app, you would structure your code differently for imports.\n",
        "\n",
        "# Assuming the OpenAI client and helper functions are needed\n",
        "# We'll re-initialize the client here for the Streamlit app process\n",
        "# In a more robust app, you might pass the client or use a global/singleton\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    from google.colab import userdata\n",
        "    # Attempt to get the API key - this might not work directly in the Streamlit subprocess\n",
        "    # A more robust solution would involve passing the key or using env vars accessible to the subprocess\n",
        "    # For this example, we'll assume the key is accessible or handle the error.\n",
        "    # Let's add a note that this needs proper key handling in a real app.\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or (userdata.get(\"OPENAI_API_KEY\") if 'userdata' in sys.modules else None)\n",
        "\n",
        "    if OPENAI_API_KEY:\n",
        "         client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "         # Define the blog generation function (copied from cell ZDzUZtD5G57i)\n",
        "         def generate_blog(content_idea: str, openai_client):\n",
        "             \"\"\"Generates a LinkedIn blog post using OpenAI.\"\"\"\n",
        "             prompt = f\"\"\"\n",
        "             You are an expert LinkedIn content writer.\n",
        "             Write a professional, engaging LinkedIn blog post from the following idea:\n",
        "             {content_idea}\n",
        "\n",
        "             Make sure to:\n",
        "             - Start with a hook\n",
        "             - Use short paragraphs\n",
        "             - Add bullet points where helpful\n",
        "             - Include a clear call-to-action\n",
        "             - Suggest relevant hashtags\n",
        "             \"\"\"\n",
        "             try:\n",
        "                 response = openai_client.chat.completions.create(\n",
        "                     model=\"gpt-4o\",  # using GPT-4o\n",
        "                     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                     temperature=0.7\n",
        "                 )\n",
        "                 return response.choices[0].message.content\n",
        "             except Exception as e:\n",
        "                 st.error(f\"Error generating blog: {e}\")\n",
        "                 return None\n",
        "\n",
        "         # Define the placeholder LinkedIn posting function (copied from cell ZDzUZtD5G57i)\n",
        "         # NOTE: This still needs a valid ACCESS_TOKEN and proper LinkedIn API call\n",
        "         def post_to_linkedin(blog_text: str):\n",
        "             \"\"\"\n",
        "             Placeholder or actual LinkedIn API Posting.\n",
        "             Requires an Access Token with w_member_social scope.\n",
        "             \"\"\"\n",
        "             # In a real scenario, you would need to retrieve or have access to the ACCESS_TOKEN here.\n",
        "             # This is a significant challenge in a simple Streamlit Colab setup.\n",
        "             # For now, we'll simulate or add a clear placeholder.\n",
        "             st.warning(\"LinkedIn API posting is currently simulated.\")\n",
        "             st.write(\"Simulating LinkedIn Post...\")\n",
        "             st.write(\"Post Content:\")\n",
        "             st.write(blog_text[:500] + \"...\") # preview first 500 chars\n",
        "             st.info(\"[LinkedIn API credentials and valid Access Token required to post automatically]\")\n",
        "\n",
        "             # --- Placeholder for actual LinkedIn API call ---\n",
        "             # You would need:\n",
        "             # - ACCESS_TOKEN (obtained from the OAuth flow)\n",
        "             # - Your LinkedIn Person URN (obtained from /v2/me endpoint using the ACCESS_TOKEN)\n",
        "             # - The POST_URL = \"https://api.linkedin.com/v2/ugcPosts\"\n",
        "             #\n",
        "             # Example (commented out):\n",
        "             # if ACCESS_TOKEN: # Assuming ACCESS_TOKEN is somehow available\n",
        "             #     try:\n",
        "             #         # Get Author URN (example - would need to be done once and stored)\n",
        "             #         # profile_url = \"https://api.linkedin.com/v2/me\"\n",
        "             #         # headers = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\", \"X-Restli-Protocol-Version\": \"2.0.0\"}\n",
        "             #         # profile_response = requests.get(profile_url, headers=headers)\n",
        "             #         # profile_response.raise_for_status()\n",
        "             #         # profile_data = profile_response.json()\n",
        "             #         # author_urn = f\"urn:linkedin:person:{profile_data.get('id')}\"\n",
        "             #\n",
        "             #         # post_data = { ... construct post data with author_urn and blog_text ... }\n",
        "             #         # headers = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\", \"Content-Type\": \"application/json\", \"X-Restli-Protocol-Version\": \"2.0.0\"}\n",
        "             #         # post_response = requests.post(POST_URL, json=post_data, headers=headers)\n",
        "             #         # post_response.raise_for_status()\n",
        "             #         # st.success(\"Blog post published successfully!\")\n",
        "             #     except Exception as e:\n",
        "             #         st.error(f\"Error publishing blog post: {e}\")\n",
        "             # else:\n",
        "             #      st.error(\"Access Token not available. Cannot post to LinkedIn.\")\n",
        "\n",
        "    else:\n",
        "        st.error(\"OpenAI API key not found. Please set OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "        generate_blog = None # Disable blog generation if client not initialized\n",
        "        post_to_linkedin = None # Disable posting if client not initialized\n",
        "\n",
        "except ImportError as e:\n",
        "    st.error(f\"Required library not found: {e}. Please ensure all necessary libraries are installed.\")\n",
        "    generate_blog = None\n",
        "    post_to_linkedin = None\n",
        "\n",
        "\n",
        "# --- Streamlit UI Layout ---\n",
        "st.title(\"Agentic AI LinkedIn Blog Generator\")\n",
        "\n",
        "st.write(\"Enter your blog post idea below and generate content.\")\n",
        "\n",
        "# Text area for user to input their blog idea\n",
        "user_input = st.text_area(\"Blog Idea\", height=150, key=\"blog_idea_input\")\n",
        "\n",
        "# Use Streamlit's session state to store the generated blog post\n",
        "if 'generated_blog' not in st.session_state:\n",
        "    st.session_state.generated_blog = None\n",
        "\n",
        "# Button to trigger blog generation\n",
        "if st.button(\"Generate Blog\"):\n",
        "    if user_input and generate_blog:\n",
        "        st.info(\"Generating blog...\")\n",
        "        # Call the blog generation function\n",
        "        st.session_state.generated_blog = generate_blog(user_input, client)\n",
        "        if st.session_state.generated_blog:\n",
        "             st.success(\"Blog generated!\")\n",
        "    elif not user_input:\n",
        "        st.warning(\"Please enter a blog idea before generating.\")\n",
        "\n",
        "\n",
        "# Display the generated blog post if it exists in session state\n",
        "if st.session_state.generated_blog:\n",
        "    st.subheader(\"Generated Blog Preview:\")\n",
        "    st.markdown(st.session_state.generated_blog)\n",
        "\n",
        "    # Add a button to post to LinkedIn (only visible if a blog is generated)\n",
        "    if post_to_linkedin and st.button(\"Post to LinkedIn\"):\n",
        "        # Call the LinkedIn posting function (currently simulated or requires token)\n",
        "        post_to_linkedin(st.session_state.generated_blog)\n",
        "else:\n",
        "    st.info(\"Enter an idea above and click 'Generate Blog' to see a preview.\")\n",
        "\n",
        "\n",
        "# Optional: Add a section for status or logs\n",
        "# st.sidebar.subheader(\"Status\")\n",
        "# st.sidebar.write(\"App is running.\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d41f2cc4",
        "outputId": "6c945082-cb70-489b-e9e8-383cff63e432"
      },
      "source": [
        "# Run the Streamlit application in the background and expose it with ngrok\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Kill any existing ngrok tunnels\n",
        "print(\"Killing existing ngrok tunnels...\")\n",
        "ngrok.kill()\n",
        "print(\"Existing ngrok tunnels killed.\")\n",
        "\n",
        "# Start the Streamlit app in the background\n",
        "# Use subprocess.Popen to keep it running\n",
        "print(\"Starting Streamlit app...\")\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n",
        "\n",
        "# Give Streamlit a moment to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Expose the Streamlit port (default is 8501)\n",
        "try:\n",
        "    streamlit_public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app running at: {streamlit_public_url}\")\n",
        "    print(\"Click the ngrok URL above to access your Streamlit app.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok tunnel for Streamlit: {e}\")\n",
        "    print(\"Please ensure ngrok is authenticated and no other sessions are running.\")\n",
        "\n",
        "# Keep the notebook alive (optional, but helpful for long-running apps)\n",
        "# from google.colab import output\n",
        "# output.eval_js('google.colab.kernel.proxyPort(8501)')\n",
        "\n",
        "# Note: To stop the Streamlit app and ngrok tunnel, you might need to\n",
        "# interrupt the Colab cell execution or run ngrok.kill() in a new cell."
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Killing existing ngrok tunnels...\n",
            "Existing ngrok tunnels killed.\n",
            "Starting Streamlit app...\n",
            "Streamlit app running at: NgrokTunnel: \"https://7ad8b9805dbf.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "Click the ngrok URL above to access your Streamlit app.\n"
          ]
        }
      ]
    }
  ]
}