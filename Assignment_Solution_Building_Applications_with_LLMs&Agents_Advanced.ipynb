{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harooniqbal4879/AgenticAI/blob/main/Assignment_Solution_Building_Applications_with_LLMs%26Agents_Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "smart-study-buddy-intro",
      "metadata": {
        "id": "smart-study-buddy-intro"
      },
      "source": [
        "# Smart Study Buddy: RAG for Your Lecture Notes with Pinecone\n",
        "\n",
        "Welcome, aspiring AI developer! In this project, we'll build a \"Smart Study Buddy.\" Imagine you have multiple sets of lecture notes (as text files or PDFs) for different subjects. Wouldn't it be amazing to have an intelligent assistant that can answer your questions based *only* on these notes and even remember the context of your current study session? That's exactly what we're going to build!\n",
        "\n",
        "## The Problem: Information Overload!\n",
        "As a student, you collect a lot of notes. Finding specific information quickly can be a challenge, especially when you're juggling multiple subjects. You want a tool that helps you query your own study material efficiently.\n",
        "\n",
        "## Our Solution: A Conversational RAG System\n",
        "We will build a **Retrieval Augmented Generation (RAG)** system. Here's how it will work:\n",
        "1.  **Ingest Your Notes**: The Study Buddy will load lecture notes from local files (e.g., `.txt`, `.pdf`).\n",
        "2.  **Create a Knowledge Base**: It will process these notes, chop them into smaller, digestible chunks, generate numerical representations (embeddings) that capture their meaning, and store these embeddings in a [Pinecone](https://www.pinecone.io/) serverless vector index. Pinecone is a specialized database designed for super-fast similarity searches on these embeddings.\n",
        "3.  **Answer Questions Intelligently**: When you ask a question:\n",
        "    *   The system first retrieves the most relevant snippets from your notes (thanks to Pinecone's speedy search).\n",
        "    *   Then, it feeds these relevant snippets, your question, and the ongoing conversation history to a Large Language Model (LLM) like GPT.\n",
        "    *   The LLM generates an answer based *specifically* on the information from your notes.\n",
        "4.  **Maintain Session Context**: The Study Buddy will remember the flow of your current conversation using LangChain's memory features, allowing for natural follow-up questions.\n",
        "5.  **Honest and Focused Answers**: We'll instruct the LLM to answer *only* using the provided lecture note context. If the answer isn't in your notes, the Study Buddy will say so, rather than inventing information.\n",
        "\n",
        "## Why is this useful for students?\n",
        "-   **Focused Learning**: Get answers directly from your course material, avoiding the vast (and sometimes distracting or incorrect) expanse of the general internet.\n",
        "-   **Efficient Revision**: Quickly find information or clarify doubts without manually sifting through pages and pages of notes.\n",
        "-   **Personalized Study Aid**: It's an AI assistant trained and focused on *your* specific learning materials.\n",
        "\n",
        "## Prerequisites\n",
        "-   A Pinecone account and API key (sign up at [pinecone.io](https://www.pinecone.io/))\n",
        "-   An OpenAI API key (for the LLM and optionally for embeddings, sign up at [openai.com](https://openai.com/))\n",
        "-   Python 3.8 or newer.\n",
        "-   Sample lecture notes (we'll guide you to create a few `.txt` and `.pdf` files) in a local input folder."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-install-libs",
      "metadata": {
        "id": "ssb-install-libs"
      },
      "source": [
        "## 1. Install Required Libraries\n",
        "\n",
        "First, let's install the necessary Python packages. We'll need:\n",
        "-   `langchain`, `langchain-community`, `langchain-openai`: Core LangChain libraries for building our RAG pipeline, document loaders, text splitters, embedding wrappers, vector store integrations, LLM integrations, and memory.\n",
        "-   `pinecone-client`: The official Python client for interacting with your Pinecone vector database (version >= 3.x.x recommended for `ServerlessSpec`).\n",
        "-   `sentence-transformers`: For generating high-quality sentence and text embeddings (a popular free option).\n",
        "-   `openai`: The official Python client for OpenAI, used by LangChain for LLM interactions and OpenAI embeddings.\n",
        "-   `pypdf`: For loading content from PDF files.\n",
        "-   `python-dotenv`: For managing environment variables securely (like API keys).\n",
        "\n",
        "Uncomment the following cell to install these packages if you haven't already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-pip-install",
      "metadata": {
        "id": "ssb-pip-install"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langchain-community langchain-openai pinecone-client sentence-transformers openai pypdf python-dotenv \"unstructured[md,txt]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-load-env",
      "metadata": {
        "id": "ssb-load-env"
      },
      "source": [
        "## 2. Load Environment Variables and Initialize Core Components\n",
        "\n",
        "We'll load our API keys from a `.env` file for security and ease of configuration.\n",
        "\n",
        "**Action Required:** Create a file named `.env` in the same directory as this notebook with the following content:\n",
        "```\n",
        "PINECONE_API_KEY='YOUR_PINECONE_API_KEY'\n",
        "OPENAI_API_KEY='YOUR_OPENAI_API_KEY'\n",
        "```\n",
        "Replace `'YOUR_PINECONE_API_KEY'` and `'YOUR_OPENAI_API_KEY'` with your actual keys.\n",
        "\n",
        "After setting up your `.env` file, we'll import necessary modules and initialize the Pinecone client and our chosen embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-init-components",
      "metadata": {
        "id": "ssb-init-components"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader # Specific loaders\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings # Option 1 for embeddings\n",
        "# from langchain_openai import OpenAIEmbeddings # Option 2 for embeddings (paid)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.vectorstores import Pinecone as LangChainPinecone # LangChain's Pinecone integration\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get API keys from environment\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Verify API keys are loaded\n",
        "if not PINECONE_API_KEY:\n",
        "    raise ValueError(\"PINECONE_API_KEY not found. Please set it in your .env file or environment.\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in your .env file or environment.\")\n",
        "\n",
        "# Initialize Pinecone client\n",
        "# This client is used for direct Pinecone operations like creating/deleting indexes\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Initialize Embedding Model\n",
        "# We'll use HuggingFaceEmbeddings with 'all-MiniLM-L6-v2' as it's a good, free, and fast model.\n",
        "# It produces 384-dimensional embeddings.\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "EMBEDDING_DIMENSION = 384 # This MUST match the output dimension of EMBEDDING_MODEL_NAME\n",
        "\n",
        "# --- OPTION 2: OpenAI Embeddings (Uncomment to use, and comment out HuggingFace above) ---\n",
        "# Remember to adjust EMBEDDING_DIMENSION if you switch.\n",
        "# 'text-embedding-ada-002' (1536 dims) or 'text-embedding-3-small' (1536 dims) or 'text-embedding-3-large' (3072 dims)\n",
        "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
        "# EMBEDDING_DIMENSION = 1536\n",
        "# print(f\"Using OpenAI embedding model.\")\n",
        "# --- End of Option 2 ---\n",
        "\n",
        "print(f\"Successfully initialized Pinecone client and embedding model: {EMBEDDING_MODEL_NAME} (Dim: {EMBEDDING_DIMENSION}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-pinecone-setup",
      "metadata": {
        "id": "ssb-pinecone-setup"
      },
      "source": [
        "## 3. Pinecone Index Setup\n",
        "\n",
        "Our Pinecone index will be the dedicated, managed, and scalable environment for storing and searching our lecture note embeddings. If it doesn't already exist, we'll create a new **Serverless Index**. Serverless indexes are great because they automatically scale based on usage and offer a cost-effective solution by separating storage and compute.\n",
        "\n",
        "**Key Parameters for Index Creation:**\n",
        "-   `name`: A unique name for your index (e.g., `smart-study-buddy-notes`).\n",
        "-   `dimension`: The dimensionality of the vectors you'll store. This **must** match the output dimension of your chosen embedding model (which we set to `EMBEDDING_DIMENSION` above).\n",
        "-   `metric`: The similarity metric to use for searching. `cosine` is common for text embeddings as it measures the cosine of the angle between vectors, focusing on orientation rather than magnitude.\n",
        "-   `spec`: We use `ServerlessSpec` to specify the cloud and region for our serverless index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-create-index",
      "metadata": {
        "id": "ssb-create-index"
      },
      "outputs": [],
      "source": [
        "INDEX_NAME = \"smart-study-buddy-notes\" # Feel free to choose a unique name\n",
        "METRIC = \"cosine\"\n",
        "\n",
        "try:\n",
        "    # Check if the index already exists\n",
        "    existing_indexes = [index_info.name for index_info in pc.list_indexes()]\n",
        "    if INDEX_NAME not in existing_indexes:\n",
        "        print(f\"Index '{INDEX_NAME}' does not exist. Creating new serverless index...\")\n",
        "        pc.create_index(\n",
        "            name=INDEX_NAME,\n",
        "            dimension=EMBEDDING_DIMENSION,\n",
        "            metric=METRIC,\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=\"aws\",        # You can choose \"aws\", \"gcp\", or \"azure\"\n",
        "                region=\"us-east-1\"  # Choose a region appropriate for you\n",
        "            )\n",
        "        )\n",
        "        # Wait for the index to be ready (optional, but good practice for immediate use)\n",
        "        import time\n",
        "        while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "            print(\"Waiting for index to be ready...\")\n",
        "            time.sleep(5)\n",
        "        print(f\"Successfully created and initialized index: '{INDEX_NAME}' with dimension {EMBEDDING_DIMENSION} and metric '{METRIC}'.\")\n",
        "    else:\n",
        "        print(f\"Using existing index: '{INDEX_NAME}'\")\n",
        "\n",
        "    # Connect to the index (this is more for direct operations, LangChain will also connect)\n",
        "    pinecone_index_obj = pc.Index(INDEX_NAME)\n",
        "    print(f\"Successfully connected to index '{INDEX_NAME}'.\")\n",
        "    print(f\"Index stats: {pinecone_index_obj.describe_index_stats()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Pinecone index setup for '{INDEX_NAME}': {str(e)}\")\n",
        "    raise  # Re-raise the exception to stop execution if setup fails"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-data-ingestion",
      "metadata": {
        "id": "ssb-data-ingestion"
      },
      "source": [
        "## 4. Data Ingestion & Processing: Your Lecture Notes\n",
        "\n",
        "Now, let's prepare your actual lecture notes to be fed into our Study Buddy.\n",
        "\n",
        "**ACTION REQUIRED: Prepare Your Sample Lecture Notes!**\n",
        "1.  Create a directory named `input_lecture_notes` in the same location as this Jupyter notebook.\n",
        "2.  Add 2-3 sample lecture note files into this directory. To test our system properly, include:\n",
        "    *   At least one plain text file (e.g., `history_lecture_1.txt`).\n",
        "    *   At least one PDF file (e.g., `calculus_chapter_2.pdf`).\n",
        "    \n",
        "    **Example Content for `history_lecture_1.txt`:**\n",
        "    ```\n",
        "    History of Ancient Civilizations - Lecture 1\n",
        "    Key topics: Mesopotamia, Egypt, Indus Valley.\n",
        "    Mesopotamia is often called the cradle of civilization. The Sumerians developed cuneiform writing.\n",
        "    The Nile river was central to ancient Egyptian life, facilitating agriculture and transport.\n",
        "    Pharaohs were considered divine rulers in Egypt.\n",
        "    The Indus Valley Civilization, also known as the Harappan Civilization, had advanced urban planning.\n",
        "    Mohenjo-daro and Harappa were major cities.\n",
        "    ```\n",
        "    **Example Content for `calculus_chapter_2.pdf` (Just create a simple PDF with this text):**\n",
        "    ```\n",
        "    Calculus I - Chapter 2: Derivatives\n",
        "    The derivative of a function measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value).\n",
        "    The derivative of f(x) = x^2 is f'(x) = 2x.\n",
        "    The power rule is a common differentiation technique.\n",
        "    Limits are fundamental to understanding derivatives.\n",
        "    ```\n",
        "    **Example Content for `literature_notes.txt`:**\n",
        "    ```\n",
        "    Introduction to Shakespearean Tragedies\n",
        "    Common themes include ambition, revenge, and fate.\n",
        "    Hamlet is a famous tragedy focusing on Prince Hamlet's quest for revenge.\n",
        "    Key characters in Hamlet: Ophelia, Claudius, Gertrude.\n",
        "    Macbeth explores the corrupting influence of unchecked ambition.\n",
        "    Lady Macbeth is a powerful and manipulative character.\n",
        "    ```\n",
        "\n",
        "**The Process We'll Follow:**\n",
        "1.  **Load Documents**: We'll iterate through files in the `input_lecture_notes` directory. We'll use LangChain's `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files to extract the text content.\n",
        "2.  **Split Documents into Chunks**: Large documents often exceed the context window limits of LLMs. More importantly for RAG, smaller, focused chunks lead to more precise retrieval of relevant information. We'll use `RecursiveCharacterTextSplitter`, which tries to split text based on sensible separators (like newlines, sentences) first to keep related information together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-load-process-docs",
      "metadata": {
        "id": "ssb-load-process-docs"
      },
      "outputs": [],
      "source": [
        "NOTES_DIR = \"./input_lecture_notes\"\n",
        "\n",
        "# Create directory if it doesn't exist (and inform the user)\n",
        "if not os.path.exists(NOTES_DIR):\n",
        "    os.makedirs(NOTES_DIR)\n",
        "    print(f\"Created input directory: {NOTES_DIR}. Please add your sample lecture note files (.txt, .pdf) to this folder.\")\n",
        "elif not os.listdir(NOTES_DIR):\n",
        "    print(f\"Input directory {NOTES_DIR} is empty. Please add your sample lecture notes to this folder for the Study Buddy to work.\")\n",
        "\n",
        "all_documents = []\n",
        "if os.path.exists(NOTES_DIR) and os.listdir(NOTES_DIR):\n",
        "    print(f\"Loading documents from {NOTES_DIR}...\")\n",
        "    for filename in os.listdir(NOTES_DIR):\n",
        "        filepath = os.path.join(NOTES_DIR, filename)\n",
        "        try:\n",
        "            if filename.endswith(\".pdf\"):\n",
        "                loader = PyPDFLoader(filepath)\n",
        "                loaded_docs = loader.load()\n",
        "                print(f\"- Loaded PDF: {filename} ({len(loaded_docs)} page(s))\")\n",
        "                all_documents.extend(loaded_docs)\n",
        "            elif filename.endswith(\".txt\"):\n",
        "                loader = TextLoader(filepath, encoding='utf-8') # Specify encoding for robustness\n",
        "                loaded_docs = loader.load()\n",
        "                print(f\"- Loaded TXT: {filename} ({len(loaded_docs)} document(s))\")\n",
        "                all_documents.extend(loaded_docs)\n",
        "            else:\n",
        "                print(f\"- Skipping unsupported file type: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {filename}: {e}\")\n",
        "\n",
        "    if all_documents:\n",
        "        print(f\"\\nSuccessfully loaded content from {len(all_documents)} source(s) (pages/files).\")\n",
        "        # print(f\"First document content preview (first 200 chars): {all_documents[0].page_content[:200]}\")\n",
        "        # print(f\"First document metadata: {all_documents[0].metadata}\")\n",
        "    else:\n",
        "        print(\"No processable documents were found or loaded. Please check the NOTES_DIR and file types.\")\n",
        "else:\n",
        "    print(f\"Input directory '{NOTES_DIR}' is missing or empty. Skipping document loading.\")\n",
        "\n",
        "# Initialize text splitter\n",
        "# RecursiveCharacterTextSplitter tries to split based on a list of characters (e.g., \"\\n\\n\", \"\\n\", \" \", \"\").\n",
        "# chunk_size: The maximum size of a chunk (in characters, by default).\n",
        "# chunk_overlap: The number of characters to overlap between chunks to maintain context.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Max characters per chunk\n",
        "    chunk_overlap=200,    # Characters of overlap between chunks\n",
        "    length_function=len   # How to measure chunk size (using len() for characters)\n",
        ")\n",
        "\n",
        "# Split documents into chunks\n",
        "chunks = []\n",
        "if all_documents:\n",
        "    print(f\"\\nSplitting {len(all_documents)} document(s)/page(s) into chunks...\")\n",
        "    chunks = text_splitter.split_documents(all_documents)\n",
        "    print(f\"Successfully created {len(chunks)} chunks from the documents.\")\n",
        "    if chunks:\n",
        "        print(f\"First chunk content preview (first 100 chars): '{chunks[0].page_content[:100]}...'\" )\n",
        "        print(f\"First chunk metadata: {chunks[0].metadata}\")\n",
        "else:\n",
        "    print(\"No documents were loaded, so no chunks to create.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-embedding-vectorstore",
      "metadata": {
        "id": "ssb-embedding-vectorstore"
      },
      "source": [
        "## 5. Embedding & Vector Store: Building the Knowledge Base in Pinecone\n",
        "\n",
        "With our lecture notes loaded and thoughtfully chunked, the next critical step is to convert these text chunks into **embeddings**. Recall that embeddings are numerical vector representations that capture the semantic meaning of the text. Texts with similar meanings will have embeddings that are close together in the vector space.\n",
        "\n",
        "We will then store these embeddings in our Pinecone index (`smart-study-buddy-notes`). LangChain's `PineconeVectorStore.from_documents` (which we imported as `LangChainPinecone`) simplifies this process significantly. It takes our list of `chunks`, uses the `embeddings` model we initialized earlier (e.g., `HuggingFaceEmbeddings`), generates the embeddings, and then uploads (upserts) them into our specified Pinecone index.\n",
        "\n",
        "**This is the step where we populate Pinecone, making your notes searchable for the RAG system!** If you re-run this notebook and your notes haven't changed, you might see Pinecone report that vectors are already present from a previous run. Pinecone stores this data persistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-embed-store-pinecone",
      "metadata": {
        "id": "ssb-embed-store-pinecone"
      },
      "outputs": [],
      "source": [
        "vectorstore = None # Initialize to None, will be our LangChainPinecone vector store object\n",
        "\n",
        "if chunks:\n",
        "    print(f\"\\nGenerating embeddings for {len(chunks)} chunks and storing them in Pinecone index '{INDEX_NAME}'...\")\n",
        "    # This might take some time depending on the number of chunks and your internet connection.\n",
        "    try:\n",
        "        # This command does the following:\n",
        "        # 1. Takes each Document chunk.\n",
        "        # 2. Uses the `embeddings` model (HuggingFaceEmbeddings) to create a vector for it.\n",
        "        # 3. Upserts (uploads/inserts) the vector and its associated text/metadata to the Pinecone index.\n",
        "        vectorstore = LangChainPinecone.from_documents(\n",
        "            documents=chunks,          # The LangChain Document chunks\n",
        "            embedding=embeddings,      # The initialized HuggingFace embedding function\n",
        "            index_name=INDEX_NAME      # The name of your Pinecone index\n",
        "        )\n",
        "        print(f\"Successfully stored embeddings in Pinecone index '{INDEX_NAME}'.\")\n",
        "\n",
        "        # It might take a few moments for Pinecone's stats to update after upserting.\n",
        "        import time\n",
        "        time.sleep(10) # Give Pinecone a moment to update stats\n",
        "        stats = pc.Index(INDEX_NAME).describe_index_stats()\n",
        "        print(f\"\\nUpdated Index Statistics for '{INDEX_NAME}':\")\n",
        "        print(f\"Total vectors: {stats.total_vector_count}\")\n",
        "        print(f\"Namespaces: {stats.namespaces}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing embeddings in Pinecone: {e}\")\n",
        "        # If 'vectorstore' couldn't be created from new documents,\n",
        "        # try to connect to an existing index if it might already have data.\n",
        "        print(f\"Attempting to connect to existing index '{INDEX_NAME}' for retriever initialization.\")\n",
        "        try:\n",
        "            vectorstore = LangChainPinecone.from_existing_index(index_name=INDEX_NAME, embedding=embeddings)\n",
        "            print(\"Successfully connected to existing index for retriever.\")\n",
        "            stats_existing = pc.Index(INDEX_NAME).describe_index_stats()\n",
        "            if stats_existing.total_vector_count > 0:\n",
        "                print(f\"Existing index contains {stats_existing.total_vector_count} vectors.\")\n",
        "            else:\n",
        "                print(\"Warning: Connected to existing index, but it appears to be empty. Q&A might not work well.\")\n",
        "        except Exception as e_existing:\n",
        "            print(f\"Could not connect to or use existing index '{INDEX_NAME}': {e_existing}\")\n",
        "            vectorstore = None # Ensure it's None if connection failed\n",
        "else:\n",
        "    print(\"\\nNo chunks were created from documents. Trying to connect to an existing Pinecone index for Q&A...\")\n",
        "    # If no new chunks, we must rely on an existing index that might have data from previous runs.\n",
        "    try:\n",
        "        vectorstore = LangChainPinecone.from_existing_index(index_name=INDEX_NAME, embedding=embeddings)\n",
        "        print(f\"Successfully connected to existing index '{INDEX_NAME}'.\")\n",
        "        stats = pc.Index(INDEX_NAME).describe_index_stats()\n",
        "        if stats.total_vector_count == 0:\n",
        "            print(f\"Warning: Index '{INDEX_NAME}' exists but is empty. The Study Buddy won't have any notes to refer to.\")\n",
        "        else:\n",
        "            print(f\"Index contains {stats.total_vector_count} vectors from previous uploads.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not connect to existing index '{INDEX_NAME}'. The Study Buddy will not function without a populated vector store: {e}\")\n",
        "        vectorstore = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-qa-chain",
      "metadata": {
        "id": "ssb-qa-chain"
      },
      "source": [
        "## 6. Building the \"Brain\": Question Answering Chain with Session Memory\n",
        "\n",
        "This is where the magic happens! We'll construct the core logic of our Smart Study Buddy using LangChain's `ConversationalRetrievalChain`. This chain is specifically designed for Q&A over documents while maintaining a conversation history.\n",
        "\n",
        "Here's what it does:\n",
        "1.  **Takes User's Question**: You type in a question.\n",
        "2.  **Considers Chat History**: It looks at the past turns in the conversation (if any) to understand the context. It might even rephrase your current question based on the history to make it a better standalone query for retrieval.\n",
        "3.  **Retrieves Relevant Documents**: It uses the `vectorstore` (our Pinecone index) as a `retriever` to find the document chunks most similar (relevant) to your (possibly rephrased) question.\n",
        "4.  **Constructs a Prompt**: It takes your original question, the retrieved document chunks (as context), and feeds them to an LLM.\n",
        "5.  **Generates an Answer**: The LLM (e.g., `ChatOpenAI` with a GPT model) generates a response based on the question and the provided context from your notes.\n",
        "6.  **Updates Memory**: The new question and answer are added to the `ConversationBufferMemory` for future turns.\n",
        "\n",
        "### Customizing the LLM's Behavior with a Prompt Template\n",
        "To ensure our Study Buddy behaves as intended (answers only from notes, indicates if info is missing), we'll use a `PromptTemplate`. This template will instruct the LLM on how to formulate its answers.\n",
        "\n",
        "**Key Components:**\n",
        "-   **LLM**: We'll use `ChatOpenAI`. `temperature=0` makes the output more deterministic and factual.\n",
        "-   **Retriever**: Our `vectorstore` (Pinecone) will be converted into a retriever.\n",
        "-   **Memory**: `ConversationBufferMemory` will store the chat history.\n",
        "-   **Prompt Template**: To guide the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-create-qa-chain",
      "metadata": {
        "id": "ssb-create-qa-chain"
      },
      "outputs": [],
      "source": [
        "qa_chain = None\n",
        "chat_history = [] # Initialize an empty list for chat history (for the chain's memory)\n",
        "\n",
        "if vectorstore:\n",
        "    # Initialize the LLM\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,  # Lower temperature for more factual, less creative responses\n",
        "        model=\"gpt-3.5-turbo\", # A good balance of capability and cost. Or use \"gpt-4-turbo-preview\" or \"gpt-4\"\n",
        "        openai_api_key=OPENAI_API_KEY\n",
        "    )\n",
        "    print(f\"LLM initialized using model: {llm.model_name}\")\n",
        "\n",
        "    # Create a retriever from our Pinecone vector store\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\", # Other options: \"mmr\" (Maximal Marginal Relevance)\n",
        "        search_kwargs={\"k\": 3}    # Retrieve top 3 most relevant chunks\n",
        "    )\n",
        "    print(f\"Retriever created from Pinecone index '{INDEX_NAME}', retrieving top 3 chunks.\")\n",
        "\n",
        "    # Define the prompt template\n",
        "    # This tells the LLM how to behave. It should use the provided context (from your notes)\n",
        "    # and the chat history to answer the question.\n",
        "    prompt_template_str = \"\"\"\n",
        "    You are a helpful AI Smart Study Buddy. Use the following pieces of context from lecture notes and the chat history to answer the question at the end.\n",
        "    Your goal is to answer the user's question based *only* on the provided lecture notes context.\n",
        "    Do not use any external knowledge or make up information.\n",
        "    If the answer to the question cannot be found in the provided context, clearly state \"I'm sorry, but I couldn't find information about that in your lecture notes.\"\n",
        "    If the context is empty or irrelevant to the question, also state that you cannot find the answer in the notes.\n",
        "\n",
        "    Context from lecture notes:\n",
        "    {context}\n",
        "\n",
        "    Chat History:\n",
        "    {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "    Helpful Answer from your lecture notes:\n",
        "    \"\"\"\n",
        "    QA_PROMPT = PromptTemplate(template=prompt_template_str, input_variables=[\"context\", \"chat_history\", \"question\"])\n",
        "    print(\"Prompt template defined.\")\n",
        "\n",
        "    # Initialize conversation memory\n",
        "    # `return_messages=True` ensures the memory object returns messages in the format expected by the chain.\n",
        "    # `memory_key='chat_history'` is the default and matches the input variable in ConversationalRetrievalChain.\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history',\n",
        "        return_messages=True,\n",
        "        output_key='answer' # Ensure the LLM's answer is stored correctly in memory for the next turn\n",
        "    )\n",
        "    print(\"Conversation memory initialized.\")\n",
        "\n",
        "    # Create the Conversational Retrieval Chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True,  # To see which chunks were retrieved\n",
        "        combine_docs_chain_kwargs={\"prompt\": QA_PROMPT}, # Pass our custom prompt\n",
        "        verbose=False # Set to True for more detailed logging from the chain\n",
        "    )\n",
        "    print(\"Conversational Retrieval Chain created successfully.\")\n",
        "else:\n",
        "    print(\"Vectorstore (Pinecone connection) not available. Skipping Conversational Retrieval Chain setup.\")\n",
        "    print(\"Please ensure documents are loaded and embeddings are stored in Pinecone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-chat-interface",
      "metadata": {
        "id": "ssb-chat-interface"
      },
      "source": [
        "## 7. Chatting with Your Smart Study Buddy!\n",
        "\n",
        "Now for the exciting part: interacting with your Study Buddy! We'll create a simple function to send questions to our `qa_chain` and display the answers along with the source document snippets that the LLM used.\n",
        "\n",
        "**We will test the following scenarios:**\n",
        "1.  **Initial Question**: Ask a question that can be answered from one of your sample lecture notes.\n",
        "2.  **Follow-up Question**: Ask a question that relies on the context of the previous question/answer within the same session (testing the memory).\n",
        "3.  **Question Not in Documents**: Ask a question whose answer is very unlikely to be in your sample notes (testing the LLM's adherence to the prompt to only use provided context)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-chat-function-test",
      "metadata": {
        "id": "ssb-chat-function-test"
      },
      "outputs": [],
      "source": [
        "def chat_with_buddy(query: str):\n",
        "    global chat_history # Uses the global chat_history list managed by the chain's memory\n",
        "    if not qa_chain:\n",
        "        print(\"QA chain is not initialized. Cannot process query. Please check previous steps.\")\n",
        "        return \"Error: QA chain not set up.\"\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nðŸ¤” User Query: {query}\")\n",
        "\n",
        "        # Invoke the chain. It uses the `memory` object which contains `chat_history`.\n",
        "        # The `chat_history` argument to `invoke` here is for passing explicit history if not using memory's implicit handling,\n",
        "        # but with `ConversationBufferMemory`, the chain manages it internally.\n",
        "        # For `ConversationalRetrievalChain`, the input is a dict with \"question\" and \"chat_history\".\n",
        "        # The memory object automatically provides the `chat_history`.\n",
        "        result = qa_chain.invoke({\"question\": query}) # `chat_history` is implicitly handled by the memory object\n",
        "\n",
        "        answer = result[\"answer\"]\n",
        "        print(f\"ðŸ’¡ Study Buddy: {answer}\")\n",
        "\n",
        "        # The memory object updates chat_history automatically.\n",
        "        # We can inspect it if needed: print(f\"Current memory: {qa_chain.memory.buffer_as_messages}\")\n",
        "\n",
        "        print(\"\\nðŸ“š Sources Used:\")\n",
        "        if result.get(\"source_documents\"):\n",
        "            for i, doc in enumerate(result[\"source_documents\"]):\n",
        "                source_name = doc.metadata.get('source', 'Unknown source')\n",
        "                # Truncate page_content for display\n",
        "                content_preview = doc.page_content.replace('\\n', ' ').strip()[:150]\n",
        "                print(f\"  {i+1}. Source: {source_name}\\n     Content Preview: '{content_preview}...'\")\n",
        "        else:\n",
        "            print(\"  No specific source documents were heavily relied upon or returned by the retriever.\")\n",
        "\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error during chat interaction: {str(e)}\"\n",
        "        print(error_message)\n",
        "        # If there's an error, you might want to log it or display a more user-friendly message.\n",
        "        # Also, consider what to do with chat_history in case of an error.\n",
        "        # For simplicity, we'll just return the error message.\n",
        "        return error_message\n",
        "\n",
        "# --- Test Scenarios ---\n",
        "if qa_chain: # Ensure the chain is ready\n",
        "    print(\"--- Starting Chat Session with Smart Study Buddy ---\")\n",
        "\n",
        "    # 1. Initial Question (assuming you have notes about history or calculus based on sample data)\n",
        "    # Adjust this question based on the content of YOUR sample notes!\n",
        "    # Example questions based on the sample notes provided earlier:\n",
        "    # question1 = \"What was cuneiform writing developed by?\"\n",
        "    # question1 = \"What is the derivative of x^2?\"\n",
        "    question1 = \"What are common themes in Shakespearean tragedies?\"\n",
        "    response1 = chat_with_buddy(question1)\n",
        "\n",
        "    # 2. Follow-up Question (relies on the context of the previous Q/A)\n",
        "    if response1 and \"Error:\" not in response1 and \"I'm sorry\" not in response1:\n",
        "        # Adjust this follow-up based on your question1 and expected answer.\n",
        "        # question2 = \"Tell me more about Mesopotamia.\"\n",
        "        # question2 = \"What is the power rule related to?\"\n",
        "        question2 = \"Can you name a key character in Hamlet?\"\n",
        "        response2 = chat_with_buddy(question2)\n",
        "    else:\n",
        "        print(\"\\nSkipping follow-up question as the first answer was not found or an error occurred.\")\n",
        "\n",
        "    # 3. Question Not in Documents\n",
        "    question3 = \"What is the airspeed velocity of an unladen swallow?\" # Unlikely to be in lecture notes\n",
        "    response3 = chat_with_buddy(question3)\n",
        "\n",
        "    question4 = \"What is the capital of France?\" # General knowledge, should not be answered from notes.\n",
        "    response4 = chat_with_buddy(question4)\n",
        "\n",
        "    print(\"\\n--- Chat Session Ended ---\")\n",
        "    print(\"\\n--- Note on Session Memory vs. Persistent Knowledge ---\")\n",
        "    print(\"The 'chat_history' (session memory) is for the current run of this notebook.\")\n",
        "    print(\"If you restart the kernel and re-run, the Study Buddy won't remember this specific conversation.\")\n",
        "    print(\"However, the knowledge (document embeddings) stored in your Pinecone index IS persistent.\")\n",
        "    print(\"So, on a new run, it can still answer questions about your notes, but it starts a fresh conversation.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping chat example as the QA chain is not set up. Please check previous steps.\")\n",
        "    print(\"Ensure you have documents in 'input_lecture_notes', API keys are correct, and Pinecone index is accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-conclusion",
      "metadata": {
        "id": "ssb-conclusion"
      },
      "source": [
        "## 8. Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've successfully built a \"Smart Study Buddy\" â€“ a conversational RAG pipeline that uses Pinecone to query your personal lecture notes and LangChain to manage the conversation flow and LLM interaction.\n",
        "\n",
        "You've learned how to:\n",
        "1.  Set up Pinecone and initialize a serverless vector index.\n",
        "2.  Load and process local documents (`.txt`, `.pdf`) using LangChain document loaders.\n",
        "3.  Chunk documents effectively for RAG.\n",
        "4.  Generate embeddings for these chunks and store them in your Pinecone index.\n",
        "5.  Create a `ConversationalRetrievalChain` with session memory (`ConversationBufferMemory`).\n",
        "6.  Use a `PromptTemplate` to guide the LLM to answer based *only* on the provided context and to indicate if the answer isn't in the notes.\n",
        "7.  Test your RAG system with initial questions, follow-up questions, and questions designed to check its adherence to the provided context.\n",
        "\n",
        "### Possible Enhancements and Further Exploration:\n",
        "-   **More Document Types**: Extend to support `.docx`, `.pptx`, etc., using appropriate LangChain loaders (often via `UnstructuredFileLoader`).\n",
        "-   **Different Embedding Models/LLMs**: Experiment with other embedding models (e.g., OpenAI's `text-embedding-3-small`) or different LLMs (`gpt-4-turbo-preview`, models from Hugging Face Hub) to see their impact. Remember to adjust `EMBEDDING_DIMENSION` if your embedding model changes!\n",
        "-   **Advanced Retrieval**: Explore techniques like Maximal Marginal Relevance (MMR) search in the retriever, or re-ranking retrieved documents before sending them to the LLM.\n",
        "-   **Metadata Filtering**: Add metadata to your document chunks during ingestion (e.g., subject, lecture number, date) and modify the retriever to filter based on this metadata for more targeted searches (e.g., \"What did we learn about photosynthesis in Biology lecture 3?\").\n",
        "-   **More Sophisticated Memory**: For very long conversations, explore `ConversationSummaryMemory` or `ConversationSummaryBufferMemory` which summarize older parts of the conversation to save tokens.\n",
        "-   **User Interface**: Build a web interface using Streamlit or Gradio to make your Study Buddy more accessible.\n",
        "-   **Evaluation**: Implement metrics to evaluate the quality of retrieval (e.g., hit rate) and generation (e.g., RAGAs, faithfulness, relevance)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssb-cleanup",
      "metadata": {
        "id": "ssb-cleanup"
      },
      "source": [
        "## 9. Important: Clean Up Pinecone Resources\n",
        "\n",
        "Pinecone serverless indexes incur costs based on storage and usage. If you are done with this demo and don't plan to use the index further, remember to **delete your Pinecone index** to avoid ongoing charges.\n",
        "\n",
        "You can do this via the Pinecone console (go to [app.pinecone.io](https://app.pinecone.io/), find your index, and delete it) or programmatically by uncommenting and running the cell below. **Make sure `INDEX_NAME` matches the index you want to delete!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssb-delete-index",
      "metadata": {
        "id": "ssb-delete-index"
      },
      "outputs": [],
      "source": [
        "# # Ensure 'pc' (Pinecone client) and 'INDEX_NAME' are correctly defined from earlier cells.\n",
        "# try:\n",
        "#     if INDEX_NAME in [index_info.name for index_info in pc.list_indexes()]:\n",
        "#         print(f\"Deleting index '{INDEX_NAME}'...\")\n",
        "#         pc.delete_index(INDEX_NAME)\n",
        "#         print(f\"Index '{INDEX_NAME}' deleted successfully.\")\n",
        "#     else:\n",
        "#         print(f\"Index '{INDEX_NAME}' not found. No deletion needed or wrong index name specified.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error deleting index '{INDEX_NAME}': {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}