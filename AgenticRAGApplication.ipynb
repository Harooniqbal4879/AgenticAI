{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEJlQFBoScortv1teNDYSn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harooniqbal4879/AgenticAI/blob/main/AgenticRAGApplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import csv\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "AVca5sAI4sfl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31edf260"
      },
      "source": [
        "%pip install --upgrade --quiet  langchain-openai"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ebd4f36"
      },
      "source": [
        "%pip install --upgrade --quiet  langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "wEUlhsGt4uqc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform necessary imports for creating agents\n",
        "from langchain import hub # Used to pull predefined prompts from LangChain Hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory # Used to store chat history in memory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "-pv6e5BJ5yfH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set the OpenAI API key and model name\n",
        "Open_API_Key = os.getenv(\"Open_API_Key\")\n",
        "if Open_API_Key is None:\n",
        "    print(\"Error: Open_API_Key environment variable not set.\")\n",
        "    print(\"Please set the Open_API_Key environment variable with your OpenAI API key.\")\n",
        "else:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = Open_API_Key\n",
        "    MODEL=\"gpt-4o-mini\"\n",
        "    client = OpenAI(api_key=Open_API_Key)\n",
        "\n",
        "## Set the Tavily API key\n",
        "Tavily_API_Key = os.getenv(\"Tavily_API_Key\")\n",
        "if Tavily_API_Key is None:\n",
        "    print(\"Error: Tavily_API_Key environment variable not set.\")\n",
        "    print(\"Please set the Tavily_API_Key environment variable with your Tavily API key.\")\n",
        "else:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = Tavily_API_Key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9SBekt66IH_",
        "outputId": "ffebe9e0-8d7a-49d3-ad57-b0b2a881a61d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Open_API_Key environment variable not set.\n",
            "Please set the Open_API_Key environment variable with your OpenAI API key.\n",
            "Error: Tavily_API_Key environment variable not set.\n",
            "Please set the Tavily_API_Key environment variable with your Tavily API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the vectorstore\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector = FAISS.load_local(\n",
        "    \"./faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "dE6Iw-B06NXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdVFzbFy4PEG"
      },
      "outputs": [],
      "source": [
        "## Create the conversational agent\n",
        "\n",
        "# Creating a retriever\n",
        "# See https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/\n",
        "retriever = # Create retriever from the loaded vector\n",
        "\n",
        "\n",
        "# import the 'tool' decorator\n",
        "from langchain.tools import tool\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# Define a tool for Amazon product search using @tool decorator\n",
        "@tool\n",
        "def amazon_product_search(query: str):\n",
        "    \"\"\"Search for information about Amazon products.\n",
        "    For any questions related to Amazon products, this tool must be used.\"\"\"\n",
        "\n",
        "    ## TODO create retriever_tool based on the retriever object\n",
        "\n",
        "# Import necessary classes from LangChain for Tavily integration\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "@tool\n",
        "def search_tavily(query: str):\n",
        "    \"\"\"\n",
        "    Executes a web search using the TavilySearchResults tool.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The search query entered by the user.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of search results containing answers, raw content, and images.\n",
        "    \"\"\"\n",
        "    # TODO: Create an instance of TavilySearchResults with customized parameters\n",
        "\n",
        "\n",
        "# hwchase17/react is a prompt template designed for ReAct-style\n",
        "# conversational agents.\n",
        "prompt = # pull \"hwchase17/react\" prompt from langchain hub\n",
        "\n",
        "## Create a list of tools: retriever_tool and search_tool\n",
        "tools = # TODO: Create a list of tools based on search_tavily and amazon_product_search.\n",
        "\n",
        "# Enable memory optimization with ConversationSummaryMemory\n",
        "# This ensures that older conversations are summarized instead of keeping full history,\n",
        "# preventing excessive context length that slows down responses.\n",
        "summary_memory = ## Create summary_memory using ConversationSummaryMemory()\n",
        "\n",
        "# Initialize OpenAI model with streaming enabled\n",
        "# Streaming allows tokens to be processed in real-time, reducing response latency.\n",
        "summary_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0, streaming=True)\n",
        "\n",
        "# Create a ReAct agent\n",
        "# The agent will reason and take actions based on retrieved tools and memory.\n",
        "summary_react_agent = ## TODO create a react agent. Check create_react_agent() from langchain\n",
        "\n",
        "# Configure the AgentExecutor to manage reasoning steps\n",
        "summary_agent_executor = # Create an agent executor. Check AgentExecutor() from langchain. Make sure to pass the summary_memory to it\n",
        "\n",
        "\n",
        "\n",
        "# Building an UI for the chatbot with agents\n",
        "import gradio as gr\n",
        "\n",
        "# Initialize session-based chat history\n",
        "session_memory = {}\n",
        "\n",
        "def get_memory(session_id):\n",
        "    \"\"\"Fetch or create a chat history instance for a given session.\"\"\"\n",
        "    # Create a new element in the dictionary that corresponds to the session memory if it does not exit\n",
        "\n",
        "# Wrap agent with session-based chat history\n",
        "agent_with_chat_history = # Create agent with memory using RunnableWithMessageHistory()\n",
        "\n",
        "# Define function for Gradio interface\n",
        "def chat_with_agent(user_input, session_id):\n",
        "    \"\"\"Processes user input and maintains session-based chat history.\"\"\"\n",
        "    memory = get_memory(session_id)  # Fetch chat history for the session\n",
        "    #memory.clear()\n",
        "    # Invoke the agent with session memory\n",
        "    response = agent_with_chat_history.invoke(\n",
        "        {\"input\": user_input, \"chat_history\": memory.messages},\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "\n",
        "    # Extract only the 'output' field from the response\n",
        "    if isinstance(response, dict) and \"output\" in response:\n",
        "        return response[\"output\"]  # Return clean text response\n",
        "    else:\n",
        "        return \"Error: Unexpected response format\"\n",
        "\n",
        "# Create Gradio app interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# ðŸ¤– Review Genie - Agents & ReAct Framework\")\n",
        "    gr.Markdown(\"Enter your query below and get AI-powered responses with session memory.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        input_box = gr.Textbox(label=\"Enter your query:\", placeholder=\"Ask something...\")\n",
        "        output_box = gr.Textbox(label=\"Response:\", lines=10)\n",
        "\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    submit_button.click(chat_with_agent, inputs=input_box, outputs=output_box)\n",
        "\n",
        "# Launch the Gradio app\n",
        "app.launch(debug=True, share=True)"
      ]
    }
  ]
}