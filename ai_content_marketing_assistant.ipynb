{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiHB/0eSW68IkdMKhpntSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harooniqbal4879/AgenticAI/blob/main/ai_content_marketing_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebd263b5",
        "outputId": "95afa949-1f2f-41eb-97e2-0a51719b93f9"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "XbILBYpOieud",
        "outputId": "ba1fbcea-7933-42d7-a94e-88fefb1b243b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-281231336.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "from typing import Dict, Any, List, Optional\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "import json\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "import time\n",
        "from datetime import datetime\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Core Dependencies for Multi-Agent System\n",
        "try:\n",
        "    from langgraph.graph import StateGraph, END\n",
        "    from langgraph.graph.state import CompiledStateGraph\n",
        "    from typing_extensions import TypedDict\n",
        "    print(\"LangGraph imported successfully\")\n",
        "except ImportError:\n",
        "    # Fallback implementation if LangGraph not available\n",
        "    print(\"LangGraph not available - using fallback implementation\")\n",
        "    class TypedDict(dict):\n",
        "        pass\n",
        "    class StateGraph:\n",
        "        def __init__(self, state_schema):\n",
        "            self.nodes = {}\n",
        "            self.edges = {}\n",
        "            self.state_schema = state_schema\n",
        "        def add_node(self, name, func):\n",
        "            self.nodes[name] = func\n",
        "        def add_edge(self, from_node, to_node):\n",
        "            self.edges[from_node] = to_node\n",
        "        def compile(self):\n",
        "            return CompiledStateGraph(self.nodes, self.edges)\n",
        "    class CompiledStateGraph:\n",
        "        def __init__(self, nodes, edges):\n",
        "            self.nodes = nodes\n",
        "            self.edges = edges\n",
        "        def invoke(self, state):\n",
        "            # Simple sequential execution for fallback\n",
        "            current = \"query_handler\"\n",
        "            while current and current != \"END\":\n",
        "                if current in self.nodes:\n",
        "                    state = self.nodes[current](state)\n",
        "                    current = self.edges.get(current, \"END\")\n",
        "            return state\n",
        "\n",
        "# Configuration Management\n",
        "@dataclass\n",
        "class Config:\n",
        "    openai_api_key: str\n",
        "    serp_api_key: Optional[str] = None\n",
        "    max_tokens: int = 2000\n",
        "    temperature: float = 0.7\n",
        "    model: str = \"gpt-4\"\n",
        "\n",
        "class ContentType(Enum):\n",
        "    RESEARCH = \"research\"\n",
        "    BLOG = \"blog\"\n",
        "    LINKEDIN = \"linkedin\"\n",
        "    IMAGE = \"image\"\n",
        "    STRATEGY = \"strategy\"\n",
        "\n",
        "# State Management for Multi-Agent System\n",
        "class WorkflowState(TypedDict):\n",
        "    user_query: str\n",
        "    content_type: str\n",
        "    research_data: Dict[str, Any]\n",
        "    generated_content: Dict[str, str]\n",
        "    images: List[str]\n",
        "    metadata: Dict[str, Any]\n",
        "    conversation_history: List[Dict[str, str]]\n",
        "    error_log: List[str]\n",
        "\n",
        "# Agent Base Class\n",
        "class BaseAgent:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.client = OpenAI(api_key=config.openai_api_key)\n",
        "\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# 1. Query Handler Agent\n",
        "class QueryHandlerAgent(BaseAgent):\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Routes requests to appropriate specialized agents\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            Analyze the following user query and determine the primary content type needed.\n",
        "            Respond with ONLY one of: research, blog, linkedin, image, strategy\n",
        "\n",
        "            Query: {state['user_query']}\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=50,\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content_type = response.choices[0].message.content.strip().lower()\n",
        "            state['content_type'] = content_type\n",
        "            state['metadata']['routing_decision'] = content_type\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error_log'].append(f\"Query Handler Error: {str(e)}\")\n",
        "            state['content_type'] = 'research'  # Default fallback\n",
        "\n",
        "        return state\n",
        "\n",
        "# 2. Deep Research Agent\n",
        "class DeepResearchAgent(BaseAgent):\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Conducts comprehensive web research and analysis\"\"\"\n",
        "        try:\n",
        "            # Simulate web research (in production, use SERP API)\n",
        "            search_query = self._extract_search_terms(state['user_query'])\n",
        "\n",
        "            # Generate research-based content using LLM\n",
        "            research_prompt = f\"\"\"\n",
        "            Conduct comprehensive research analysis for: {state['user_query']}\n",
        "\n",
        "            Provide:\n",
        "            1. Key findings and insights\n",
        "            2. Current trends and statistics\n",
        "            3. Expert opinions and quotes\n",
        "            4. Relevant case studies\n",
        "            5. Source recommendations\n",
        "\n",
        "            Format as structured research report.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": research_prompt}],\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                temperature=self.config.temperature\n",
        "            )\n",
        "\n",
        "            research_content = response.choices[0].message.content\n",
        "            state['research_data'] = {\n",
        "                'content': research_content,\n",
        "                'search_terms': search_query,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'sources': self._generate_mock_sources(search_query)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error_log'].append(f\"Research Agent Error: {str(e)}\")\n",
        "            state['research_data'] = {'content': 'Research unavailable', 'error': str(e)}\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _extract_search_terms(self, query: str) -> List[str]:\n",
        "        # Simple keyword extraction\n",
        "        words = re.findall(r'\\b\\w+\\b', query.lower())\n",
        "        return [word for word in words if len(word) > 3]\n",
        "\n",
        "    def _generate_mock_sources(self, terms: List[str]) -> List[str]:\n",
        "        return [\n",
        "            f\"https://example-research.com/{term}-study-2024\"\n",
        "            for term in terms[:3]\n",
        "        ]\n",
        "\n",
        "# 3. SEO Blog Writer Agent\n",
        "class SEOBlogWriterAgent(BaseAgent):\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Creates search-optimized long-form content\"\"\"\n",
        "        try:\n",
        "            research_context = state.get('research_data', {}).get('content', '')\n",
        "\n",
        "            blog_prompt = f\"\"\"\n",
        "            Write a comprehensive, SEO-optimized blog post based on:\n",
        "            Query: {state['user_query']}\n",
        "            Research Context: {research_context[:1000]}...\n",
        "\n",
        "            Requirements:\n",
        "            - 1500+ words\n",
        "            - Clear H1, H2, H3 structure\n",
        "            - Meta description\n",
        "            - Focus keywords\n",
        "            - Internal linking suggestions\n",
        "            - Call-to-action\n",
        "            - Engaging introduction and conclusion\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": blog_prompt}],\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                temperature=self.config.temperature\n",
        "            )\n",
        "\n",
        "            blog_content = response.choices[0].message.content\n",
        "            state['generated_content']['blog'] = blog_content\n",
        "            state['generated_content']['blog_metadata'] = {\n",
        "                'word_count': len(blog_content.split()),\n",
        "                'estimated_read_time': len(blog_content.split()) // 200,\n",
        "                'seo_keywords': self._extract_keywords(state['user_query'])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error_log'].append(f\"Blog Writer Error: {str(e)}\")\n",
        "            state['generated_content']['blog'] = f\"Blog generation failed: {str(e)}\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _extract_keywords(self, query: str) -> List[str]:\n",
        "        return re.findall(r'\\b\\w+\\b', query.lower())[:5]\n",
        "\n",
        "# 4. LinkedIn Post Writer Agent\n",
        "class LinkedInPostWriterAgent(BaseAgent):\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Generates engaging professional social content\"\"\"\n",
        "        try:\n",
        "            research_context = state.get('research_data', {}).get('content', '')\n",
        "\n",
        "            linkedin_prompt = f\"\"\"\n",
        "            Create an engaging LinkedIn post based on:\n",
        "            Query: {state['user_query']}\n",
        "            Research: {research_context[:500]}...\n",
        "\n",
        "            Requirements:\n",
        "            - Professional tone\n",
        "            - Hook in first line\n",
        "            - 3-5 bullet points or key insights\n",
        "            - Call-to-action\n",
        "            - 5-8 relevant hashtags\n",
        "            - Emoji usage for engagement\n",
        "            - 150-300 words optimal length\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": linkedin_prompt}],\n",
        "                max_tokens=800,\n",
        "                temperature=self.config.temperature\n",
        "            )\n",
        "\n",
        "            linkedin_content = response.choices[0].message.content\n",
        "            state['generated_content']['linkedin'] = linkedin_content\n",
        "            state['generated_content']['linkedin_metadata'] = {\n",
        "                'character_count': len(linkedin_content),\n",
        "                'hashtags': self._extract_hashtags(linkedin_content),\n",
        "                'engagement_score': self._calculate_engagement_score(linkedin_content)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error_log'].append(f\"LinkedIn Writer Error: {str(e)}\")\n",
        "            state['generated_content']['linkedin'] = f\"LinkedIn post generation failed: {str(e)}\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _extract_hashtags(self, content: str) -> List[str]:\n",
        "        return re.findall(r'#\\w+', content)\n",
        "\n",
        "    def _calculate_engagement_score(self, content: str) -> int:\n",
        "        # Simple engagement scoring\n",
        "        score = 0\n",
        "        score += len(re.findall(r'[?!]', content)) * 2  # Questions and exclamations\n",
        "        score += len(self._extract_hashtags(content))   # Hashtags\n",
        "        score += len(re.findall(r'[📊📈💡🚀🎯]', content)) * 3  # Engagement emojis\n",
        "        return min(score, 100)\n",
        "\n",
        "# 5. Image Generation Agent\n",
        "class ImageGenerationAgent(BaseAgent):\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Produces custom visuals with prompt optimization\"\"\"\n",
        "        try:\n",
        "            # Optimize image prompt based on content\n",
        "            image_prompt = self._optimize_image_prompt(state['user_query'])\n",
        "\n",
        "            # For demo purposes, generate image descriptions instead of actual images\n",
        "            # In production, you'd use DALL-E 3 API\n",
        "            image_description = f\"\"\"\n",
        "            Image Concept: {image_prompt}\n",
        "            Style: Professional, modern, high-quality\n",
        "            Format: 16:9 landscape for blog headers, 1:1 square for social media\n",
        "            Elements: Clean typography, brand-appropriate colors, engaging visuals\n",
        "            \"\"\"\n",
        "\n",
        "            state['images'] = [image_description]\n",
        "            state['generated_content']['image_prompts'] = [image_prompt]\n",
        "\n",
        "            # If OpenAI API key is available and DALL-E is accessible, generate actual image\n",
        "            try:\n",
        "                if hasattr(self.client, 'images'):\n",
        "                    image_response = self.client.images.generate(\n",
        "                        model=\"dall-e-3\",\n",
        "                        prompt=image_prompt,\n",
        "                        size=\"1024x1024\",\n",
        "                        quality=\"standard\",\n",
        "                        n=1\n",
        "                    )\n",
        "                    state['images'].append(image_response.data[0].url)\n",
        "            except Exception:\n",
        "                pass  # Continue with description only\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error_log'].append(f\"Image Generation Error: {str(e)}\")\n",
        "            state['images'] = [f\"Image generation failed: {str(e)}\"]\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _optimize_image_prompt(self, query: str) -> str:\n",
        "        prompt_optimization = f\"\"\"\n",
        "        Create a DALL-E optimized prompt for: {query}\n",
        "\n",
        "        Make it:\n",
        "        - Visually descriptive\n",
        "        - Professional and modern\n",
        "        - Suitable for content marketing\n",
        "        - High quality and engaging\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_optimization}],\n",
        "                max_tokens=150,\n",
        "                temperature=0.8\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except:\n",
        "            return f\"Professional illustration about {query}, modern design, high quality\"\n",
        "\n",
        "# 6. Content Strategist Agent\n",
        "class ContentStrategistAgent(BaseAgent):\n",
        "    def execute(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Formats and organizes research into readable content\"\"\"\n",
        "        try:\n",
        "            # Compile all generated content into a strategic summary\n",
        "            strategy_prompt = f\"\"\"\n",
        "            As a content strategist, analyze and provide strategic recommendations for:\n",
        "\n",
        "            Original Query: {state['user_query']}\n",
        "            Content Type: {state['content_type']}\n",
        "\n",
        "            Generated Content Summary:\n",
        "            - Blog: {len(state['generated_content'].get('blog', ''))} characters\n",
        "            - LinkedIn: {len(state['generated_content'].get('linkedin', ''))} characters\n",
        "            - Images: {len(state['images'])} generated\n",
        "\n",
        "            Provide:\n",
        "            1. Content performance predictions\n",
        "            2. Distribution strategy\n",
        "            3. Optimization recommendations\n",
        "            4. Next content suggestions\n",
        "            5. KPI tracking recommendations\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": strategy_prompt}],\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                temperature=self.config.temperature\n",
        "            )\n",
        "\n",
        "            strategy_content = response.choices[0].message.content\n",
        "            state['generated_content']['strategy'] = strategy_content\n",
        "\n",
        "            # Generate content quality score\n",
        "            state['metadata']['quality_score'] = self._calculate_quality_score(state)\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error_log'].append(f\"Content Strategist Error: {str(e)}\")\n",
        "            state['generated_content']['strategy'] = f\"Strategy generation failed: {str(e)}\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _calculate_quality_score(self, state: WorkflowState) -> int:\n",
        "        score = 50  # Base score\n",
        "\n",
        "        # Add points for successful content generation\n",
        "        if state['generated_content'].get('blog'):\n",
        "            score += 15\n",
        "        if state['generated_content'].get('linkedin'):\n",
        "            score += 15\n",
        "        if state['research_data'].get('content'):\n",
        "            score += 10\n",
        "        if state['images']:\n",
        "            score += 10\n",
        "\n",
        "        # Deduct points for errors\n",
        "        score -= len(state['error_log']) * 5\n",
        "\n",
        "        return max(0, min(100, score))\n",
        "\n",
        "# Multi-Agent Orchestrator\n",
        "class ContentMarketingOrchestrator:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.agents = {\n",
        "            'query_handler': QueryHandlerAgent(config),\n",
        "            'research': DeepResearchAgent(config),\n",
        "            'blog_writer': SEOBlogWriterAgent(config),\n",
        "            'linkedin_writer': LinkedInPostWriterAgent(config),\n",
        "            'image_generator': ImageGenerationAgent(config),\n",
        "            'content_strategist': ContentStrategistAgent(config)\n",
        "        }\n",
        "        self.workflow = self._build_workflow()\n",
        "\n",
        "    def _build_workflow(self):\n",
        "        \"\"\"Build LangGraph workflow\"\"\"\n",
        "        workflow = StateGraph(WorkflowState)\n",
        "\n",
        "        # Add all agent nodes\n",
        "        workflow.add_node(\"query_handler\", self.agents['query_handler'].execute)\n",
        "        workflow.add_node(\"research\", self.agents['research'].execute)\n",
        "        workflow.add_node(\"blog_writer\", self.agents['blog_writer'].execute)\n",
        "        workflow.add_node(\"linkedin_writer\", self.agents['linkedin_writer'].execute)\n",
        "        workflow.add_node(\"image_generator\", self.agents['image_generator'].execute)\n",
        "        workflow.add_node(\"content_strategist\", self.agents['content_strategist'].execute)\n",
        "\n",
        "        # Define workflow edges based on content type routing\n",
        "        workflow.add_edge(\"query_handler\", \"research\")\n",
        "        workflow.add_edge(\"research\", \"blog_writer\")\n",
        "        workflow.add_edge(\"blog_writer\", \"linkedin_writer\")\n",
        "        workflow.add_edge(\"linkedin_writer\", \"image_generator\")\n",
        "        workflow.add_edge(\"image_generator\", \"content_strategist\")\n",
        "        workflow.add_edge(\"content_strategist\", END)\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def process_request(self, user_query: str, conversation_history: List = None) -> Dict[str, Any]:\n",
        "        \"\"\"Process user request through multi-agent workflow\"\"\"\n",
        "        initial_state: WorkflowState = {\n",
        "            'user_query': user_query,\n",
        "            'content_type': '',\n",
        "            'research_data': {},\n",
        "            'generated_content': {},\n",
        "            'images': [],\n",
        "            'metadata': {'start_time': datetime.now().isoformat()},\n",
        "            'conversation_history': conversation_history or [],\n",
        "            'error_log': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            final_state = self.workflow.invoke(initial_state)\n",
        "            final_state['metadata']['end_time'] = datetime.now().isoformat()\n",
        "            final_state['metadata']['success'] = len(final_state['error_log']) == 0\n",
        "\n",
        "            return final_state\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                **initial_state,\n",
        "                'error_log': [f\"Workflow Error: {str(e)}\"],\n",
        "                'metadata': {\n",
        "                    'end_time': datetime.now().isoformat(),\n",
        "                    'success': False,\n",
        "                    'error': str(e)\n",
        "                }\n",
        "            }\n",
        "\n"
      ]
    }
  ]
}