{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harooniqbal4879/AgenticAI/blob/main/Agentic_RAG_Starter_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95395c4",
      "metadata": {
        "id": "c95395c4"
      },
      "source": [
        "# ü§ñ Agentic RAG Assistant (LangChain + OpenAI + FAISS)\n",
        "\n",
        "This Colab notebook demonstrates a starter project for an **Agentic AI-powered Retrieval-Augmented Generation (RAG)** assistant using:\n",
        "- LangChain\n",
        "- OpenAI GPT-3.5/4\n",
        "- FAISS (Vector Store)\n",
        "- PDF Document Loader & Text Chunker\n",
        "\n",
        "*Generated on June 18, 2025*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9fa386",
      "metadata": {
        "id": "1b9fa386"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üì¶ Install Required Packages\n",
        "!pip install -U langchain-community openai faiss-cpu tiktoken pypdf requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff447f20",
      "metadata": {
        "id": "ff447f20"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üîë Set Your OpenAI API Key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"sk-...\"  # Replace with your key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "74ace2cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ace2cf",
        "outputId": "df8468fb-8f0a-46b4-e086-1e93421931f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Loaded and chunked 0 documents\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# üìÑ Load PDF and Chunk Text\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_path = \"/content/sample_data/Testing_RAG.pdf\"  # Upload your PDF via sidebar\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = splitter.split_documents(pages)\n",
        "print(f\"üìö Loaded and chunked {len(docs)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caaa4196",
      "metadata": {
        "id": "caaa4196"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üîó Embed and Store in FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ab0315",
      "metadata": {
        "id": "b7ab0315"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üß† Refine User Query with LLM\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.3)\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Refine the user's question for clarity and specificity:\\n\\nUser: {question}\\n\\nRefined:\"\n",
        ")\n",
        "refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45fc934",
      "metadata": {
        "id": "e45fc934"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üîç Build the RetrievalQA Chain (RAG)\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3906b1d3",
      "metadata": {
        "id": "3906b1d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ‚ùì Ask a Question\n",
        "user_question = \"What are the biggest staffing challenges in 2024?\"\n",
        "refined_q = refine_chain.run(user_question)\n",
        "\n",
        "print(\"üîÅ Refined Question:\", refined_q)\n",
        "response = qa_chain.run(refined_q)\n",
        "print(\"\\nüí° Answer:\\n\", response)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}